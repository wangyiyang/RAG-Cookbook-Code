{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVYgDcYssmpZ7afcLYAZp2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangyiyang/RAG-Cookbook-Code/blob/main/ch04/safety_checker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrgn4X5kkhbF"
      },
      "outputs": [],
      "source": [
        "%pip install numpy transformers torch sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "内容安全检查器\n",
        "实现内容安全检测、输出过滤净化和隐私信息保护\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import hashlib\n",
        "from typing import List, Dict, Any, Optional, Set, Tuple\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "\n",
        "\n",
        "class SafetyLevel(Enum):\n",
        "    \"\"\"安全级别枚举\"\"\"\n",
        "    SAFE = \"safe\"\n",
        "    WARNING = \"warning\"\n",
        "    UNSAFE = \"unsafe\"\n",
        "    BLOCKED = \"blocked\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SafetyIssue:\n",
        "    \"\"\"安全问题\"\"\"\n",
        "    issue_type: str\n",
        "    severity: SafetyLevel\n",
        "    description: str\n",
        "    confidence: float\n",
        "    location: Optional[str] = None\n",
        "    suggestions: Optional[List[str]] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SafetyReport:\n",
        "    \"\"\"安全报告\"\"\"\n",
        "    is_safe: bool\n",
        "    overall_score: float\n",
        "    issues: List[SafetyIssue]\n",
        "    processed_content: Optional[str] = None\n",
        "\n",
        "    def get_issues_by_type(self, issue_type: str) -> List[SafetyIssue]:\n",
        "        \"\"\"按类型获取安全问题\"\"\"\n",
        "        return [issue for issue in self.issues if issue.issue_type == issue_type]\n",
        "\n",
        "    def get_issues_by_severity(self, severity: SafetyLevel) -> List[SafetyIssue]:\n",
        "        \"\"\"按严重程度获取安全问题\"\"\"\n",
        "        return [issue for issue in self.issues if issue.severity == severity]\n",
        "\n",
        "\n",
        "class BiasDetector:\n",
        "    \"\"\"偏见检测器\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.bias_patterns = {\n",
        "            'gender': [\n",
        "                r'(男|女)(性|人).*?(更|较|比较).*?(适合|擅长|不擅长)',\n",
        "                r'(男|女)(孩|生).*?(天生|生来).*?',\n",
        "                r'(男|女)(性|人).*?(应该|不应该|必须|不能).*?'\n",
        "            ],\n",
        "            'racial': [\n",
        "                r'(.*族|.*人).*?(天生|生来|本来).*?(聪明|愚笨|懒惰|勤劳)',\n",
        "                r'(.*族|.*人).*?(都|全部|一律).*?',\n",
        "                r'典型的.*?(.*族|.*人).*?'\n",
        "            ],\n",
        "            'age': [\n",
        "                r'(老|年轻)(人|者).*?(应该|不应该|不能|只能).*?',\n",
        "                r'(.*岁|老年|年轻).*?(肯定|一定|必然).*?'\n",
        "            ],\n",
        "            'professional': [\n",
        "                r'(.*职业|.*工作).*?(低级|高级|下等|上等).*?',\n",
        "                r'只有.*?(学历|背景).*?才.*?'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        self.sensitive_terms = {\n",
        "            '歧视性词汇': [\n",
        "                '劣等', '低贱', '下等人', '贫民', '暴民'\n",
        "            ],\n",
        "            '刻板印象': [\n",
        "                '天生', '生来就', '注定', '血统', '基因决定'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def detect_bias(self, text: str) -> float:\n",
        "        \"\"\"检测文本中的偏见\"\"\"\n",
        "        bias_score = 0.0\n",
        "        detected_issues = []\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # 检测偏见模式\n",
        "        for bias_type, patterns in self.bias_patterns.items():\n",
        "            for pattern in patterns:\n",
        "                matches = re.findall(pattern, text_lower)\n",
        "                if matches:\n",
        "                    bias_score += 0.3\n",
        "                    detected_issues.append({\n",
        "                        'type': bias_type,\n",
        "                        'pattern': pattern,\n",
        "                        'matches': matches\n",
        "                    })\n",
        "\n",
        "        # 检测敏感词汇\n",
        "        for category, terms in self.sensitive_terms.items():\n",
        "            for term in terms:\n",
        "                if term in text_lower:\n",
        "                    bias_score += 0.2\n",
        "                    detected_issues.append({\n",
        "                        'type': 'sensitive_term',\n",
        "                        'category': category,\n",
        "                        'term': term\n",
        "                    })\n",
        "\n",
        "        return min(1.0, bias_score)\n",
        "\n",
        "    def get_bias_issues(self, text: str) -> List[SafetyIssue]:\n",
        "        \"\"\"获取详细的偏见问题列表\"\"\"\n",
        "        issues = []\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # 检测不同类型的偏见\n",
        "        for bias_type, patterns in self.bias_patterns.items():\n",
        "            for pattern in patterns:\n",
        "                matches = re.finditer(pattern, text_lower)\n",
        "                for match in matches:\n",
        "                    issues.append(SafetyIssue(\n",
        "                        issue_type=f\"bias_{bias_type}\",\n",
        "                        severity=SafetyLevel.WARNING,\n",
        "                        description=f\"检测到{bias_type}偏见: {match.group()}\",\n",
        "                        confidence=0.8,\n",
        "                        location=f\"位置 {match.start()}-{match.end()}\",\n",
        "                        suggestions=[\n",
        "                            \"避免使用刻板印象的表述\",\n",
        "                            \"使用更中性和客观的语言\",\n",
        "                            \"考虑个体差异而非群体标签\"\n",
        "                        ]\n",
        "                    ))\n",
        "\n",
        "        return issues\n",
        "\n",
        "\n",
        "class ToxicContentDetector:\n",
        "    \"\"\"有毒内容检测器\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.toxic_categories = {\n",
        "            'hate_speech': {\n",
        "                'patterns': [\n",
        "                    r'(仇恨|憎恨|厌恶).*?(民族|种族|宗教)',\n",
        "                    r'(消灭|铲除|清理).*?(.*族|.*教)',\n",
        "                    r'(他们|这些人).*?(都应该|必须|应当).*?(死|滚|消失)'\n",
        "                ],\n",
        "                'keywords': ['仇杀', '种族清洗', '屠杀', '灭绝']\n",
        "            },\n",
        "            'harassment': {\n",
        "                'patterns': [\n",
        "                    r'(你|他|她).*?(白痴|傻子|蠢货|废物)',\n",
        "                    r'(去死|死开|滚开).*?',\n",
        "                    r'(威胁|要杀|要打).*?'\n",
        "                ],\n",
        "                'keywords': ['骚扰', '威胁', '恐吓', '辱骂']\n",
        "            },\n",
        "            'violence': {\n",
        "                'patterns': [\n",
        "                    r'(如何|怎样).*?(制造|制作).*?(炸弹|武器)',\n",
        "                    r'(伤害|攻击|袭击).*?(方法|技巧)',\n",
        "                    r'(杀死|杀害).*?(步骤|过程)'\n",
        "                ],\n",
        "                'keywords': ['暴力', '攻击', '伤害', '武器制造']\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def detect_toxicity(self, text: str) -> float:\n",
        "        \"\"\"检测有毒内容\"\"\"\n",
        "        toxicity_score = 0.0\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        for category, config in self.toxic_categories.items():\n",
        "            # 检测模式\n",
        "            for pattern in config['patterns']:\n",
        "                if re.search(pattern, text_lower):\n",
        "                    toxicity_score += 0.4\n",
        "\n",
        "            # 检测关键词\n",
        "            for keyword in config['keywords']:\n",
        "                if keyword in text_lower:\n",
        "                    toxicity_score += 0.2\n",
        "\n",
        "        return min(1.0, toxicity_score)\n",
        "\n",
        "    def get_toxicity_issues(self, text: str) -> List[SafetyIssue]:\n",
        "        \"\"\"获取有毒内容问题列表\"\"\"\n",
        "        issues = []\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        for category, config in self.toxic_categories.items():\n",
        "            # 检测模式匹配\n",
        "            for pattern in config['patterns']:\n",
        "                matches = re.finditer(pattern, text_lower)\n",
        "                for match in matches:\n",
        "                    severity = SafetyLevel.UNSAFE if category == 'hate_speech' else SafetyLevel.WARNING\n",
        "\n",
        "                    issues.append(SafetyIssue(\n",
        "                        issue_type=f\"toxic_{category}\",\n",
        "                        severity=severity,\n",
        "                        description=f\"检测到{category}内容: {match.group()}\",\n",
        "                        confidence=0.9,\n",
        "                        location=f\"位置 {match.start()}-{match.end()}\",\n",
        "                        suggestions=[\n",
        "                            \"移除有害内容\",\n",
        "                            \"使用积极正面的表述\",\n",
        "                            \"避免可能引起争议的内容\"\n",
        "                        ]\n",
        "                    ))\n",
        "\n",
        "        return issues\n",
        "\n",
        "\n",
        "class PrivacyChecker:\n",
        "    \"\"\"隐私检查器\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.privacy_patterns = {\n",
        "            'phone': {\n",
        "                'pattern': r'1[3-9]\\d{9}',\n",
        "                'description': '手机号码'\n",
        "            },\n",
        "            'email': {\n",
        "                'pattern': r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}',\n",
        "                'description': '邮箱地址'\n",
        "            },\n",
        "            'id_card': {\n",
        "                'pattern': r'\\b\\d{17}[\\dX]\\b',\n",
        "                'description': '身份证号'\n",
        "            },\n",
        "            'credit_card': {\n",
        "                'pattern': r'\\b\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}\\b',\n",
        "                'description': '信用卡号'\n",
        "            },\n",
        "            'bank_account': {\n",
        "                'pattern': r'\\b\\d{16,19}\\b',\n",
        "                'description': '银行账号'\n",
        "            },\n",
        "            'address': {\n",
        "                'pattern': r'(.*省.*市.*区.*路.*号|.*街.*弄.*号)',\n",
        "                'description': '详细地址'\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def check_privacy(self, text: str) -> List[SafetyIssue]:\n",
        "        \"\"\"检查隐私信息泄露\"\"\"\n",
        "        privacy_violations = []\n",
        "\n",
        "        for privacy_type, config in self.privacy_patterns.items():\n",
        "            pattern = config['pattern']\n",
        "            description = config['description']\n",
        "\n",
        "            matches = re.finditer(pattern, text)\n",
        "            for match in matches:\n",
        "                # 对敏感信息进行脱敏处理\n",
        "                masked_content = self._mask_sensitive_info(match.group(), privacy_type)\n",
        "\n",
        "                privacy_violations.append(SafetyIssue(\n",
        "                    issue_type=f\"privacy_{privacy_type}\",\n",
        "                    severity=SafetyLevel.WARNING,\n",
        "                    description=f\"检测到{description}: {masked_content}\",\n",
        "                    confidence=0.95,\n",
        "                    location=f\"位置 {match.start()}-{match.end()}\",\n",
        "                    suggestions=[\n",
        "                        \"移除或脱敏个人信息\",\n",
        "                        \"使用示例数据替代真实信息\",\n",
        "                        \"添加隐私声明\"\n",
        "                    ]\n",
        "                ))\n",
        "\n",
        "        return privacy_violations\n",
        "\n",
        "    def _mask_sensitive_info(self, content: str, info_type: str) -> str:\n",
        "        \"\"\"脱敏处理敏感信息\"\"\"\n",
        "        if info_type == 'phone':\n",
        "            return content[:3] + '****' + content[-4:]\n",
        "        elif info_type == 'email':\n",
        "            parts = content.split('@')\n",
        "            if len(parts) == 2:\n",
        "                username = parts[0]\n",
        "                if len(username) <= 2:\n",
        "                    return '*' + '@' + parts[1]\n",
        "                else:\n",
        "                    return username[:2] + '***@' + parts[1]\n",
        "            return content\n",
        "        elif info_type == 'id_card':\n",
        "            return content[:6] + '********' + content[-4:]\n",
        "        elif info_type == 'credit_card':\n",
        "            return '**** **** **** ' + content[-4:]\n",
        "        else:\n",
        "            return '***敏感信息***'\n",
        "\n",
        "\n",
        "class FactualVerifier:\n",
        "    \"\"\"事实核查器\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.known_facts = {\n",
        "            'technology': {\n",
        "                'rag': {\n",
        "                    'definition': 'RAG是检索增强生成技术',\n",
        "                    'components': ['检索器', '生成器'],\n",
        "                    'applications': ['问答系统', '知识助手']\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.fact_error_patterns = [\n",
        "            r'100%.*?(准确|正确|无误)',  # 过于绝对的表述\n",
        "            r'永远不会.*?(错误|失败)',  # 不现实的声明\n",
        "            r'所有.*?都.*?',            # 过于绝对的概括\n",
        "            r'从来没有.*?',            # 绝对否定\n",
        "            r'绝对.*?(安全|可靠)'       # 过度保证\n",
        "        ]\n",
        "\n",
        "    def verify_facts(self, content: str, context: str) -> List[SafetyIssue]:\n",
        "        \"\"\"验证事实准确性\"\"\"\n",
        "        factual_errors = []\n",
        "\n",
        "        # 检测过于绝对的表述\n",
        "        for pattern in self.fact_error_patterns:\n",
        "            matches = re.finditer(pattern, content)\n",
        "            for match in matches:\n",
        "                factual_errors.append(SafetyIssue(\n",
        "                    issue_type=\"factual_absolute_claim\",\n",
        "                    severity=SafetyLevel.WARNING,\n",
        "                    description=f\"检测到过于绝对的表述: {match.group()}\",\n",
        "                    confidence=0.7,\n",
        "                    location=f\"位置 {match.start()}-{match.end()}\",\n",
        "                    suggestions=[\n",
        "                        \"使用更谨慎的表述\",\n",
        "                        \"添加适当的限定词\",\n",
        "                        \"承认可能存在的例外情况\"\n",
        "                    ]\n",
        "                ))\n",
        "\n",
        "        # 检测与上下文的不一致\n",
        "        inconsistencies = self._check_context_consistency(content, context)\n",
        "        factual_errors.extend(inconsistencies)\n",
        "\n",
        "        return factual_errors\n",
        "\n",
        "    def _check_context_consistency(self, content: str, context: str) -> List[SafetyIssue]:\n",
        "        \"\"\"检查与上下文的一致性\"\"\"\n",
        "        issues = []\n",
        "\n",
        "        # 简化的一致性检查\n",
        "        # 实际应用中需要更复杂的NLP技术\n",
        "\n",
        "        return issues\n",
        "\n",
        "\n",
        "class OutputSanitizer:\n",
        "    \"\"\"输出净化器\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.replacement_rules = [\n",
        "            {\n",
        "                'pattern': r'1[3-9]\\d{9}',\n",
        "                'replacement': '[手机号码]',\n",
        "                'description': '手机号码脱敏'\n",
        "            },\n",
        "            {\n",
        "                'pattern': r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}',\n",
        "                'replacement': '[邮箱地址]',\n",
        "                'description': '邮箱地址脱敏'\n",
        "            },\n",
        "            {\n",
        "                'pattern': r'\\b\\d{17}[\\dX]\\b',\n",
        "                'replacement': '[身份证号]',\n",
        "                'description': '身份证号脱敏'\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        self.content_filters = {\n",
        "            'inappropriate_language': [\n",
        "                '傻逼', '操蛋', '他妈的', '狗屎'\n",
        "            ],\n",
        "            'spam_indicators': [\n",
        "                '点击这里', '立即购买', '限时优惠', '马上行动'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def sanitize_output(self, content: str) -> Tuple[str, List[str]]:\n",
        "        \"\"\"输出内容净化\"\"\"\n",
        "        sanitized_content = content\n",
        "        applied_rules = []\n",
        "\n",
        "        # 1. 敏感信息脱敏\n",
        "        for rule in self.replacement_rules:\n",
        "            if re.search(rule['pattern'], sanitized_content):\n",
        "                sanitized_content = re.sub(\n",
        "                    rule['pattern'],\n",
        "                    rule['replacement'],\n",
        "                    sanitized_content\n",
        "                )\n",
        "                applied_rules.append(rule['description'])\n",
        "\n",
        "        # 2. 不当内容过滤\n",
        "        for category, terms in self.content_filters.items():\n",
        "            for term in terms:\n",
        "                if term in sanitized_content:\n",
        "                    sanitized_content = sanitized_content.replace(\n",
        "                        term,\n",
        "                        '[已过滤]'\n",
        "                    )\n",
        "                    applied_rules.append(f'{category}过滤')\n",
        "\n",
        "        # 3. 格式规范化\n",
        "        sanitized_content = self.normalize_format(sanitized_content)\n",
        "\n",
        "        return sanitized_content, applied_rules\n",
        "\n",
        "    def normalize_format(self, content: str) -> str:\n",
        "        \"\"\"格式规范化\"\"\"\n",
        "        # 移除多余空行\n",
        "        content = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', content)\n",
        "\n",
        "        # 统一标点符号\n",
        "        content = content.replace('，，', '，')\n",
        "        content = content.replace('。。', '。')\n",
        "\n",
        "        # 移除首尾空白\n",
        "        content = content.strip()\n",
        "\n",
        "        return content\n",
        "\n",
        "\n",
        "class ContentSafetyChecker:\n",
        "    \"\"\"内容安全检查器主类\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.bias_detector = BiasDetector()\n",
        "        self.toxic_detector = ToxicContentDetector()\n",
        "        self.privacy_checker = PrivacyChecker()\n",
        "        self.factual_verifier = FactualVerifier()\n",
        "        self.output_sanitizer = OutputSanitizer()\n",
        "\n",
        "        # 安全阈值配置\n",
        "        self.safety_thresholds = {\n",
        "            'bias_score': 0.2,      # 降低偏见容忍度\n",
        "            'toxicity_score': 0.1,  # 降低有毒内容容忍度\n",
        "            'overall_safety': 0.90  # 进一步提高总体安全要求\n",
        "        }\n",
        "\n",
        "    def comprehensive_safety_check(\n",
        "        self,\n",
        "        generated_content: str,\n",
        "        context: str = \"\"\n",
        "    ) -> SafetyReport:\n",
        "        \"\"\"综合安全检查\"\"\"\n",
        "        all_issues = []\n",
        "\n",
        "        # 1. 偏见检测\n",
        "        bias_score = self.bias_detector.detect_bias(generated_content)\n",
        "        if bias_score > self.safety_thresholds['bias_score']:\n",
        "            bias_issues = self.bias_detector.get_bias_issues(generated_content)\n",
        "            all_issues.extend(bias_issues)\n",
        "\n",
        "        # 2. 有毒内容检测\n",
        "        toxicity_score = self.toxic_detector.detect_toxicity(generated_content)\n",
        "        if toxicity_score > self.safety_thresholds['toxicity_score']:\n",
        "            toxic_issues = self.toxic_detector.get_toxicity_issues(generated_content)\n",
        "            all_issues.extend(toxic_issues)\n",
        "\n",
        "        # 3. 隐私信息检查 - 总是检查，不设阈值\n",
        "        privacy_issues = self.privacy_checker.check_privacy(generated_content)\n",
        "        all_issues.extend(privacy_issues)\n",
        "\n",
        "        # 4. 事实核查\n",
        "        factual_issues = self.factual_verifier.verify_facts(\n",
        "            generated_content, context\n",
        "        )\n",
        "        all_issues.extend(factual_issues)\n",
        "\n",
        "        # 5. 内容净化\n",
        "        sanitized_content, applied_rules = self.output_sanitizer.sanitize_output(\n",
        "            generated_content\n",
        "        )\n",
        "\n",
        "        # 计算总体安全分数\n",
        "        overall_score = self._calculate_safety_score(\n",
        "            bias_score, toxicity_score, len(privacy_issues), len(factual_issues)\n",
        "        )\n",
        "\n",
        "        # 判断是否安全\n",
        "        is_safe = (\n",
        "            overall_score >= self.safety_thresholds['overall_safety'] and\n",
        "            not any(issue.severity == SafetyLevel.UNSAFE for issue in all_issues)\n",
        "        )\n",
        "\n",
        "        return SafetyReport(\n",
        "            is_safe=is_safe,\n",
        "            overall_score=overall_score,\n",
        "            issues=all_issues,\n",
        "            processed_content=sanitized_content\n",
        "        )\n",
        "\n",
        "    def _calculate_safety_score(\n",
        "        self,\n",
        "        bias_score: float,\n",
        "        toxicity_score: float,\n",
        "        privacy_violations: int,\n",
        "        factual_errors: int\n",
        "    ) -> float:\n",
        "        \"\"\"计算总体安全分数\"\"\"\n",
        "        base_score = 1.0\n",
        "\n",
        "        # 偏见扣分 - 增加权重\n",
        "        base_score -= bias_score * 0.5\n",
        "\n",
        "        # 有毒内容扣分\n",
        "        base_score -= toxicity_score * 0.4\n",
        "\n",
        "        # 隐私泄露扣分\n",
        "        base_score -= privacy_violations * 0.1\n",
        "\n",
        "        # 事实错误扣分\n",
        "        base_score -= factual_errors * 0.05\n",
        "\n",
        "        return max(0.0, base_score)\n",
        "\n",
        "    def quick_safety_check(self, content: str) -> bool:\n",
        "        \"\"\"快速安全检查\"\"\"\n",
        "        # 简化的快速检查\n",
        "        bias_score = self.bias_detector.detect_bias(content)\n",
        "        toxicity_score = self.toxic_detector.detect_toxicity(content)\n",
        "\n",
        "        return (bias_score <= self.safety_thresholds['bias_score'] and\n",
        "                toxicity_score <= self.safety_thresholds['toxicity_score'])\n",
        "\n",
        "    def get_safety_suggestions(self, issues: List[SafetyIssue]) -> List[str]:\n",
        "        \"\"\"获取安全建议\"\"\"\n",
        "        suggestions = set()\n",
        "\n",
        "        for issue in issues:\n",
        "            if issue.suggestions:\n",
        "                suggestions.update(issue.suggestions)\n",
        "\n",
        "        return list(suggestions)\n",
        "\n",
        "    def debug_privacy_check(self, content: str) -> Dict[str, Any]:\n",
        "        \"\"\"调试隐私检查 - 显示所有匹配结果\"\"\"\n",
        "        debug_info = {}\n",
        "\n",
        "        for privacy_type, config in self.privacy_checker.privacy_patterns.items():\n",
        "            pattern = config['pattern']\n",
        "            description = config['description']\n",
        "\n",
        "            matches = list(re.finditer(pattern, content))\n",
        "            debug_info[privacy_type] = {\n",
        "                'pattern': pattern,\n",
        "                'description': description,\n",
        "                'matches': [match.group() for match in matches],\n",
        "                'positions': [(match.start(), match.end()) for match in matches]\n",
        "            }\n",
        "\n",
        "        return debug_info\n",
        "\n",
        "    def test_regex_patterns(self, test_text: str) -> None:\n",
        "        \"\"\"测试正则表达式模式\"\"\"\n",
        "        print(f\"测试文本: {test_text}\")\n",
        "\n",
        "        # 直接测试邮箱正则表达式\n",
        "        email_pattern = r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}'\n",
        "        email_matches = re.findall(email_pattern, test_text)\n",
        "        print(f\"邮箱正则表达式直接测试: {email_matches}\")\n",
        "\n",
        "        # 测试其他模式\n",
        "        phone_pattern = r'1[3-9]\\d{9}'\n",
        "        phone_matches = re.findall(phone_pattern, test_text)\n",
        "        print(f\"手机正则表达式直接测试: {phone_matches}\")\n",
        "\n",
        "\n",
        "# 使用示例\n",
        "if __name__ == \"__main__\":\n",
        "    # 初始化安全检查器\n",
        "    safety_checker = ContentSafetyChecker()\n",
        "\n",
        "    # 测试内容\n",
        "    test_contents = [\n",
        "        \"RAG是一种先进的AI技术，它结合了检索和生成的优势。\",\n",
        "        \"男性天生就比女性更适合编程工作。\",  # 偏见内容\n",
        "        \"我的手机号是13812345678，邮箱是test@example.com。\",  # 隐私信息\n",
        "        \"RAG技术100%准确，永远不会出错。\"  # 过于绝对的表述\n",
        "    ]\n",
        "\n",
        "    for i, content in enumerate(test_contents, 1):\n",
        "        print(f\"\\n测试内容 {i}: {content}\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # 综合安全检查\n",
        "        safety_report = safety_checker.comprehensive_safety_check(content)\n",
        "\n",
        "        print(f\"安全状态: {'✅ 安全' if safety_report.is_safe else '❌ 不安全'}\")\n",
        "        print(f\"安全分数: {safety_report.overall_score:.3f}\")\n",
        "\n",
        "        if safety_report.issues:\n",
        "            print(f\"检测到 {len(safety_report.issues)} 个问题:\")\n",
        "            for issue in safety_report.issues:\n",
        "                print(f\"  - {issue.issue_type}: {issue.description}\")\n",
        "\n",
        "        if safety_report.processed_content != content:\n",
        "            print(f\"净化后内容: {safety_report.processed_content}\")\n",
        "\n",
        "        # 添加隐私检查调试信息\n",
        "        if \"手机号\" in content or \"@\" in content:\n",
        "            debug_info = safety_checker.debug_privacy_check(content)\n",
        "            print(\"隐私检查调试信息:\")\n",
        "            for privacy_type, info in debug_info.items():\n",
        "                if info['matches']:\n",
        "                    print(f\"  {privacy_type}: 匹配 {info['matches']}\")\n",
        "                else:\n",
        "                    print(f\"  {privacy_type}: 无匹配 (模式: {info['pattern']})\")\n",
        "\n",
        "            # 添加直接正则测试\n",
        "            print(\"正则表达式直接测试:\")\n",
        "            safety_checker.test_regex_patterns(content)\n",
        "\n",
        "        print(\"=\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjj6bXwEkr0o",
        "outputId": "6c41b3ac-984c-4631-cbd9-c268d0aaf3e8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "测试内容 1: RAG是一种先进的AI技术，它结合了检索和生成的优势。\n",
            "==================================================\n",
            "安全状态: ✅ 安全\n",
            "安全分数: 1.000\n",
            "==================================================\n",
            "\n",
            "测试内容 2: 男性天生就比女性更适合编程工作。\n",
            "==================================================\n",
            "安全状态: ❌ 不安全\n",
            "安全分数: 0.750\n",
            "检测到 1 个问题:\n",
            "  - bias_gender: 检测到gender偏见: 男性天生就比女性更适合\n",
            "==================================================\n",
            "\n",
            "测试内容 3: 我的手机号是13812345678，邮箱是test@example.com。\n",
            "==================================================\n",
            "安全状态: ❌ 不安全\n",
            "安全分数: 0.800\n",
            "检测到 2 个问题:\n",
            "  - privacy_phone: 检测到手机号码: 138****5678\n",
            "  - privacy_email: 检测到邮箱地址: te***@example.com\n",
            "净化后内容: 我的手机号是[手机号码]，邮箱是[邮箱地址]。\n",
            "隐私检查调试信息:\n",
            "  phone: 匹配 ['13812345678']\n",
            "  email: 匹配 ['test@example.com']\n",
            "  id_card: 无匹配 (模式: \\b\\d{17}[\\dX]\\b)\n",
            "  credit_card: 无匹配 (模式: \\b\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}\\b)\n",
            "  bank_account: 无匹配 (模式: \\b\\d{16,19}\\b)\n",
            "  address: 无匹配 (模式: (.*省.*市.*区.*路.*号|.*街.*弄.*号))\n",
            "正则表达式直接测试:\n",
            "测试文本: 我的手机号是13812345678，邮箱是test@example.com。\n",
            "邮箱正则表达式直接测试: ['test@example.com']\n",
            "手机正则表达式直接测试: ['13812345678']\n",
            "==================================================\n",
            "\n",
            "测试内容 4: RAG技术100%准确，永远不会出错。\n",
            "==================================================\n",
            "安全状态: ✅ 安全\n",
            "安全分数: 0.950\n",
            "检测到 1 个问题:\n",
            "  - factual_absolute_claim: 检测到过于绝对的表述: 100%准确\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sOVZylFNkuit"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}