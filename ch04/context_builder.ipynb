{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNbYzsci5u/lsbWqTgil+NA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangyiyang/RAG-Cookbook-Code/blob/main/ch04/context_builder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OjLfqIUe9R9"
      },
      "outputs": [],
      "source": [
        "%pip install numpy transformers torch sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "上下文构建器\n",
        "实现多文档信息融合、冲突解决和智能截断\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import hashlib\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Document:\n",
        "    \"\"\"文档数据结构\"\"\"\n",
        "    id: str\n",
        "    page_content: str\n",
        "    metadata: Dict[str, Any]\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if not self.id:\n",
        "            # 如果没有ID，基于内容生成\n",
        "            self.id = hashlib.md5(self.page_content.encode()).hexdigest()[:8]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Fact:\n",
        "    \"\"\"事实信息数据结构\"\"\"\n",
        "    content: str\n",
        "    entity: str\n",
        "    attribute: str\n",
        "    value: str\n",
        "    confidence: float = 1.0\n",
        "\n",
        "\n",
        "class FactChecker:\n",
        "    \"\"\"事实检查器\"\"\"\n",
        "\n",
        "    def extract_facts(self, text: str) -> List[Fact]:\n",
        "        \"\"\"从文本中提取事实信息\"\"\"\n",
        "        facts = []\n",
        "\n",
        "        # 简化的事实提取（实际应用中使用NER和关系抽取）\n",
        "        sentences = text.split('。')\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if len(sentence.strip()) < 10:\n",
        "                continue\n",
        "\n",
        "            # 识别简单的事实模式：A是B、A有B等\n",
        "            patterns = [\n",
        "                r'(.+?)是(.+)',\n",
        "                r'(.+?)有(.+)',\n",
        "                r'(.+?)包含(.+)',\n",
        "                r'(.+?)支持(.+)'\n",
        "            ]\n",
        "\n",
        "            for pattern in patterns:\n",
        "                match = re.search(pattern, sentence)\n",
        "                if match:\n",
        "                    entity = match.group(1).strip()\n",
        "                    value = match.group(2).strip()\n",
        "\n",
        "                    facts.append(Fact(\n",
        "                        content=sentence,\n",
        "                        entity=entity,\n",
        "                        attribute='description',\n",
        "                        value=value,\n",
        "                        confidence=0.8\n",
        "                    ))\n",
        "                    break\n",
        "\n",
        "        return facts\n",
        "\n",
        "    def are_conflicting(self, fact1: Fact, fact2: Fact) -> bool:\n",
        "        \"\"\"判断两个事实是否冲突\"\"\"\n",
        "        # 简化的冲突检测\n",
        "        if fact1.entity.lower() == fact2.entity.lower():\n",
        "            if fact1.attribute == fact2.attribute:\n",
        "                if fact1.value.lower() != fact2.value.lower():\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "\n",
        "class SourceRanker:\n",
        "    \"\"\"信息源权威性排序器\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.authority_scores = {\n",
        "            'wikipedia': 0.9,\n",
        "            'academic_paper': 0.95,\n",
        "            'official_doc': 0.98,\n",
        "            'news': 0.7,\n",
        "            'blog': 0.5,\n",
        "            'social_media': 0.3,\n",
        "            'unknown': 0.6\n",
        "        }\n",
        "\n",
        "    def get_source_authority(self, document: Document) -> float:\n",
        "        \"\"\"获取文档权威性分数\"\"\"\n",
        "        source_type = document.metadata.get('source_type', 'unknown')\n",
        "        base_score = self.authority_scores.get(source_type, 0.6)\n",
        "\n",
        "        # 根据其他因素调整分数\n",
        "        if 'publish_date' in document.metadata:\n",
        "            # 较新的文档权威性稍高\n",
        "            base_score += 0.05\n",
        "\n",
        "        if document.metadata.get('citation_count', 0) > 100:\n",
        "            # 高引用文档权威性更高\n",
        "            base_score += 0.1\n",
        "\n",
        "        return min(base_score, 1.0)\n",
        "\n",
        "\n",
        "class ConflictResolver:\n",
        "    \"\"\"信息冲突解决器\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.fact_checker = FactChecker()\n",
        "        self.source_ranker = SourceRanker()\n",
        "\n",
        "    def resolve_conflicts(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"解决文档间的信息冲突\"\"\"\n",
        "        # 检测冲突\n",
        "        conflicts = self.detect_conflicts(documents)\n",
        "\n",
        "        if not conflicts:\n",
        "            return documents\n",
        "\n",
        "        # 解决冲突\n",
        "        resolved_docs = []\n",
        "        for doc in documents:\n",
        "            if doc.id in conflicts:\n",
        "                # 选择权威性更高的信息\n",
        "                authoritative_content = self.select_authoritative_info(\n",
        "                    doc, conflicts[doc.id]\n",
        "                )\n",
        "                doc.page_content = authoritative_content\n",
        "            resolved_docs.append(doc)\n",
        "\n",
        "        return resolved_docs\n",
        "\n",
        "    def detect_conflicts(self, documents: List[Document]) -> Dict[str, List[Dict]]:\n",
        "        \"\"\"检测信息冲突\"\"\"\n",
        "        conflicts = defaultdict(list)\n",
        "\n",
        "        # 提取所有文档的事实\n",
        "        doc_facts = {}\n",
        "        for doc in documents:\n",
        "            facts = self.fact_checker.extract_facts(doc.page_content)\n",
        "            doc_facts[doc.id] = facts\n",
        "\n",
        "        # 检测冲突\n",
        "        for doc_id1, facts1 in doc_facts.items():\n",
        "            for doc_id2, facts2 in doc_facts.items():\n",
        "                if doc_id1 >= doc_id2:  # 避免重复比较\n",
        "                    continue\n",
        "\n",
        "                for fact1 in facts1:\n",
        "                    for fact2 in facts2:\n",
        "                        if self.fact_checker.are_conflicting(fact1, fact2):\n",
        "                            conflicts[doc_id1].append({\n",
        "                                'conflicting_doc': doc_id2,\n",
        "                                'fact1': fact1,\n",
        "                                'fact2': fact2\n",
        "                            })\n",
        "\n",
        "        return conflicts\n",
        "\n",
        "    def select_authoritative_info(\n",
        "        self,\n",
        "        document: Document,\n",
        "        conflict_info: List[Dict]\n",
        "    ) -> str:\n",
        "        \"\"\"选择权威性更高的信息\"\"\"\n",
        "        doc_authority = self.source_ranker.get_source_authority(document)\n",
        "\n",
        "        # 简化处理：如果当前文档权威性较高，保留原内容\n",
        "        # 否则，标记存在争议\n",
        "        if doc_authority > 0.8:\n",
        "            return document.page_content\n",
        "        else:\n",
        "            return (document.page_content +\n",
        "                   \"\\n\\n[注意：此信息可能存在不同观点，请谨慎参考]\")\n",
        "\n",
        "\n",
        "class ContextBuilder:\n",
        "    \"\"\"上下文构建器主类\"\"\"\n",
        "\n",
        "    def __init__(self, max_context_length: int = 4000):\n",
        "        self.max_context_length = max_context_length\n",
        "        self.conflict_resolver = ConflictResolver()\n",
        "\n",
        "    def build_context(\n",
        "        self,\n",
        "        query: str,\n",
        "        documents: List[Document]\n",
        "    ) -> str:\n",
        "        \"\"\"智能上下文构建\"\"\"\n",
        "        # 1. 文档去重和排序\n",
        "        unique_docs = self.deduplicate_documents(documents)\n",
        "        ranked_docs = self.rank_by_relevance(query, unique_docs)\n",
        "\n",
        "        # 2. 信息冲突检测与解决\n",
        "        resolved_docs = self.conflict_resolver.resolve_conflicts(ranked_docs)\n",
        "\n",
        "        # 3. 上下文组装\n",
        "        context_parts = []\n",
        "        current_length = 0\n",
        "\n",
        "        for i, doc in enumerate(resolved_docs):\n",
        "            # 添加来源标识\n",
        "            source_info = f\"来源{i+1}[{doc.metadata.get('title', 'Unknown')}]:\"\n",
        "            content = f\"{source_info}\\n{doc.page_content}\\n\"\n",
        "\n",
        "            # 长度控制\n",
        "            if current_length + len(content) > self.max_context_length:\n",
        "                # 智能截断：保留最重要的信息\n",
        "                remaining_space = self.max_context_length - current_length\n",
        "                truncated_content = self.intelligent_truncate(\n",
        "                    content, remaining_space, query\n",
        "                )\n",
        "                if truncated_content:\n",
        "                    context_parts.append(truncated_content)\n",
        "                break\n",
        "\n",
        "            context_parts.append(content)\n",
        "            current_length += len(content)\n",
        "\n",
        "        return \"\\n\".join(context_parts)\n",
        "\n",
        "    def deduplicate_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"文档去重\"\"\"\n",
        "        seen_content = set()\n",
        "        unique_docs = []\n",
        "\n",
        "        for doc in documents:\n",
        "            # 使用内容hash进行去重\n",
        "            content_hash = hashlib.md5(doc.page_content.encode()).hexdigest()\n",
        "\n",
        "            if content_hash not in seen_content:\n",
        "                seen_content.add(content_hash)\n",
        "                unique_docs.append(doc)\n",
        "\n",
        "        return unique_docs\n",
        "\n",
        "    def rank_by_relevance(\n",
        "        self,\n",
        "        query: str,\n",
        "        documents: List[Document]\n",
        "    ) -> List[Document]:\n",
        "        \"\"\"根据相关性排序文档\"\"\"\n",
        "        scored_docs = []\n",
        "\n",
        "        for doc in documents:\n",
        "            relevance_score = self.calculate_relevance(query, doc)\n",
        "            scored_docs.append((doc, relevance_score))\n",
        "\n",
        "        # 按相关性分数降序排序\n",
        "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return [doc for doc, score in scored_docs]\n",
        "\n",
        "    def calculate_relevance(self, query: str, document: Document) -> float:\n",
        "        \"\"\"计算文档与查询的相关性\"\"\"\n",
        "        query_words = set(query.lower().split())\n",
        "        doc_words = set(document.page_content.lower().split())\n",
        "\n",
        "        # 简单的词汇重叠相关性计算\n",
        "        intersection = query_words & doc_words\n",
        "        union = query_words | doc_words\n",
        "\n",
        "        jaccard_score = len(intersection) / len(union) if union else 0\n",
        "\n",
        "        # 考虑文档元数据中的权威性\n",
        "        authority_boost = self.conflict_resolver.source_ranker.get_source_authority(document)\n",
        "\n",
        "        return jaccard_score * 0.7 + authority_boost * 0.3\n",
        "\n",
        "    def intelligent_truncate(\n",
        "        self,\n",
        "        content: str,\n",
        "        max_length: int,\n",
        "        query: str\n",
        "    ) -> str:\n",
        "        \"\"\"基于查询相关性的智能截断\"\"\"\n",
        "        if len(content) <= max_length:\n",
        "            return content\n",
        "\n",
        "        sentences = content.split('。')\n",
        "        sentence_scores = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if len(sentence.strip()) < 5:\n",
        "                continue\n",
        "            # 计算句子与查询的相关性\n",
        "            relevance_score = self.calculate_sentence_relevance(sentence, query)\n",
        "            sentence_scores.append((sentence, relevance_score))\n",
        "\n",
        "        # 按相关性排序，选择最相关的句子\n",
        "        sentence_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        selected_content = \"\"\n",
        "        for sentence, score in sentence_scores:\n",
        "            potential_content = selected_content + sentence + \"。\"\n",
        "            if len(potential_content) <= max_length:\n",
        "                selected_content = potential_content\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return selected_content\n",
        "\n",
        "    def calculate_sentence_relevance(self, sentence: str, query: str) -> float:\n",
        "        \"\"\"计算句子与查询的相关性\"\"\"\n",
        "        query_words = set(query.lower().split())\n",
        "        sentence_words = set(sentence.lower().split())\n",
        "\n",
        "        # 词汇重叠分数\n",
        "        intersection = query_words & sentence_words\n",
        "        overlap_score = len(intersection) / len(query_words) if query_words else 0\n",
        "\n",
        "        # 句子长度惩罚（太短的句子信息量有限）\n",
        "        length_score = min(len(sentence) / 50, 1.0)\n",
        "\n",
        "        return overlap_score * 0.8 + length_score * 0.2\n",
        "\n",
        "\n",
        "# 使用示例\n",
        "if __name__ == \"__main__\":\n",
        "    # 创建示例文档\n",
        "    documents = [\n",
        "        Document(\n",
        "            id=\"doc1\",\n",
        "            page_content=\"RAG是检索增强生成技术，它结合了信息检索和文本生成。RAG通过检索相关文档来提升生成质量。\",\n",
        "            metadata={\"title\": \"RAG技术介绍\", \"source_type\": \"academic_paper\"}\n",
        "        ),\n",
        "        Document(\n",
        "            id=\"doc2\",\n",
        "            page_content=\"检索增强生成（RAG）是一种新型的AI技术。它能够根据检索到的信息生成更准确的回答。\",\n",
        "            metadata={\"title\": \"AI技术概述\", \"source_type\": \"blog\"}\n",
        "        ),\n",
        "        Document(\n",
        "            id=\"doc3\",\n",
        "            page_content=\"RAG系统包含两个主要组件：检索器和生成器。检索器负责找到相关文档，生成器负责产生最终答案。\",\n",
        "            metadata={\"title\": \"RAG系统架构\", \"source_type\": \"official_doc\"}\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # 初始化上下文构建器\n",
        "    context_builder = ContextBuilder(max_context_length=500)\n",
        "\n",
        "    # 构建上下文\n",
        "    query = \"什么是RAG技术？\"\n",
        "    context = context_builder.build_context(query, documents)\n",
        "\n",
        "    print(\"构建的上下文:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(context)\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"上下文长度: {len(context)} 字符\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytbwGx_sfJ_A",
        "outputId": "d525c5f6-2a44-412e-ecb9-bc7360785b82"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "构建的上下文:\n",
            "==================================================\n",
            "来源1[RAG系统架构]:\n",
            "RAG系统包含两个主要组件：检索器和生成器。检索器负责找到相关文档，生成器负责产生最终答案。\n",
            "\n",
            "来源2[RAG技术介绍]:\n",
            "RAG是检索增强生成技术，它结合了信息检索和文本生成。RAG通过检索相关文档来提升生成质量。\n",
            "\n",
            "来源3[AI技术概述]:\n",
            "检索增强生成（RAG）是一种新型的AI技术。它能够根据检索到的信息生成更准确的回答。\n",
            "\n",
            "==================================================\n",
            "上下文长度: 180 字符\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FJUZQkcBfL5z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}