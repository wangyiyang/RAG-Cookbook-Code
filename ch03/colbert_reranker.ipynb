{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNDTaGFQXBiZei32URuS5Ih",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangyiyang/RAG-Cookbook-Code/blob/main/ch03/colbert_reranker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaxrWbNpM-xq",
        "outputId": "abad9949-366d-4eb8-d90b-70bd84f860b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install sentence-transformers torch transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ColBERTé‡æ’åºç³»ç»Ÿ\n",
        "å®ç°ç²¾ç»†åŒ–çš„Tokençº§äº¤äº’é‡æ’åºç®—æ³•\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "import time\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ColBERTResult:\n",
        "    \"\"\"ColBERTé‡æ’åºç»“æœ\"\"\"\n",
        "    document_id: str\n",
        "    content: str\n",
        "    score: float\n",
        "    token_scores: List[float]\n",
        "    interaction_matrix: Optional[np.ndarray] = None\n",
        "\n",
        "\n",
        "class MockColBERTEncoder:\n",
        "    \"\"\"æ¨¡æ‹Ÿçš„ColBERTç¼–ç å™¨ï¼ˆå®é™…åº”ç”¨ä¸­ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼‰\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim: int = 128):\n",
        "        self.embedding_dim = embedding_dim\n",
        "        # æ¨¡æ‹Ÿè¯æ±‡è¡¨\n",
        "        self.vocab = {}\n",
        "        self.vocab_size = 30000\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"ç®€å•çš„åˆ†è¯ï¼ˆå®é™…åº”ç”¨ä¸­ä½¿ç”¨BERTåˆ†è¯å™¨ï¼‰\"\"\"\n",
        "        import re\n",
        "        # ä¸­è‹±æ–‡æ··åˆåˆ†è¯\n",
        "        tokens = re.findall(r'[\\u4e00-\\u9fff]|[a-zA-Z]+|\\d+', text.lower())\n",
        "        return tokens[:512]  # é™åˆ¶æœ€å¤§é•¿åº¦\n",
        "\n",
        "    def encode_tokens(self, tokens: List[str]) -> np.ndarray:\n",
        "        \"\"\"ç¼–ç tokensä¸ºå‘é‡ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰\"\"\"\n",
        "        embeddings = []\n",
        "        for token in tokens:\n",
        "            # ç®€å•çš„hashç¼–ç æ¨¡æ‹Ÿ\n",
        "            token_hash = hash(token) % self.vocab_size\n",
        "            # ç”Ÿæˆéšæœºä½†ä¸€è‡´çš„embedding\n",
        "            np.random.seed(token_hash)\n",
        "            embedding = np.random.normal(0, 1, self.embedding_dim)\n",
        "            embedding = embedding / np.linalg.norm(embedding)  # å½’ä¸€åŒ–\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        return np.array(embeddings)\n",
        "\n",
        "\n",
        "class ColBERTReRanker:\n",
        "    \"\"\"ColBERTé‡æ’åºå™¨\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim: int = 128):\n",
        "        self.encoder = MockColBERTEncoder(embedding_dim)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.cache = {}  # æ–‡æ¡£ç¼–ç ç¼“å­˜\n",
        "\n",
        "    def encode_query(self, query: str) -> np.ndarray:\n",
        "        \"\"\"ç¼–ç æŸ¥è¯¢ä¸ºtoken embeddings\"\"\"\n",
        "        tokens = self.encoder.tokenize(query)\n",
        "        if not tokens:\n",
        "            return np.array([]).reshape(0, self.embedding_dim)\n",
        "\n",
        "        embeddings = self.encoder.encode_tokens(tokens)\n",
        "        return embeddings\n",
        "\n",
        "    def encode_document(self, document: str, doc_id: str = None) -> np.ndarray:\n",
        "        \"\"\"ç¼–ç æ–‡æ¡£ä¸ºtoken embeddingsï¼ˆå¸¦ç¼“å­˜ï¼‰\"\"\"\n",
        "        if doc_id and doc_id in self.cache:\n",
        "            return self.cache[doc_id]\n",
        "\n",
        "        tokens = self.encoder.tokenize(document)\n",
        "        if not tokens:\n",
        "            return np.array([]).reshape(0, self.embedding_dim)\n",
        "\n",
        "        embeddings = self.encoder.encode_tokens(tokens)\n",
        "\n",
        "        if doc_id:\n",
        "            self.cache[doc_id] = embeddings\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def compute_interaction_score(\n",
        "        self,\n",
        "        query_embeddings: np.ndarray,\n",
        "        doc_embeddings: np.ndarray\n",
        "    ) -> Tuple[float, np.ndarray]:\n",
        "        \"\"\"è®¡ç®—æŸ¥è¯¢å’Œæ–‡æ¡£çš„äº¤äº’å¾—åˆ†\"\"\"\n",
        "        if query_embeddings.size == 0 or doc_embeddings.size == 0:\n",
        "            return 0.0, np.array([])\n",
        "\n",
        "        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼šæ¯ä¸ªæŸ¥è¯¢tokenä¸æ¯ä¸ªæ–‡æ¡£tokençš„ç›¸ä¼¼åº¦\n",
        "        similarity_matrix = np.dot(query_embeddings, doc_embeddings.T)\n",
        "\n",
        "        # ColBERTæ ¸å¿ƒï¼šæ¯ä¸ªæŸ¥è¯¢tokenæ‰¾æœ€ç›¸ä¼¼çš„æ–‡æ¡£token\n",
        "        max_similarities = np.max(similarity_matrix, axis=1)\n",
        "\n",
        "        # æ€»åˆ†æ•°ï¼šæ‰€æœ‰æŸ¥è¯¢tokençš„æœ€å¤§ç›¸ä¼¼åº¦ä¹‹å’Œ\n",
        "        total_score = np.sum(max_similarities)\n",
        "\n",
        "        return total_score, max_similarities\n",
        "\n",
        "    def rerank(\n",
        "        self,\n",
        "        query: str,\n",
        "        documents: List[str],\n",
        "        top_k: int = 5,\n",
        "        doc_ids: Optional[List[str]] = None\n",
        "    ) -> List[ColBERTResult]:\n",
        "        \"\"\"é‡æ’åºæ–‡æ¡£\"\"\"\n",
        "        if not documents:\n",
        "            return []\n",
        "\n",
        "        # ç¼–ç æŸ¥è¯¢\n",
        "        query_embeddings = self.encode_query(query)\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for i, document in enumerate(documents):\n",
        "            doc_id = doc_ids[i] if doc_ids else f\"doc_{i}\"\n",
        "\n",
        "            # ç¼–ç æ–‡æ¡£\n",
        "            doc_embeddings = self.encode_document(document, doc_id)\n",
        "\n",
        "            # è®¡ç®—äº¤äº’å¾—åˆ†\n",
        "            score, token_scores = self.compute_interaction_score(\n",
        "                query_embeddings, doc_embeddings\n",
        "            )\n",
        "\n",
        "            result = ColBERTResult(\n",
        "                document_id=doc_id,\n",
        "                content=document,\n",
        "                score=score,\n",
        "                token_scores=token_scores.tolist()\n",
        "            )\n",
        "            results.append(result)\n",
        "\n",
        "        # æŒ‰åˆ†æ•°æ’åº\n",
        "        results.sort(key=lambda x: x.score, reverse=True)\n",
        "\n",
        "        return results[:top_k]\n",
        "\n",
        "    def batch_rerank(\n",
        "        self,\n",
        "        queries: List[str],\n",
        "        document_lists: List[List[str]],\n",
        "        top_k: int = 5\n",
        "    ) -> List[List[ColBERTResult]]:\n",
        "        \"\"\"æ‰¹é‡é‡æ’åº\"\"\"\n",
        "        if len(queries) != len(document_lists):\n",
        "            raise ValueError(\"æŸ¥è¯¢å’Œæ–‡æ¡£åˆ—è¡¨æ•°é‡ä¸åŒ¹é…\")\n",
        "\n",
        "        results = []\n",
        "        for query, docs in zip(queries, document_lists):\n",
        "            query_results = self.rerank(query, docs, top_k)\n",
        "            results.append(query_results)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def explain_ranking(\n",
        "        self,\n",
        "        query: str,\n",
        "        document: str\n",
        "    ) -> Dict[str, any]:\n",
        "        \"\"\"è§£é‡Šæ’åºç»“æœ\"\"\"\n",
        "        query_tokens = self.encoder.tokenize(query)\n",
        "        doc_tokens = self.encoder.tokenize(document)\n",
        "\n",
        "        query_embeddings = self.encode_query(query)\n",
        "        doc_embeddings = self.encode_document(document)\n",
        "\n",
        "        score, token_scores = self.compute_interaction_score(\n",
        "            query_embeddings, doc_embeddings\n",
        "        )\n",
        "\n",
        "        # æ‰¾åˆ°æ¯ä¸ªæŸ¥è¯¢tokençš„æœ€ä½³åŒ¹é…æ–‡æ¡£token\n",
        "        similarity_matrix = np.dot(query_embeddings, doc_embeddings.T)\n",
        "        best_matches = np.argmax(similarity_matrix, axis=1)\n",
        "\n",
        "        explanations = []\n",
        "        for i, (q_token, score) in enumerate(zip(query_tokens, token_scores)):\n",
        "            if i < len(best_matches):\n",
        "                best_doc_token = doc_tokens[best_matches[i]] if best_matches[i] < len(doc_tokens) else \"N/A\"\n",
        "                explanations.append({\n",
        "                    'query_token': q_token,\n",
        "                    'matched_doc_token': best_doc_token,\n",
        "                    'similarity_score': score\n",
        "                })\n",
        "\n",
        "        return {\n",
        "            'total_score': score,\n",
        "            'query_tokens': query_tokens,\n",
        "            'doc_tokens': doc_tokens,\n",
        "            'token_explanations': explanations,\n",
        "            'avg_token_score': np.mean(token_scores) if len(token_scores) > 0 else 0\n",
        "        }\n",
        "\n",
        "    def get_cache_stats(self) -> Dict[str, int]:\n",
        "        \"\"\"è·å–ç¼“å­˜ç»Ÿè®¡\"\"\"\n",
        "        return {\n",
        "            'cached_documents': len(self.cache),\n",
        "            'cache_size_mb': sum(emb.nbytes for emb in self.cache.values()) / (1024 * 1024)\n",
        "        }\n",
        "\n",
        "    def clear_cache(self) -> None:\n",
        "        \"\"\"æ¸…ç©ºç¼“å­˜\"\"\"\n",
        "        self.cache.clear()\n",
        "\n",
        "\n",
        "class ColBERTBatchProcessor:\n",
        "    \"\"\"ColBERTæ‰¹å¤„ç†å™¨\"\"\"\n",
        "\n",
        "    def __init__(self, reranker: ColBERTReRanker, batch_size: int = 32):\n",
        "        self.reranker = reranker\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def process_large_dataset(\n",
        "        self,\n",
        "        queries: List[str],\n",
        "        document_collection: List[str],\n",
        "        top_k_per_query: int = 10\n",
        "    ) -> List[List[ColBERTResult]]:\n",
        "        \"\"\"å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for i, query in enumerate(queries):\n",
        "            print(f\"å¤„ç†æŸ¥è¯¢ {i+1}/{len(queries)}: {query[:50]}...\")\n",
        "\n",
        "            # åˆ†æ‰¹å¤„ç†æ–‡æ¡£\n",
        "            query_results = []\n",
        "            for j in range(0, len(document_collection), self.batch_size):\n",
        "                batch_docs = document_collection[j:j + self.batch_size]\n",
        "                batch_doc_ids = [f\"doc_{j+k}\" for k in range(len(batch_docs))]\n",
        "\n",
        "                batch_results = self.reranker.rerank(\n",
        "                    query, batch_docs,\n",
        "                    top_k=len(batch_docs),  # å…ˆè·å–æ‰€æœ‰ç»“æœ\n",
        "                    doc_ids=batch_doc_ids\n",
        "                )\n",
        "                query_results.extend(batch_results)\n",
        "\n",
        "            # é‡æ–°æ’åºå¹¶å–top_k\n",
        "            query_results.sort(key=lambda x: x.score, reverse=True)\n",
        "            results.append(query_results[:top_k_per_query])\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "# ä½¿ç”¨ç¤ºä¾‹\n",
        "if __name__ == \"__main__\":\n",
        "    # åˆ›å»ºé‡æ’åºå™¨\n",
        "    reranker = ColBERTReRanker()\n",
        "\n",
        "    # æµ‹è¯•æ•°æ®\n",
        "    query = \"RAGæ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯\"\n",
        "    documents = [\n",
        "        \"RAGæ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„AIæŠ€æœ¯ï¼Œèƒ½å¤Ÿä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯\",\n",
        "        \"BERTæ˜¯è°·æ­Œå¼€å‘çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚\",\n",
        "        \"æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯é€šè¿‡å¤–éƒ¨çŸ¥è¯†æå‡è¯­è¨€æ¨¡å‹çš„å›ç­”è´¨é‡å’Œå‡†ç¡®æ€§\",\n",
        "        \"ColBERTæ˜¯ä¸€ç§é«˜æ•ˆçš„æ£€ç´¢æ¨¡å‹ï¼Œé‡‡ç”¨åæœŸäº¤äº’æ¶æ„\",\n",
        "        \"å‘é‡æ•°æ®åº“å­˜å‚¨æ–‡æ¡£çš„è¯­ä¹‰è¡¨ç¤ºï¼Œæ”¯æŒç›¸ä¼¼æ€§æœç´¢\"\n",
        "    ]\n",
        "\n",
        "    # æ‰§è¡Œé‡æ’åº\n",
        "    print(\"ğŸš€ ColBERTé‡æ’åºæµ‹è¯•\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    start_time = time.time()\n",
        "    results = reranker.rerank(query, documents, top_k=3)\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    print(f\"æŸ¥è¯¢: {query}\")\n",
        "    print(f\"å¤„ç†æ—¶é—´: {elapsed:.3f}s\")\n",
        "    print()\n",
        "\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"{i}. [å¾—åˆ†: {result.score:.3f}]\")\n",
        "        print(f\"   æ–‡æ¡£: {result.content}\")\n",
        "        print(f\"   Tokenå¹³å‡åˆ†: {np.mean(result.token_scores):.3f}\")\n",
        "        print()\n",
        "\n",
        "    # æ’åºè§£é‡Š\n",
        "    print(\"ğŸ“Š æ’åºè§£é‡Šï¼ˆTop 1ï¼‰:\")\n",
        "    explanation = reranker.explain_ranking(query, results[0].content)\n",
        "    print(f\"æ€»åˆ†: {explanation['total_score']:.3f}\")\n",
        "    print(f\"å¹³å‡Tokenåˆ†: {explanation['avg_token_score']:.3f}\")\n",
        "    print()\n",
        "    print(\"TokenåŒ¹é…è¯¦æƒ…:\")\n",
        "    for exp in explanation['token_explanations'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª\n",
        "        print(f\"  '{exp['query_token']}' -> '{exp['matched_doc_token']}' (åˆ†æ•°: {exp['similarity_score']:.3f})\")\n",
        "\n",
        "    # ç¼“å­˜ç»Ÿè®¡\n",
        "    cache_stats = reranker.get_cache_stats()\n",
        "    print(f\"\\nğŸ“ˆ ç¼“å­˜ç»Ÿè®¡:\")\n",
        "    print(f\"  ç¼“å­˜æ–‡æ¡£æ•°: {cache_stats['cached_documents']}\")\n",
        "    print(f\"  ç¼“å­˜å¤§å°: {cache_stats['cache_size_mb']:.2f}MB\")\n",
        "\n",
        "    # æ‰¹å¤„ç†æµ‹è¯•\n",
        "    print(\"\\nğŸ”„ æ‰¹å¤„ç†æµ‹è¯•:\")\n",
        "    batch_processor = ColBERTBatchProcessor(reranker, batch_size=3)\n",
        "\n",
        "    test_queries = [\"RAGæŠ€æœ¯\", \"BERTæ¨¡å‹\"]\n",
        "    batch_results = batch_processor.process_large_dataset(\n",
        "        test_queries, documents, top_k_per_query=2\n",
        "    )\n",
        "\n",
        "    for i, (query, results) in enumerate(zip(test_queries, batch_results)):\n",
        "        print(f\"\\næŸ¥è¯¢ {i+1}: {query}\")\n",
        "        for j, result in enumerate(results):\n",
        "            print(f\"  {j+1}. [åˆ†æ•°: {result.score:.3f}] {result.content[:40]}...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGN5u6RgN8mn",
        "outputId": "943a6ac1-5587-4781-b971-f62943168a4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ ColBERTé‡æ’åºæµ‹è¯•\n",
            "==================================================\n",
            "æŸ¥è¯¢: RAGæ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯\n",
            "å¤„ç†æ—¶é—´: 0.076s\n",
            "\n",
            "1. [å¾—åˆ†: 8.169]\n",
            "   æ–‡æ¡£: æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯é€šè¿‡å¤–éƒ¨çŸ¥è¯†æå‡è¯­è¨€æ¨¡å‹çš„å›ç­”è´¨é‡å’Œå‡†ç¡®æ€§\n",
            "   Tokenå¹³å‡åˆ†: 0.908\n",
            "\n",
            "2. [å¾—åˆ†: 7.354]\n",
            "   æ–‡æ¡£: RAGæ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„AIæŠ€æœ¯ï¼Œèƒ½å¤Ÿä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯\n",
            "   Tokenå¹³å‡åˆ†: 0.817\n",
            "\n",
            "3. [å¾—åˆ†: 3.362]\n",
            "   æ–‡æ¡£: ColBERTæ˜¯ä¸€ç§é«˜æ•ˆçš„æ£€ç´¢æ¨¡å‹ï¼Œé‡‡ç”¨åæœŸäº¤äº’æ¶æ„\n",
            "   Tokenå¹³å‡åˆ†: 0.374\n",
            "\n",
            "ğŸ“Š æ’åºè§£é‡Šï¼ˆTop 1ï¼‰:\n",
            "æ€»åˆ†: 1.000\n",
            "å¹³å‡Tokenåˆ†: 0.908\n",
            "\n",
            "TokenåŒ¹é…è¯¦æƒ…:\n",
            "  'rag' -> 'çŸ¥' (åˆ†æ•°: 0.169)\n",
            "  'æ£€' -> 'æ£€' (åˆ†æ•°: 1.000)\n",
            "  'ç´¢' -> 'ç´¢' (åˆ†æ•°: 1.000)\n",
            "  'å¢' -> 'å¢' (åˆ†æ•°: 1.000)\n",
            "  'å¼º' -> 'å¼º' (åˆ†æ•°: 1.000)\n",
            "\n",
            "ğŸ“ˆ ç¼“å­˜ç»Ÿè®¡:\n",
            "  ç¼“å­˜æ–‡æ¡£æ•°: 5\n",
            "  ç¼“å­˜å¤§å°: 0.12MB\n",
            "\n",
            "ğŸ”„ æ‰¹å¤„ç†æµ‹è¯•:\n",
            "å¤„ç†æŸ¥è¯¢ 1/2: RAGæŠ€æœ¯...\n",
            "å¤„ç†æŸ¥è¯¢ 2/2: BERTæ¨¡å‹...\n",
            "\n",
            "æŸ¥è¯¢ 1: RAGæŠ€æœ¯\n",
            "  1. [åˆ†æ•°: 3.000] RAGæ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„AIæŠ€æœ¯ï¼Œèƒ½å¤Ÿä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯...\n",
            "  2. [åˆ†æ•°: 2.169] æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯é€šè¿‡å¤–éƒ¨çŸ¥è¯†æå‡è¯­è¨€æ¨¡å‹çš„å›ç­”è´¨é‡å’Œå‡†ç¡®æ€§...\n",
            "\n",
            "æŸ¥è¯¢ 2: BERTæ¨¡å‹\n",
            "  1. [åˆ†æ•°: 3.000] BERTæ˜¯è°·æ­Œå¼€å‘çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚...\n",
            "  2. [åˆ†æ•°: 2.207] ColBERTæ˜¯ä¸€ç§é«˜æ•ˆçš„æ£€ç´¢æ¨¡å‹ï¼Œé‡‡ç”¨åæœŸäº¤äº’æ¶æ„...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CuL7o2WwN-v9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}