{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNDTaGFQXBiZei32URuS5Ih",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangyiyang/RAG-Cookbook-Code/blob/main/ch03/colbert_reranker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaxrWbNpM-xq",
        "outputId": "abad9949-366d-4eb8-d90b-70bd84f860b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install sentence-transformers torch transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ColBERT重排序系统\n",
        "实现精细化的Token级交互重排序算法\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "import time\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ColBERTResult:\n",
        "    \"\"\"ColBERT重排序结果\"\"\"\n",
        "    document_id: str\n",
        "    content: str\n",
        "    score: float\n",
        "    token_scores: List[float]\n",
        "    interaction_matrix: Optional[np.ndarray] = None\n",
        "\n",
        "\n",
        "class MockColBERTEncoder:\n",
        "    \"\"\"模拟的ColBERT编码器（实际应用中使用预训练模型）\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim: int = 128):\n",
        "        self.embedding_dim = embedding_dim\n",
        "        # 模拟词汇表\n",
        "        self.vocab = {}\n",
        "        self.vocab_size = 30000\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"简单的分词（实际应用中使用BERT分词器）\"\"\"\n",
        "        import re\n",
        "        # 中英文混合分词\n",
        "        tokens = re.findall(r'[\\u4e00-\\u9fff]|[a-zA-Z]+|\\d+', text.lower())\n",
        "        return tokens[:512]  # 限制最大长度\n",
        "\n",
        "    def encode_tokens(self, tokens: List[str]) -> np.ndarray:\n",
        "        \"\"\"编码tokens为向量（模拟实现）\"\"\"\n",
        "        embeddings = []\n",
        "        for token in tokens:\n",
        "            # 简单的hash编码模拟\n",
        "            token_hash = hash(token) % self.vocab_size\n",
        "            # 生成随机但一致的embedding\n",
        "            np.random.seed(token_hash)\n",
        "            embedding = np.random.normal(0, 1, self.embedding_dim)\n",
        "            embedding = embedding / np.linalg.norm(embedding)  # 归一化\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        return np.array(embeddings)\n",
        "\n",
        "\n",
        "class ColBERTReRanker:\n",
        "    \"\"\"ColBERT重排序器\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim: int = 128):\n",
        "        self.encoder = MockColBERTEncoder(embedding_dim)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.cache = {}  # 文档编码缓存\n",
        "\n",
        "    def encode_query(self, query: str) -> np.ndarray:\n",
        "        \"\"\"编码查询为token embeddings\"\"\"\n",
        "        tokens = self.encoder.tokenize(query)\n",
        "        if not tokens:\n",
        "            return np.array([]).reshape(0, self.embedding_dim)\n",
        "\n",
        "        embeddings = self.encoder.encode_tokens(tokens)\n",
        "        return embeddings\n",
        "\n",
        "    def encode_document(self, document: str, doc_id: str = None) -> np.ndarray:\n",
        "        \"\"\"编码文档为token embeddings（带缓存）\"\"\"\n",
        "        if doc_id and doc_id in self.cache:\n",
        "            return self.cache[doc_id]\n",
        "\n",
        "        tokens = self.encoder.tokenize(document)\n",
        "        if not tokens:\n",
        "            return np.array([]).reshape(0, self.embedding_dim)\n",
        "\n",
        "        embeddings = self.encoder.encode_tokens(tokens)\n",
        "\n",
        "        if doc_id:\n",
        "            self.cache[doc_id] = embeddings\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def compute_interaction_score(\n",
        "        self,\n",
        "        query_embeddings: np.ndarray,\n",
        "        doc_embeddings: np.ndarray\n",
        "    ) -> Tuple[float, np.ndarray]:\n",
        "        \"\"\"计算查询和文档的交互得分\"\"\"\n",
        "        if query_embeddings.size == 0 or doc_embeddings.size == 0:\n",
        "            return 0.0, np.array([])\n",
        "\n",
        "        # 计算相似度矩阵：每个查询token与每个文档token的相似度\n",
        "        similarity_matrix = np.dot(query_embeddings, doc_embeddings.T)\n",
        "\n",
        "        # ColBERT核心：每个查询token找最相似的文档token\n",
        "        max_similarities = np.max(similarity_matrix, axis=1)\n",
        "\n",
        "        # 总分数：所有查询token的最大相似度之和\n",
        "        total_score = np.sum(max_similarities)\n",
        "\n",
        "        return total_score, max_similarities\n",
        "\n",
        "    def rerank(\n",
        "        self,\n",
        "        query: str,\n",
        "        documents: List[str],\n",
        "        top_k: int = 5,\n",
        "        doc_ids: Optional[List[str]] = None\n",
        "    ) -> List[ColBERTResult]:\n",
        "        \"\"\"重排序文档\"\"\"\n",
        "        if not documents:\n",
        "            return []\n",
        "\n",
        "        # 编码查询\n",
        "        query_embeddings = self.encode_query(query)\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for i, document in enumerate(documents):\n",
        "            doc_id = doc_ids[i] if doc_ids else f\"doc_{i}\"\n",
        "\n",
        "            # 编码文档\n",
        "            doc_embeddings = self.encode_document(document, doc_id)\n",
        "\n",
        "            # 计算交互得分\n",
        "            score, token_scores = self.compute_interaction_score(\n",
        "                query_embeddings, doc_embeddings\n",
        "            )\n",
        "\n",
        "            result = ColBERTResult(\n",
        "                document_id=doc_id,\n",
        "                content=document,\n",
        "                score=score,\n",
        "                token_scores=token_scores.tolist()\n",
        "            )\n",
        "            results.append(result)\n",
        "\n",
        "        # 按分数排序\n",
        "        results.sort(key=lambda x: x.score, reverse=True)\n",
        "\n",
        "        return results[:top_k]\n",
        "\n",
        "    def batch_rerank(\n",
        "        self,\n",
        "        queries: List[str],\n",
        "        document_lists: List[List[str]],\n",
        "        top_k: int = 5\n",
        "    ) -> List[List[ColBERTResult]]:\n",
        "        \"\"\"批量重排序\"\"\"\n",
        "        if len(queries) != len(document_lists):\n",
        "            raise ValueError(\"查询和文档列表数量不匹配\")\n",
        "\n",
        "        results = []\n",
        "        for query, docs in zip(queries, document_lists):\n",
        "            query_results = self.rerank(query, docs, top_k)\n",
        "            results.append(query_results)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def explain_ranking(\n",
        "        self,\n",
        "        query: str,\n",
        "        document: str\n",
        "    ) -> Dict[str, any]:\n",
        "        \"\"\"解释排序结果\"\"\"\n",
        "        query_tokens = self.encoder.tokenize(query)\n",
        "        doc_tokens = self.encoder.tokenize(document)\n",
        "\n",
        "        query_embeddings = self.encode_query(query)\n",
        "        doc_embeddings = self.encode_document(document)\n",
        "\n",
        "        score, token_scores = self.compute_interaction_score(\n",
        "            query_embeddings, doc_embeddings\n",
        "        )\n",
        "\n",
        "        # 找到每个查询token的最佳匹配文档token\n",
        "        similarity_matrix = np.dot(query_embeddings, doc_embeddings.T)\n",
        "        best_matches = np.argmax(similarity_matrix, axis=1)\n",
        "\n",
        "        explanations = []\n",
        "        for i, (q_token, score) in enumerate(zip(query_tokens, token_scores)):\n",
        "            if i < len(best_matches):\n",
        "                best_doc_token = doc_tokens[best_matches[i]] if best_matches[i] < len(doc_tokens) else \"N/A\"\n",
        "                explanations.append({\n",
        "                    'query_token': q_token,\n",
        "                    'matched_doc_token': best_doc_token,\n",
        "                    'similarity_score': score\n",
        "                })\n",
        "\n",
        "        return {\n",
        "            'total_score': score,\n",
        "            'query_tokens': query_tokens,\n",
        "            'doc_tokens': doc_tokens,\n",
        "            'token_explanations': explanations,\n",
        "            'avg_token_score': np.mean(token_scores) if len(token_scores) > 0 else 0\n",
        "        }\n",
        "\n",
        "    def get_cache_stats(self) -> Dict[str, int]:\n",
        "        \"\"\"获取缓存统计\"\"\"\n",
        "        return {\n",
        "            'cached_documents': len(self.cache),\n",
        "            'cache_size_mb': sum(emb.nbytes for emb in self.cache.values()) / (1024 * 1024)\n",
        "        }\n",
        "\n",
        "    def clear_cache(self) -> None:\n",
        "        \"\"\"清空缓存\"\"\"\n",
        "        self.cache.clear()\n",
        "\n",
        "\n",
        "class ColBERTBatchProcessor:\n",
        "    \"\"\"ColBERT批处理器\"\"\"\n",
        "\n",
        "    def __init__(self, reranker: ColBERTReRanker, batch_size: int = 32):\n",
        "        self.reranker = reranker\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def process_large_dataset(\n",
        "        self,\n",
        "        queries: List[str],\n",
        "        document_collection: List[str],\n",
        "        top_k_per_query: int = 10\n",
        "    ) -> List[List[ColBERTResult]]:\n",
        "        \"\"\"处理大规模数据集\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for i, query in enumerate(queries):\n",
        "            print(f\"处理查询 {i+1}/{len(queries)}: {query[:50]}...\")\n",
        "\n",
        "            # 分批处理文档\n",
        "            query_results = []\n",
        "            for j in range(0, len(document_collection), self.batch_size):\n",
        "                batch_docs = document_collection[j:j + self.batch_size]\n",
        "                batch_doc_ids = [f\"doc_{j+k}\" for k in range(len(batch_docs))]\n",
        "\n",
        "                batch_results = self.reranker.rerank(\n",
        "                    query, batch_docs,\n",
        "                    top_k=len(batch_docs),  # 先获取所有结果\n",
        "                    doc_ids=batch_doc_ids\n",
        "                )\n",
        "                query_results.extend(batch_results)\n",
        "\n",
        "            # 重新排序并取top_k\n",
        "            query_results.sort(key=lambda x: x.score, reverse=True)\n",
        "            results.append(query_results[:top_k_per_query])\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "# 使用示例\n",
        "if __name__ == \"__main__\":\n",
        "    # 创建重排序器\n",
        "    reranker = ColBERTReRanker()\n",
        "\n",
        "    # 测试数据\n",
        "    query = \"RAG检索增强生成技术\"\n",
        "    documents = [\n",
        "        \"RAG是一种结合检索和生成的AI技术，能够从外部知识库中检索相关信息\",\n",
        "        \"BERT是谷歌开发的预训练语言模型，在自然语言处理任务中表现优异\",\n",
        "        \"检索增强生成技术通过外部知识提升语言模型的回答质量和准确性\",\n",
        "        \"ColBERT是一种高效的检索模型，采用后期交互架构\",\n",
        "        \"向量数据库存储文档的语义表示，支持相似性搜索\"\n",
        "    ]\n",
        "\n",
        "    # 执行重排序\n",
        "    print(\"🚀 ColBERT重排序测试\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    start_time = time.time()\n",
        "    results = reranker.rerank(query, documents, top_k=3)\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    print(f\"查询: {query}\")\n",
        "    print(f\"处理时间: {elapsed:.3f}s\")\n",
        "    print()\n",
        "\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"{i}. [得分: {result.score:.3f}]\")\n",
        "        print(f\"   文档: {result.content}\")\n",
        "        print(f\"   Token平均分: {np.mean(result.token_scores):.3f}\")\n",
        "        print()\n",
        "\n",
        "    # 排序解释\n",
        "    print(\"📊 排序解释（Top 1）:\")\n",
        "    explanation = reranker.explain_ranking(query, results[0].content)\n",
        "    print(f\"总分: {explanation['total_score']:.3f}\")\n",
        "    print(f\"平均Token分: {explanation['avg_token_score']:.3f}\")\n",
        "    print()\n",
        "    print(\"Token匹配详情:\")\n",
        "    for exp in explanation['token_explanations'][:5]:  # 只显示前5个\n",
        "        print(f\"  '{exp['query_token']}' -> '{exp['matched_doc_token']}' (分数: {exp['similarity_score']:.3f})\")\n",
        "\n",
        "    # 缓存统计\n",
        "    cache_stats = reranker.get_cache_stats()\n",
        "    print(f\"\\n📈 缓存统计:\")\n",
        "    print(f\"  缓存文档数: {cache_stats['cached_documents']}\")\n",
        "    print(f\"  缓存大小: {cache_stats['cache_size_mb']:.2f}MB\")\n",
        "\n",
        "    # 批处理测试\n",
        "    print(\"\\n🔄 批处理测试:\")\n",
        "    batch_processor = ColBERTBatchProcessor(reranker, batch_size=3)\n",
        "\n",
        "    test_queries = [\"RAG技术\", \"BERT模型\"]\n",
        "    batch_results = batch_processor.process_large_dataset(\n",
        "        test_queries, documents, top_k_per_query=2\n",
        "    )\n",
        "\n",
        "    for i, (query, results) in enumerate(zip(test_queries, batch_results)):\n",
        "        print(f\"\\n查询 {i+1}: {query}\")\n",
        "        for j, result in enumerate(results):\n",
        "            print(f\"  {j+1}. [分数: {result.score:.3f}] {result.content[:40]}...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGN5u6RgN8mn",
        "outputId": "943a6ac1-5587-4781-b971-f62943168a4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 ColBERT重排序测试\n",
            "==================================================\n",
            "查询: RAG检索增强生成技术\n",
            "处理时间: 0.076s\n",
            "\n",
            "1. [得分: 8.169]\n",
            "   文档: 检索增强生成技术通过外部知识提升语言模型的回答质量和准确性\n",
            "   Token平均分: 0.908\n",
            "\n",
            "2. [得分: 7.354]\n",
            "   文档: RAG是一种结合检索和生成的AI技术，能够从外部知识库中检索相关信息\n",
            "   Token平均分: 0.817\n",
            "\n",
            "3. [得分: 3.362]\n",
            "   文档: ColBERT是一种高效的检索模型，采用后期交互架构\n",
            "   Token平均分: 0.374\n",
            "\n",
            "📊 排序解释（Top 1）:\n",
            "总分: 1.000\n",
            "平均Token分: 0.908\n",
            "\n",
            "Token匹配详情:\n",
            "  'rag' -> '知' (分数: 0.169)\n",
            "  '检' -> '检' (分数: 1.000)\n",
            "  '索' -> '索' (分数: 1.000)\n",
            "  '增' -> '增' (分数: 1.000)\n",
            "  '强' -> '强' (分数: 1.000)\n",
            "\n",
            "📈 缓存统计:\n",
            "  缓存文档数: 5\n",
            "  缓存大小: 0.12MB\n",
            "\n",
            "🔄 批处理测试:\n",
            "处理查询 1/2: RAG技术...\n",
            "处理查询 2/2: BERT模型...\n",
            "\n",
            "查询 1: RAG技术\n",
            "  1. [分数: 3.000] RAG是一种结合检索和生成的AI技术，能够从外部知识库中检索相关信息...\n",
            "  2. [分数: 2.169] 检索增强生成技术通过外部知识提升语言模型的回答质量和准确性...\n",
            "\n",
            "查询 2: BERT模型\n",
            "  1. [分数: 3.000] BERT是谷歌开发的预训练语言模型，在自然语言处理任务中表现优异...\n",
            "  2. [分数: 2.207] ColBERT是一种高效的检索模型，采用后期交互架构...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CuL7o2WwN-v9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}