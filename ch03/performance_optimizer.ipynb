{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNRZ+U9X/I9eqQaQuIHmVW0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangyiyang/RAG-Cookbook-Code/blob/main/ch03/performance_optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjrJOl59Sndm",
        "outputId": "a21602df-03aa-42f3-bbbb-9264a28b523b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed faiss-cpu-1.11.0.post1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "%pip install sentence-transformers torch transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "性能优化与缓存策略\n",
        "提升检索系统的响应速度和吞吐量\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import hashlib\n",
        "import asyncio\n",
        "from typing import Dict, List, Any, Optional, Callable\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from dataclasses import dataclass, field\n",
        "from collections import OrderedDict\n",
        "import threading\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class CacheItem:\n",
        "    \"\"\"缓存项\"\"\"\n",
        "    value: Any\n",
        "    timestamp: float\n",
        "    access_count: int = 0\n",
        "    ttl: float = 3600  # 默认1小时过期\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PerformanceMetrics:\n",
        "    \"\"\"性能指标\"\"\"\n",
        "    total_requests: int = 0\n",
        "    cache_hits: int = 0\n",
        "    cache_misses: int = 0\n",
        "    avg_response_time: float = 0.0\n",
        "    total_response_time: float = 0.0\n",
        "\n",
        "    @property\n",
        "    def cache_hit_rate(self) -> float:\n",
        "        if self.total_requests == 0:\n",
        "            return 0.0\n",
        "        return self.cache_hits / self.total_requests\n",
        "\n",
        "    def update_response_time(self, response_time: float):\n",
        "        \"\"\"更新响应时间统计\"\"\"\n",
        "        self.total_response_time += response_time\n",
        "        self.total_requests += 1\n",
        "        self.avg_response_time = self.total_response_time / self.total_requests\n",
        "\n",
        "\n",
        "class LRUCache:\n",
        "    \"\"\"LRU缓存实现\"\"\"\n",
        "\n",
        "    def __init__(self, max_size: int = 1000, default_ttl: float = 3600):\n",
        "        self.max_size = max_size\n",
        "        self.default_ttl = default_ttl\n",
        "        self.cache: OrderedDict[str, CacheItem] = OrderedDict()\n",
        "        self.lock = threading.RLock()\n",
        "\n",
        "    def _generate_key(self, *args, **kwargs) -> str:\n",
        "        \"\"\"生成缓存键\"\"\"\n",
        "        key_str = str(args) + str(sorted(kwargs.items()))\n",
        "        return hashlib.md5(key_str.encode()).hexdigest()\n",
        "\n",
        "    def get(self, key: str) -> Optional[Any]:\n",
        "        \"\"\"获取缓存值\"\"\"\n",
        "        with self.lock:\n",
        "            if key not in self.cache:\n",
        "                return None\n",
        "\n",
        "            item = self.cache[key]\n",
        "\n",
        "            # 检查是否过期\n",
        "            if time.time() - item.timestamp > item.ttl:\n",
        "                del self.cache[key]\n",
        "                return None\n",
        "\n",
        "            # 更新访问统计\n",
        "            item.access_count += 1\n",
        "\n",
        "            # 移到最后（LRU策略）\n",
        "            self.cache.move_to_end(key)\n",
        "\n",
        "            return item.value\n",
        "\n",
        "    def put(self, key: str, value: Any, ttl: Optional[float] = None) -> None:\n",
        "        \"\"\"设置缓存值\"\"\"\n",
        "        with self.lock:\n",
        "            if ttl is None:\n",
        "                ttl = self.default_ttl\n",
        "\n",
        "            # 如果已存在，更新值\n",
        "            if key in self.cache:\n",
        "                self.cache[key].value = value\n",
        "                self.cache[key].timestamp = time.time()\n",
        "                self.cache[key].ttl = ttl\n",
        "                self.cache.move_to_end(key)\n",
        "                return\n",
        "\n",
        "            # 检查容量，删除最旧的项\n",
        "            while len(self.cache) >= self.max_size:\n",
        "                oldest_key = next(iter(self.cache))\n",
        "                del self.cache[oldest_key]\n",
        "\n",
        "            # 添加新项\n",
        "            self.cache[key] = CacheItem(\n",
        "                value=value,\n",
        "                timestamp=time.time(),\n",
        "                ttl=ttl\n",
        "            )\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"清空缓存\"\"\"\n",
        "        with self.lock:\n",
        "            self.cache.clear()\n",
        "\n",
        "    def size(self) -> int:\n",
        "        \"\"\"获取缓存大小\"\"\"\n",
        "        with self.lock:\n",
        "            return len(self.cache)\n",
        "\n",
        "    def cleanup_expired(self) -> int:\n",
        "        \"\"\"清理过期项\"\"\"\n",
        "        with self.lock:\n",
        "            current_time = time.time()\n",
        "            expired_keys = []\n",
        "\n",
        "            for key, item in self.cache.items():\n",
        "                if current_time - item.timestamp > item.ttl:\n",
        "                    expired_keys.append(key)\n",
        "\n",
        "            for key in expired_keys:\n",
        "                del self.cache[key]\n",
        "\n",
        "            return len(expired_keys)\n",
        "\n",
        "\n",
        "class QueryCache:\n",
        "    \"\"\"查询缓存管理器\"\"\"\n",
        "\n",
        "    def __init__(self, max_size: int = 1000):\n",
        "        self.query_cache = LRUCache(max_size, default_ttl=1800)  # 30分钟\n",
        "        self.embedding_cache = LRUCache(max_size * 2, default_ttl=7200)  # 2小时\n",
        "        self.result_cache = LRUCache(max_size // 2, default_ttl=600)  # 10分钟\n",
        "\n",
        "    def get_query_result(self, query: str, **kwargs) -> Optional[Any]:\n",
        "        \"\"\"获取查询结果缓存\"\"\"\n",
        "        key = self._generate_query_key(query, **kwargs)\n",
        "        return self.query_cache.get(key)\n",
        "\n",
        "    def cache_query_result(self, query: str, result: Any, **kwargs) -> None:\n",
        "        \"\"\"缓存查询结果\"\"\"\n",
        "        key = self._generate_query_key(query, **kwargs)\n",
        "        self.query_cache.put(key, result)\n",
        "\n",
        "    def get_embedding(self, text: str) -> Optional[Any]:\n",
        "        \"\"\"获取文本嵌入缓存\"\"\"\n",
        "        key = hashlib.md5(text.encode()).hexdigest()\n",
        "        return self.embedding_cache.get(key)\n",
        "\n",
        "    def cache_embedding(self, text: str, embedding: Any) -> None:\n",
        "        \"\"\"缓存文本嵌入\"\"\"\n",
        "        key = hashlib.md5(text.encode()).hexdigest()\n",
        "        self.embedding_cache.put(key, embedding, ttl=7200)\n",
        "\n",
        "    def _generate_query_key(self, query: str, **kwargs) -> str:\n",
        "        \"\"\"生成查询键\"\"\"\n",
        "        key_data = {'query': query, **kwargs}\n",
        "        key_str = str(sorted(key_data.items()))\n",
        "        return hashlib.md5(key_str.encode()).hexdigest()\n",
        "\n",
        "    def get_cache_stats(self) -> Dict[str, Dict]:\n",
        "        \"\"\"获取缓存统计\"\"\"\n",
        "        return {\n",
        "            'query_cache': {\n",
        "                'size': self.query_cache.size(),\n",
        "                'max_size': self.query_cache.max_size\n",
        "            },\n",
        "            'embedding_cache': {\n",
        "                'size': self.embedding_cache.size(),\n",
        "                'max_size': self.embedding_cache.max_size\n",
        "            },\n",
        "            'result_cache': {\n",
        "                'size': self.result_cache.size(),\n",
        "                'max_size': self.result_cache.max_size\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "class AsyncRetrievalPipeline:\n",
        "    \"\"\"异步检索管道\"\"\"\n",
        "\n",
        "    def __init__(self, max_workers: int = 10):\n",
        "        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n",
        "        self.semaphore = asyncio.Semaphore(max_workers)\n",
        "\n",
        "    async def parallel_retrieval(\n",
        "        self,\n",
        "        query: str,\n",
        "        retrievers: List[Callable],\n",
        "        top_k: int = 10\n",
        "    ) -> List[Any]:\n",
        "        \"\"\"并行执行多种检索策略\"\"\"\n",
        "\n",
        "        async def run_retriever(retriever_func):\n",
        "            async with self.semaphore:\n",
        "                loop = asyncio.get_event_loop()\n",
        "                return await loop.run_in_executor(\n",
        "                    self.executor,\n",
        "                    retriever_func,\n",
        "                    query,\n",
        "                    top_k\n",
        "                )\n",
        "\n",
        "        # 创建异步任务\n",
        "        tasks = [run_retriever(retriever) for retriever in retrievers]\n",
        "\n",
        "        # 并发执行\n",
        "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "        # 过滤异常结果\n",
        "        valid_results = [r for r in results if not isinstance(r, Exception)]\n",
        "\n",
        "        return valid_results\n",
        "\n",
        "    async def batch_process(\n",
        "        self,\n",
        "        queries: List[str],\n",
        "        process_func: Callable,\n",
        "        batch_size: int = 50\n",
        "    ) -> List[Any]:\n",
        "        \"\"\"批量处理查询\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for i in range(0, len(queries), batch_size):\n",
        "            batch = queries[i:i + batch_size]\n",
        "\n",
        "            # 创建批次任务\n",
        "            tasks = [\n",
        "                self._process_single_query(query, process_func)\n",
        "                for query in batch\n",
        "            ]\n",
        "\n",
        "            # 执行批次\n",
        "            batch_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "            results.extend(batch_results)\n",
        "\n",
        "        return results\n",
        "\n",
        "    async def _process_single_query(self, query: str, process_func: Callable):\n",
        "        \"\"\"处理单个查询\"\"\"\n",
        "        async with self.semaphore:\n",
        "            loop = asyncio.get_event_loop()\n",
        "            return await loop.run_in_executor(self.executor, process_func, query)\n",
        "\n",
        "\n",
        "class RetrievalOptimizer:\n",
        "    \"\"\"检索优化器\"\"\"\n",
        "\n",
        "    def __init__(self, cache_size: int = 1000):\n",
        "        self.cache = QueryCache(cache_size)\n",
        "        self.metrics = PerformanceMetrics()\n",
        "        self.async_pipeline = AsyncRetrievalPipeline()\n",
        "\n",
        "    def optimized_retrieval(\n",
        "        self,\n",
        "        query: str,\n",
        "        retrieval_func: Callable,\n",
        "        use_cache: bool = True,\n",
        "        **kwargs\n",
        "    ) -> Any:\n",
        "        \"\"\"优化的检索方法\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # 1. 尝试从缓存获取\n",
        "            if use_cache:\n",
        "                cached_result = self.cache.get_query_result(query, **kwargs)\n",
        "                if cached_result is not None:\n",
        "                    self.metrics.cache_hits += 1\n",
        "                    self.metrics.update_response_time(time.time() - start_time)\n",
        "                    return cached_result\n",
        "                else:\n",
        "                    self.metrics.cache_misses += 1\n",
        "\n",
        "            # 2. 执行检索\n",
        "            result = retrieval_func(query, **kwargs)\n",
        "\n",
        "            # 3. 缓存结果\n",
        "            if use_cache and result is not None:\n",
        "                self.cache.cache_query_result(query, result, **kwargs)\n",
        "\n",
        "            # 4. 更新性能指标\n",
        "            self.metrics.update_response_time(time.time() - start_time)\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            self.metrics.update_response_time(time.time() - start_time)\n",
        "            raise e\n",
        "\n",
        "    def get_embedding_with_cache(\n",
        "        self,\n",
        "        text: str,\n",
        "        embedding_func: Callable\n",
        "    ) -> Any:\n",
        "        \"\"\"带缓存的嵌入计算\"\"\"\n",
        "        # 检查缓存\n",
        "        cached_embedding = self.cache.get_embedding(text)\n",
        "        if cached_embedding is not None:\n",
        "            return cached_embedding\n",
        "\n",
        "        # 计算嵌入\n",
        "        embedding = embedding_func(text)\n",
        "\n",
        "        # 缓存结果\n",
        "        self.cache.cache_embedding(text, embedding)\n",
        "\n",
        "        return embedding\n",
        "\n",
        "    async def optimized_async_retrieval(\n",
        "        self,\n",
        "        query: str,\n",
        "        retrievers: List[Callable],\n",
        "        top_k: int = 10\n",
        "    ) -> List[Any]:\n",
        "        \"\"\"异步优化检索\"\"\"\n",
        "        return await self.async_pipeline.parallel_retrieval(query, retrievers, top_k)\n",
        "\n",
        "    def get_performance_report(self) -> Dict[str, Any]:\n",
        "        \"\"\"获取性能报告\"\"\"\n",
        "        cache_stats = self.cache.get_cache_stats()\n",
        "\n",
        "        return {\n",
        "            'metrics': {\n",
        "                'total_requests': self.metrics.total_requests,\n",
        "                'cache_hit_rate': f\"{self.metrics.cache_hit_rate:.1%}\",\n",
        "                'avg_response_time': f\"{self.metrics.avg_response_time:.3f}s\",\n",
        "                'cache_hits': self.metrics.cache_hits,\n",
        "                'cache_misses': self.metrics.cache_misses\n",
        "            },\n",
        "            'cache_stats': cache_stats,\n",
        "            'recommendations': self._generate_recommendations()\n",
        "        }\n",
        "\n",
        "    def _generate_recommendations(self) -> List[str]:\n",
        "        \"\"\"生成优化建议\"\"\"\n",
        "        recommendations = []\n",
        "\n",
        "        # 缓存命中率建议\n",
        "        if self.metrics.cache_hit_rate < 0.6:\n",
        "            recommendations.append(\"缓存命中率偏低，建议增加缓存容量或调整TTL\")\n",
        "\n",
        "        # 响应时间建议\n",
        "        if self.metrics.avg_response_time > 0.5:\n",
        "            recommendations.append(\"平均响应时间偏高，建议优化检索算法或增加并发\")\n",
        "\n",
        "        # 缓存容量建议\n",
        "        cache_stats = self.cache.get_cache_stats()\n",
        "        for cache_name, stats in cache_stats.items():\n",
        "            usage_rate = stats['size'] / stats['max_size']\n",
        "            if usage_rate > 0.9:\n",
        "                recommendations.append(f\"{cache_name}使用率过高，建议增加容量\")\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def cleanup_caches(self) -> Dict[str, int]:\n",
        "        \"\"\"清理过期缓存\"\"\"\n",
        "        results = {}\n",
        "        results['query_cache'] = self.cache.query_cache.cleanup_expired()\n",
        "        results['embedding_cache'] = self.cache.embedding_cache.cleanup_expired()\n",
        "        results['result_cache'] = self.cache.result_cache.cleanup_expired()\n",
        "        return results\n",
        "\n",
        "\n",
        "# 装饰器：自动缓存\n",
        "def cached_retrieval(cache_ttl: int = 1800):\n",
        "    \"\"\"检索缓存装饰器\"\"\"\n",
        "    def decorator(func):\n",
        "        cache = LRUCache(default_ttl=cache_ttl)\n",
        "\n",
        "        def wrapper(query: str, *args, **kwargs):\n",
        "            # 生成缓存键\n",
        "            key = cache._generate_key(query, *args, **kwargs)\n",
        "\n",
        "            # 尝试从缓存获取\n",
        "            result = cache.get(key)\n",
        "            if result is not None:\n",
        "                return result\n",
        "\n",
        "            # 执行函数\n",
        "            result = func(query, *args, **kwargs)\n",
        "\n",
        "            # 缓存结果\n",
        "            cache.put(key, result)\n",
        "\n",
        "            return result\n",
        "\n",
        "        wrapper.cache = cache\n",
        "        return wrapper\n",
        "\n",
        "    return decorator\n",
        "\n",
        "\n",
        "# 使用示例\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 模拟检索函数\n",
        "    def mock_retrieval(query: str, top_k: int = 10):\n",
        "        \"\"\"模拟检索函数\"\"\"\n",
        "        time.sleep(0.1)  # 模拟检索延迟\n",
        "        return [f\"文档{i}: 关于{query}的内容\" for i in range(top_k)]\n",
        "\n",
        "    def mock_embedding(text: str):\n",
        "        \"\"\"模拟嵌入函数\"\"\"\n",
        "        time.sleep(0.05)  # 模拟计算延迟\n",
        "        return [0.1, 0.2, 0.3] * 100  # 模拟向量\n",
        "\n",
        "    # 初始化优化器\n",
        "    optimizer = RetrievalOptimizer(cache_size=100)\n",
        "\n",
        "    # 测试缓存效果\n",
        "    queries = [\"RAG技术\", \"检索算法\", \"重排序\", \"RAG技术\"]  # 重复查询测试缓存\n",
        "\n",
        "    print(\"测试检索优化...\")\n",
        "    for query in queries:\n",
        "        start = time.time()\n",
        "        result = optimizer.optimized_retrieval(query, mock_retrieval, top_k=5)\n",
        "        duration = time.time() - start\n",
        "        print(f\"查询 '{query}' 耗时: {duration:.3f}s, 结果数: {len(result)}\")\n",
        "\n",
        "    # 测试嵌入缓存\n",
        "    print(\"\\n测试嵌入缓存...\")\n",
        "    texts = [\"RAG原理\", \"检索技术\", \"RAG原理\"]  # 重复文本测试缓存\n",
        "\n",
        "    for text in texts:\n",
        "        start = time.time()\n",
        "        embedding = optimizer.get_embedding_with_cache(text, mock_embedding)\n",
        "        duration = time.time() - start\n",
        "        print(f\"嵌入 '{text}' 耗时: {duration:.3f}s, 维度: {len(embedding)}\")\n",
        "\n",
        "    # 性能报告\n",
        "    print(\"\\n性能报告:\")\n",
        "    report = optimizer.get_performance_report()\n",
        "    for key, value in report['metrics'].items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    if report['recommendations']:\n",
        "        print(\"\\n优化建议:\")\n",
        "        for rec in report['recommendations']:\n",
        "            print(f\"  - {rec}\")\n",
        "\n",
        "    # 清理缓存\n",
        "    print(\"\\n清理过期缓存:\")\n",
        "    cleanup_results = optimizer.cleanup_caches()\n",
        "    for cache_name, count in cleanup_results.items():\n",
        "        print(f\"  {cache_name}: 清理了 {count} 个过期项\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfoI6cYZSs3e",
        "outputId": "e9260142-6c66-41a4-8b2a-58facf37a079"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "测试检索优化...\n",
            "查询 'RAG技术' 耗时: 0.100s, 结果数: 5\n",
            "查询 '检索算法' 耗时: 0.100s, 结果数: 5\n",
            "查询 '重排序' 耗时: 0.100s, 结果数: 5\n",
            "查询 'RAG技术' 耗时: 0.000s, 结果数: 5\n",
            "\n",
            "测试嵌入缓存...\n",
            "嵌入 'RAG原理' 耗时: 0.051s, 维度: 300\n",
            "嵌入 '检索技术' 耗时: 0.050s, 维度: 300\n",
            "嵌入 'RAG原理' 耗时: 0.000s, 维度: 300\n",
            "\n",
            "性能报告:\n",
            "  total_requests: 4\n",
            "  cache_hit_rate: 25.0%\n",
            "  avg_response_time: 0.075s\n",
            "  cache_hits: 1\n",
            "  cache_misses: 3\n",
            "\n",
            "优化建议:\n",
            "  - 缓存命中率偏低，建议增加缓存容量或调整TTL\n",
            "\n",
            "清理过期缓存:\n",
            "  query_cache: 清理了 0 个过期项\n",
            "  embedding_cache: 清理了 0 个过期项\n",
            "  result_cache: 清理了 0 个过期项\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xDfwktJIS0ua"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}