{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMQbWGvAX6qMYG/+qxq8uQN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangyiyang/RAG-Cookbook-Code/blob/main/ch03/evaluation_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-vP0f1XOq0l",
        "outputId": "d7fe7f2b-6a4e-498d-9b60-1132368f50c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m127.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed faiss-cpu-1.11.0.post1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "%pip install sentence-transformers torch transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "评估指标与效果监控\n",
        "提供全面的检索系统评估方法\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from typing import List, Dict, Set, Tuple, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "import math\n",
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class EvaluationResult:\n",
        "    \"\"\"评估结果\"\"\"\n",
        "    precision: float\n",
        "    recall: float\n",
        "    f1_score: float\n",
        "    ndcg: float\n",
        "    map_score: float  # Mean Average Precision\n",
        "    mrr: float       # Mean Reciprocal Rank\n",
        "    hit_rate: float\n",
        "    query_id: str = \"\"\n",
        "\n",
        "    def to_dict(self) -> Dict[str, float]:\n",
        "        return {\n",
        "            'precision': self.precision,\n",
        "            'recall': self.recall,\n",
        "            'f1_score': self.f1_score,\n",
        "            'ndcg': self.ndcg,\n",
        "            'map_score': self.map_score,\n",
        "            'mrr': self.mrr,\n",
        "            'hit_rate': self.hit_rate\n",
        "        }\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PerformanceStats:\n",
        "    \"\"\"性能统计\"\"\"\n",
        "    avg_response_time: float\n",
        "    p95_response_time: float\n",
        "    p99_response_time: float\n",
        "    throughput: float  # QPS\n",
        "    error_rate: float\n",
        "\n",
        "    def to_dict(self) -> Dict[str, float]:\n",
        "        return {\n",
        "            'avg_response_time': self.avg_response_time,\n",
        "            'p95_response_time': self.p95_response_time,\n",
        "            'p99_response_time': self.p99_response_time,\n",
        "            'throughput': self.throughput,\n",
        "            'error_rate': self.error_rate\n",
        "        }\n",
        "\n",
        "\n",
        "class RetrievalEvaluator:\n",
        "    \"\"\"检索系统评估器\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.response_times = []\n",
        "        self.error_count = 0\n",
        "        self.total_requests = 0\n",
        "\n",
        "    def evaluate_single_query(\n",
        "        self,\n",
        "        retrieved_docs: List[str],\n",
        "        relevant_docs: List[str],\n",
        "        query_id: str = \"\"\n",
        "    ) -> EvaluationResult:\n",
        "        \"\"\"评估单个查询的结果\"\"\"\n",
        "\n",
        "        retrieved_set = set(retrieved_docs)\n",
        "        relevant_set = set(relevant_docs)\n",
        "\n",
        "        # 计算基础指标\n",
        "        true_positives = len(retrieved_set & relevant_set)\n",
        "        precision = true_positives / len(retrieved_set) if retrieved_set else 0.0\n",
        "        recall = true_positives / len(relevant_set) if relevant_set else 0.0\n",
        "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "        # 计算NDCG\n",
        "        ndcg = self._calculate_ndcg(retrieved_docs, relevant_docs)\n",
        "\n",
        "        # 计算MAP\n",
        "        map_score = self._calculate_average_precision(retrieved_docs, relevant_docs)\n",
        "\n",
        "        # 计算MRR\n",
        "        mrr = self._calculate_reciprocal_rank(retrieved_docs, relevant_docs)\n",
        "\n",
        "        # 计算Hit Rate\n",
        "        hit_rate = 1.0 if true_positives > 0 else 0.0\n",
        "\n",
        "        return EvaluationResult(\n",
        "            precision=precision,\n",
        "            recall=recall,\n",
        "            f1_score=f1_score,\n",
        "            ndcg=ndcg,\n",
        "            map_score=map_score,\n",
        "            mrr=mrr,\n",
        "            hit_rate=hit_rate,\n",
        "            query_id=query_id\n",
        "        )\n",
        "\n",
        "    def evaluate_batch(\n",
        "        self,\n",
        "        queries: List[str],\n",
        "        retrieved_results: List[List[str]],\n",
        "        ground_truth: List[List[str]]\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"批量评估多个查询\"\"\"\n",
        "\n",
        "        if len(queries) != len(retrieved_results) != len(ground_truth):\n",
        "            raise ValueError(\"查询、检索结果和真实标签的数量不匹配\")\n",
        "\n",
        "        all_results = []\n",
        "\n",
        "        for i, (query, retrieved, relevant) in enumerate(zip(queries, retrieved_results, ground_truth)):\n",
        "            result = self.evaluate_single_query(retrieved, relevant, query_id=str(i))\n",
        "            all_results.append(result)\n",
        "\n",
        "        # 计算平均指标\n",
        "        avg_metrics = self._calculate_average_metrics(all_results)\n",
        "\n",
        "        return avg_metrics\n",
        "\n",
        "    def _calculate_ndcg(self, retrieved_docs: List[str], relevant_docs: List[str], k: int = 10) -> float:\n",
        "        \"\"\"计算NDCG@k\"\"\"\n",
        "        if not relevant_docs:\n",
        "            return 0.0\n",
        "\n",
        "        # 计算DCG\n",
        "        dcg = 0.0\n",
        "        relevant_set = set(relevant_docs)\n",
        "\n",
        "        for i, doc in enumerate(retrieved_docs[:k]):\n",
        "            if doc in relevant_set:\n",
        "                # 相关性得分（简化为1）\n",
        "                relevance = 1.0\n",
        "                dcg += relevance / math.log2(i + 2)  # i+2 因为log2(1)=0\n",
        "\n",
        "        # 计算IDCG（理想DCG）\n",
        "        idcg = 0.0\n",
        "        for i in range(min(len(relevant_docs), k)):\n",
        "            idcg += 1.0 / math.log2(i + 2)\n",
        "\n",
        "        return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "    def _calculate_average_precision(self, retrieved_docs: List[str], relevant_docs: List[str]) -> float:\n",
        "        \"\"\"计算Average Precision\"\"\"\n",
        "        if not relevant_docs:\n",
        "            return 0.0\n",
        "\n",
        "        relevant_set = set(relevant_docs)\n",
        "        num_relevant = 0\n",
        "        precision_sum = 0.0\n",
        "\n",
        "        for i, doc in enumerate(retrieved_docs):\n",
        "            if doc in relevant_set:\n",
        "                num_relevant += 1\n",
        "                precision_at_i = num_relevant / (i + 1)\n",
        "                precision_sum += precision_at_i\n",
        "\n",
        "        return precision_sum / len(relevant_docs) if relevant_docs else 0.0\n",
        "\n",
        "    def _calculate_reciprocal_rank(self, retrieved_docs: List[str], relevant_docs: List[str]) -> float:\n",
        "        \"\"\"计算Reciprocal Rank\"\"\"\n",
        "        relevant_set = set(relevant_docs)\n",
        "\n",
        "        for i, doc in enumerate(retrieved_docs):\n",
        "            if doc in relevant_set:\n",
        "                return 1.0 / (i + 1)\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    def _calculate_average_metrics(self, results: List[EvaluationResult]) -> Dict[str, float]:\n",
        "        \"\"\"计算平均指标\"\"\"\n",
        "        if not results:\n",
        "            return {}\n",
        "\n",
        "        metrics = {\n",
        "            'precision': np.mean([r.precision for r in results]),\n",
        "            'recall': np.mean([r.recall for r in results]),\n",
        "            'f1_score': np.mean([r.f1_score for r in results]),\n",
        "            'ndcg': np.mean([r.ndcg for r in results]),\n",
        "            'map': np.mean([r.map_score for r in results]),\n",
        "            'mrr': np.mean([r.mrr for r in results]),\n",
        "            'hit_rate': np.mean([r.hit_rate for r in results])\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def record_response_time(self, response_time: float) -> None:\n",
        "        \"\"\"记录响应时间\"\"\"\n",
        "        self.response_times.append(response_time)\n",
        "        self.total_requests += 1\n",
        "\n",
        "    def record_error(self) -> None:\n",
        "        \"\"\"记录错误\"\"\"\n",
        "        self.error_count += 1\n",
        "        self.total_requests += 1\n",
        "\n",
        "    def get_performance_stats(self, time_window: float = 60.0) -> PerformanceStats:\n",
        "        \"\"\"获取性能统计\"\"\"\n",
        "        if not self.response_times:\n",
        "            return PerformanceStats(0, 0, 0, 0, 0)\n",
        "\n",
        "        response_times = np.array(self.response_times)\n",
        "\n",
        "        avg_response_time = np.mean(response_times)\n",
        "        p95_response_time = np.percentile(response_times, 95)\n",
        "        p99_response_time = np.percentile(response_times, 99)\n",
        "\n",
        "        # 计算吞吐量（QPS）\n",
        "        throughput = len(self.response_times) / time_window if time_window > 0 else 0\n",
        "\n",
        "        # 计算错误率\n",
        "        error_rate = self.error_count / self.total_requests if self.total_requests > 0 else 0\n",
        "\n",
        "        return PerformanceStats(\n",
        "            avg_response_time=avg_response_time,\n",
        "            p95_response_time=p95_response_time,\n",
        "            p99_response_time=p99_response_time,\n",
        "            throughput=throughput,\n",
        "            error_rate=error_rate\n",
        "        )\n",
        "\n",
        "    def reset_stats(self) -> None:\n",
        "        \"\"\"重置统计信息\"\"\"\n",
        "        self.response_times = []\n",
        "        self.error_count = 0\n",
        "        self.total_requests = 0\n",
        "\n",
        "\n",
        "class ABTestEvaluator:\n",
        "    \"\"\"A/B测试评估器\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.group_a_evaluator = RetrievalEvaluator()\n",
        "        self.group_b_evaluator = RetrievalEvaluator()\n",
        "\n",
        "    def compare_systems(\n",
        "        self,\n",
        "        queries: List[str],\n",
        "        system_a_results: List[List[str]],\n",
        "        system_b_results: List[List[str]],\n",
        "        ground_truth: List[List[str]]\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"比较两个检索系统\"\"\"\n",
        "\n",
        "        # 评估系统A\n",
        "        metrics_a = self.group_a_evaluator.evaluate_batch(queries, system_a_results, ground_truth)\n",
        "\n",
        "        # 评估系统B\n",
        "        metrics_b = self.group_b_evaluator.evaluate_batch(queries, system_b_results, ground_truth)\n",
        "\n",
        "        # 计算改进百分比\n",
        "        improvements = {}\n",
        "        for metric in metrics_a:\n",
        "            if metrics_a[metric] > 0:\n",
        "                improvement = (metrics_b[metric] - metrics_a[metric]) / metrics_a[metric] * 100\n",
        "                improvements[metric] = improvement\n",
        "            else:\n",
        "                improvements[metric] = 0.0\n",
        "\n",
        "        # 统计学显著性检验（简化版）\n",
        "        significance_tests = self._simple_significance_test(\n",
        "            system_a_results, system_b_results, ground_truth\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'system_a_metrics': metrics_a,\n",
        "            'system_b_metrics': metrics_b,\n",
        "            'improvements': improvements,\n",
        "            'significance_tests': significance_tests,\n",
        "            'winner': self._determine_winner(metrics_a, metrics_b)\n",
        "        }\n",
        "\n",
        "    def _simple_significance_test(\n",
        "        self,\n",
        "        results_a: List[List[str]],\n",
        "        results_b: List[List[str]],\n",
        "        ground_truth: List[List[str]]\n",
        "    ) -> Dict[str, bool]:\n",
        "        \"\"\"简化的显著性检验\"\"\"\n",
        "        # 这里只做简单的差异检验，实际应用中需要更严格的统计检验\n",
        "\n",
        "        ndcg_a = []\n",
        "        ndcg_b = []\n",
        "\n",
        "        for i in range(len(results_a)):\n",
        "            ndcg_a.append(self.group_a_evaluator._calculate_ndcg(results_a[i], ground_truth[i]))\n",
        "            ndcg_b.append(self.group_b_evaluator._calculate_ndcg(results_b[i], ground_truth[i]))\n",
        "\n",
        "        # 简单的t检验逻辑（实际应用建议使用scipy.stats）\n",
        "        mean_diff = np.mean(ndcg_b) - np.mean(ndcg_a)\n",
        "        std_diff = np.std(np.array(ndcg_b) - np.array(ndcg_a))\n",
        "\n",
        "        # 简化的显著性判断\n",
        "        is_significant = abs(mean_diff) > 2 * std_diff / math.sqrt(len(ndcg_a))\n",
        "\n",
        "        return {\n",
        "            'ndcg_significant': is_significant,\n",
        "            'mean_difference': mean_diff,\n",
        "            'confidence': 0.95 if is_significant else 0.8\n",
        "        }\n",
        "\n",
        "    def _determine_winner(\n",
        "        self,\n",
        "        metrics_a: Dict[str, float],\n",
        "        metrics_b: Dict[str, float]\n",
        "    ) -> str:\n",
        "        \"\"\"确定获胜系统\"\"\"\n",
        "        key_metrics = ['ndcg', 'map', 'mrr', 'f1_score']\n",
        "\n",
        "        b_wins = 0\n",
        "        a_wins = 0\n",
        "\n",
        "        for metric in key_metrics:\n",
        "            if metric in metrics_a and metric in metrics_b:\n",
        "                if metrics_b[metric] > metrics_a[metric]:\n",
        "                    b_wins += 1\n",
        "                elif metrics_a[metric] > metrics_b[metric]:\n",
        "                    a_wins += 1\n",
        "\n",
        "        if b_wins > a_wins:\n",
        "            return \"System B\"\n",
        "        elif a_wins > b_wins:\n",
        "            return \"System A\"\n",
        "        else:\n",
        "            return \"Tie\"\n",
        "\n",
        "\n",
        "class QualityMonitor:\n",
        "    \"\"\"质量监控器\"\"\"\n",
        "\n",
        "    def __init__(self, alert_thresholds: Dict[str, float] = None):\n",
        "        self.thresholds = alert_thresholds or {\n",
        "            'precision': 0.8,\n",
        "            'recall': 0.7,\n",
        "            'ndcg': 0.75,\n",
        "            'response_time': 0.5,\n",
        "            'error_rate': 0.05\n",
        "        }\n",
        "        self.alerts = []\n",
        "        self.evaluator = RetrievalEvaluator()\n",
        "\n",
        "    def monitor_query(\n",
        "        self,\n",
        "        query: str,\n",
        "        retrieved_docs: List[str],\n",
        "        relevant_docs: List[str],\n",
        "        response_time: float\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"监控单个查询\"\"\"\n",
        "\n",
        "        # 记录性能\n",
        "        self.evaluator.record_response_time(response_time)\n",
        "\n",
        "        # 评估质量\n",
        "        result = self.evaluator.evaluate_single_query(retrieved_docs, relevant_docs, query)\n",
        "\n",
        "        # 检查告警\n",
        "        alerts = self._check_alerts(result, response_time)\n",
        "\n",
        "        return {\n",
        "            'query': query,\n",
        "            'metrics': result.to_dict(),\n",
        "            'response_time': response_time,\n",
        "            'alerts': alerts\n",
        "        }\n",
        "\n",
        "    def _check_alerts(self, result: EvaluationResult, response_time: float) -> List[str]:\n",
        "        \"\"\"检查告警条件\"\"\"\n",
        "        alerts = []\n",
        "\n",
        "        if result.precision < self.thresholds['precision']:\n",
        "            alerts.append(f\"精确率过低: {result.precision:.3f} < {self.thresholds['precision']}\")\n",
        "\n",
        "        if result.recall < self.thresholds['recall']:\n",
        "            alerts.append(f\"召回率过低: {result.recall:.3f} < {self.thresholds['recall']}\")\n",
        "\n",
        "        if result.ndcg < self.thresholds['ndcg']:\n",
        "            alerts.append(f\"NDCG过低: {result.ndcg:.3f} < {self.thresholds['ndcg']}\")\n",
        "\n",
        "        if response_time > self.thresholds['response_time']:\n",
        "            alerts.append(f\"响应时间过长: {response_time:.3f}s > {self.thresholds['response_time']}s\")\n",
        "\n",
        "        # 记录告警\n",
        "        for alert in alerts:\n",
        "            self.alerts.append({\n",
        "                'timestamp': time.time(),\n",
        "                'message': alert,\n",
        "                'severity': 'warning'\n",
        "            })\n",
        "\n",
        "        return alerts\n",
        "\n",
        "    def get_health_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"获取系统健康状态\"\"\"\n",
        "        perf_stats = self.evaluator.get_performance_stats()\n",
        "\n",
        "        recent_alerts = [\n",
        "            alert for alert in self.alerts\n",
        "            if time.time() - alert['timestamp'] < 3600  # 最近1小时\n",
        "        ]\n",
        "\n",
        "        health_score = self._calculate_health_score(perf_stats)\n",
        "\n",
        "        return {\n",
        "            'health_score': health_score,\n",
        "            'performance': perf_stats.to_dict(),\n",
        "            'recent_alerts': len(recent_alerts),\n",
        "            'alert_rate': len(recent_alerts) / 60,  # 每分钟告警数\n",
        "            'status': 'healthy' if health_score > 0.8 else 'warning' if health_score > 0.6 else 'critical'\n",
        "        }\n",
        "\n",
        "    def _calculate_health_score(self, perf_stats: PerformanceStats) -> float:\n",
        "        \"\"\"计算健康分数\"\"\"\n",
        "        score = 1.0\n",
        "\n",
        "        # 响应时间扣分\n",
        "        if perf_stats.avg_response_time > self.thresholds['response_time']:\n",
        "            score -= 0.2\n",
        "\n",
        "        # 错误率扣分\n",
        "        if perf_stats.error_rate > self.thresholds['error_rate']:\n",
        "            score -= 0.3\n",
        "\n",
        "        # 告警频率扣分\n",
        "        recent_alert_count = len([\n",
        "            a for a in self.alerts\n",
        "            if time.time() - a['timestamp'] < 3600\n",
        "        ])\n",
        "        if recent_alert_count > 10:\n",
        "            score -= 0.2\n",
        "\n",
        "        return max(0.0, score)\n",
        "\n",
        "\n",
        "# 使用示例\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 模拟数据\n",
        "    queries = [\"RAG技术\", \"检索算法\", \"重排序方法\"]\n",
        "\n",
        "    # 系统A的检索结果\n",
        "    system_a_results = [\n",
        "        [\"doc1\", \"doc2\", \"doc3\"],\n",
        "        [\"doc4\", \"doc5\", \"doc6\"],\n",
        "        [\"doc7\", \"doc8\", \"doc9\"]\n",
        "    ]\n",
        "\n",
        "    # 系统B的检索结果\n",
        "    system_b_results = [\n",
        "        [\"doc1\", \"doc3\", \"doc2\"],\n",
        "        [\"doc4\", \"doc6\", \"doc5\"],\n",
        "        [\"doc8\", \"doc7\", \"doc9\"]\n",
        "    ]\n",
        "\n",
        "    # 真实相关文档\n",
        "    ground_truth = [\n",
        "        [\"doc1\", \"doc2\"],\n",
        "        [\"doc4\", \"doc5\"],\n",
        "        [\"doc7\", \"doc8\"]\n",
        "    ]\n",
        "\n",
        "    # 单次评估\n",
        "    evaluator = RetrievalEvaluator()\n",
        "    result = evaluator.evaluate_single_query(\n",
        "        [\"doc1\", \"doc2\", \"doc3\"],\n",
        "        [\"doc1\", \"doc2\"],\n",
        "        \"query1\"\n",
        "    )\n",
        "    print(\"单次评估结果:\")\n",
        "    for metric, value in result.to_dict().items():\n",
        "        print(f\"  {metric}: {value:.3f}\")\n",
        "\n",
        "    # 批量评估\n",
        "    print(\"\\n批量评估结果:\")\n",
        "    batch_metrics = evaluator.evaluate_batch(queries, system_a_results, ground_truth)\n",
        "    for metric, value in batch_metrics.items():\n",
        "        print(f\"  {metric}: {value:.3f}\")\n",
        "\n",
        "    # A/B测试\n",
        "    print(\"\\nA/B测试结果:\")\n",
        "    ab_evaluator = ABTestEvaluator()\n",
        "    comparison = ab_evaluator.compare_systems(\n",
        "        queries, system_a_results, system_b_results, ground_truth\n",
        "    )\n",
        "\n",
        "    print(f\"获胜者: {comparison['winner']}\")\n",
        "    print(\"改进情况:\")\n",
        "    for metric, improvement in comparison['improvements'].items():\n",
        "        print(f\"  {metric}: {improvement:+.1f}%\")\n",
        "\n",
        "    # 质量监控\n",
        "    print(\"\\n质量监控:\")\n",
        "    monitor = QualityMonitor()\n",
        "\n",
        "    for i, query in enumerate(queries):\n",
        "        monitor_result = monitor.monitor_query(\n",
        "            query,\n",
        "            system_a_results[i],\n",
        "            ground_truth[i],\n",
        "            response_time=0.2 + i * 0.1\n",
        "        )\n",
        "\n",
        "        if monitor_result['alerts']:\n",
        "            print(f\"查询 '{query}' 触发告警:\")\n",
        "            for alert in monitor_result['alerts']:\n",
        "                print(f\"  - {alert}\")\n",
        "\n",
        "    # 健康状态\n",
        "    health = monitor.get_health_status()\n",
        "    print(f\"\\n系统健康状态: {health['status']}\")\n",
        "    print(f\"健康分数: {health['health_score']:.2f}\")\n",
        "    print(f\"最近告警数: {health['recent_alerts']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrTIU-K8O7JA",
        "outputId": "5df85ed9-e5e7-4323-8523-071883315f83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "单次评估结果:\n",
            "  precision: 0.667\n",
            "  recall: 1.000\n",
            "  f1_score: 0.800\n",
            "  ndcg: 1.000\n",
            "  map_score: 1.000\n",
            "  mrr: 1.000\n",
            "  hit_rate: 1.000\n",
            "\n",
            "批量评估结果:\n",
            "  precision: 0.667\n",
            "  recall: 1.000\n",
            "  f1_score: 0.800\n",
            "  ndcg: 1.000\n",
            "  map: 1.000\n",
            "  mrr: 1.000\n",
            "  hit_rate: 1.000\n",
            "\n",
            "A/B测试结果:\n",
            "获胜者: System A\n",
            "改进情况:\n",
            "  precision: +0.0%\n",
            "  recall: +0.0%\n",
            "  f1_score: +0.0%\n",
            "  ndcg: -5.4%\n",
            "  map: -11.1%\n",
            "  mrr: +0.0%\n",
            "  hit_rate: +0.0%\n",
            "\n",
            "质量监控:\n",
            "查询 'RAG技术' 触发告警:\n",
            "  - 精确率过低: 0.667 < 0.8\n",
            "查询 '检索算法' 触发告警:\n",
            "  - 精确率过低: 0.667 < 0.8\n",
            "查询 '重排序方法' 触发告警:\n",
            "  - 精确率过低: 0.667 < 0.8\n",
            "\n",
            "系统健康状态: healthy\n",
            "健康分数: 1.00\n",
            "最近告警数: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zKn1O8MFPU89"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}