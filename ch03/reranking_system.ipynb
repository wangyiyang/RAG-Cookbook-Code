{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZDWq+xRMYfIcgtImZKor3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangyiyang/RAG-Cookbook-Code/blob/main/ch03/reranking_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4beKWDExUwJt",
        "outputId": "b4480189-21bf-43b5-dc97-818df4f889bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed faiss-cpu-1.11.0.post1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "%pip install sentence-transformers torch transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "重排序算法系统\n",
        "包含Cross-Encoder、ColBERT等重排序方法\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import re\n",
        "\n",
        "\n",
        "def simple_chinese_tokenizer(text: str) -> List[str]:\n",
        "    \"\"\"简单的中英文分词器\"\"\"\n",
        "    # 分离中英文和标点符号\n",
        "    pattern = r'[a-zA-Z]+|[0-9]+|[\\u4e00-\\u9fff]'\n",
        "    tokens = re.findall(pattern, text.lower())\n",
        "    return tokens\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RerankResult:\n",
        "    \"\"\"重排序结果\"\"\"\n",
        "    doc_id: str\n",
        "    content: str\n",
        "    original_score: float\n",
        "    rerank_score: float\n",
        "    final_rank: int\n",
        "\n",
        "\n",
        "class MockCrossEncoder:\n",
        "    \"\"\"模拟Cross-Encoder（实际使用sentence-transformers库）\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"):\n",
        "        self.model_name = model_name\n",
        "        self.max_length = 512\n",
        "\n",
        "    def predict(self, query_doc_pairs: List[Tuple[str, str]]) -> List[float]:\n",
        "        \"\"\"预测查询-文档对的相关性分数\"\"\"\n",
        "        scores = []\n",
        "        for query, doc in query_doc_pairs:\n",
        "            # 模拟相关性计算（实际使用预训练模型）\n",
        "            score = self._mock_relevance_score(query, doc)\n",
        "            scores.append(score)\n",
        "        return scores\n",
        "\n",
        "    def _mock_relevance_score(self, query: str, doc: str) -> float:\n",
        "        \"\"\"模拟相关性分数计算\"\"\"\n",
        "        # 使用改进的分词器\n",
        "        query_words = set(simple_chinese_tokenizer(query))\n",
        "        doc_words = set(simple_chinese_tokenizer(doc))\n",
        "\n",
        "        if len(query_words) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        overlap_ratio = len(query_words & doc_words) / len(query_words)\n",
        "\n",
        "        # 计算精确匹配关键词的权重（重要改进）\n",
        "        exact_match_bonus = 0.0\n",
        "        important_matches = 0\n",
        "        for q_word in query_words:\n",
        "            if q_word.lower() in [w.lower() for w in doc_words]:\n",
        "                # 重要关键词（如RAG）给予更高权重\n",
        "                if len(q_word) >= 2:  # 降低长度要求，适应中文\n",
        "                    if q_word.lower() == 'rag':  # RAG是核心关键词\n",
        "                        exact_match_bonus += 0.4\n",
        "                        important_matches += 1\n",
        "                    elif q_word in ['检索', '技术', '原理']:  # 其他重要词\n",
        "                        exact_match_bonus += 0.2\n",
        "                        important_matches += 1\n",
        "                    else:\n",
        "                        exact_match_bonus += 0.1\n",
        "\n",
        "        # 考虑文档长度因子\n",
        "        length_factor = min(1.0, max(0.3, len(simple_chinese_tokenizer(doc)) / 30))\n",
        "\n",
        "        # 模拟语义相似度（基于内容相关性）\n",
        "        semantic_factor = 0.3\n",
        "        # 技术词汇匹配奖励\n",
        "        tech_words = {'技术', '算法', '检索', '生成', '嵌入', '相似度', '匹配'}\n",
        "        query_tech = len(set(simple_chinese_tokenizer(query)) & tech_words)\n",
        "        doc_tech = len(set(simple_chinese_tokenizer(doc)) & tech_words)\n",
        "        if query_tech > 0 and doc_tech > 0:\n",
        "            semantic_factor += min(0.2, (query_tech + doc_tech) * 0.05)\n",
        "\n",
        "        # 主题相关性（根据文档内容判断）\n",
        "        if 'rag' in doc.lower():\n",
        "            semantic_factor += 0.3  # RAG相关内容\n",
        "        elif '检索' in doc and '算法' in doc:\n",
        "            semantic_factor += 0.2  # 检索算法相关\n",
        "        elif '向量' in doc and '检索' in doc:\n",
        "            semantic_factor += 0.15  # 向量检索相关\n",
        "\n",
        "        # 最终分数计算\n",
        "        base_score = 0.1\n",
        "        final_score = base_score + (overlap_ratio * 0.2 + exact_match_bonus + length_factor * 0.15 + semantic_factor)\n",
        "\n",
        "        return min(1.0, final_score)\n",
        "\n",
        "\n",
        "class CrossEncoderReRanker:\n",
        "    \"\"\"Cross-Encoder重排序器\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"):\n",
        "        self.model = MockCrossEncoder(model_name)\n",
        "        self.max_length = 512\n",
        "\n",
        "    def rerank(self, query: str, documents: List[Dict], top_k: int = 5) -> List[RerankResult]:\n",
        "        \"\"\"执行重排序\"\"\"\n",
        "        if not documents:\n",
        "            return []\n",
        "\n",
        "        # 1. 构建查询-文档对\n",
        "        query_doc_pairs = []\n",
        "        for doc in documents:\n",
        "            content = doc.get('content', str(doc))\n",
        "            # 智能截断\n",
        "            truncated_content = self._intelligent_truncate(content, query)\n",
        "            query_doc_pairs.append((query, truncated_content))\n",
        "\n",
        "        # 2. 批量计算相关性分数\n",
        "        rerank_scores = self.model.predict(query_doc_pairs)\n",
        "\n",
        "        # 3. 构建结果并排序\n",
        "        results = []\n",
        "        for i, (doc, score) in enumerate(zip(documents, rerank_scores)):\n",
        "            result = RerankResult(\n",
        "                doc_id=doc.get('doc_id', str(i)),\n",
        "                content=doc.get('content', str(doc)),\n",
        "                original_score=doc.get('score', 0.0),\n",
        "                rerank_score=score,\n",
        "                final_rank=0  # 将在排序后设置\n",
        "            )\n",
        "            results.append(result)\n",
        "\n",
        "        # 4. 按重排序分数排序\n",
        "        results.sort(key=lambda x: x.rerank_score, reverse=True)\n",
        "\n",
        "        # 5. 设置最终排名\n",
        "        for rank, result in enumerate(results[:top_k], 1):\n",
        "            result.final_rank = rank\n",
        "\n",
        "        return results[:top_k]\n",
        "\n",
        "    def _intelligent_truncate(self, content: str, query: str, max_tokens: int = 400) -> str:\n",
        "        \"\"\"智能截断策略：保留最相关的部分\"\"\"\n",
        "        if len(content.split()) <= max_tokens:\n",
        "            return content\n",
        "\n",
        "        # 分句处理\n",
        "        sentences = self._split_sentences(content)\n",
        "        if not sentences:\n",
        "            return content[:max_tokens * 4]  # 估算字符数\n",
        "\n",
        "        # 计算每个句子的相关性\n",
        "        sentence_scores = []\n",
        "        query_words = set(simple_chinese_tokenizer(query))\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_words = set(simple_chinese_tokenizer(sentence))\n",
        "            # 词汇重叠度\n",
        "            overlap = len(query_words & sentence_words) / max(len(query_words), 1)\n",
        "            # 句子长度因子（偏向中等长度句子）\n",
        "            length_factor = max(0.1, 1 - abs(len(simple_chinese_tokenizer(sentence)) - 10) / 20)\n",
        "            score = overlap * length_factor\n",
        "            sentence_scores.append((sentence, score))\n",
        "\n",
        "        # 选择最相关的句子\n",
        "        sentence_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        selected_sentences = []\n",
        "        current_length = 0\n",
        "\n",
        "        for sentence, score in sentence_scores:\n",
        "            sentence_length = len(simple_chinese_tokenizer(sentence))\n",
        "            if current_length + sentence_length <= max_tokens:\n",
        "                selected_sentences.append(sentence)\n",
        "                current_length += sentence_length\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # 保持原文顺序\n",
        "        result_sentences = []\n",
        "        for sentence in sentences:\n",
        "            if sentence in selected_sentences:\n",
        "                result_sentences.append(sentence)\n",
        "\n",
        "        return '。'.join(result_sentences) if result_sentences else content[:max_tokens * 4]\n",
        "\n",
        "    def _split_sentences(self, text: str) -> List[str]:\n",
        "        \"\"\"分句处理\"\"\"\n",
        "        # 中文分句\n",
        "        sentences = re.split(r'[。！？；]', text)\n",
        "        # 过滤空句子和过短句子\n",
        "        sentences = [s.strip() for s in sentences if len(s.strip()) > 5]\n",
        "        return sentences\n",
        "\n",
        "\n",
        "class ColBERTReRanker:\n",
        "    \"\"\"ColBERT重排序器（模拟实现）\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"colbert-ir/colbertv2.0\"):\n",
        "        self.model_name = model_name\n",
        "        self.dim = 128  # ColBERT向量维度\n",
        "\n",
        "    def rerank(self, query: str, documents: List[Dict], top_k: int = 5) -> List[RerankResult]:\n",
        "        \"\"\"ColBERT重排序\"\"\"\n",
        "        if not documents:\n",
        "            return []\n",
        "\n",
        "        # 1. 查询编码\n",
        "        query_embeddings = self._encode_query(query)\n",
        "\n",
        "        results = []\n",
        "        for i, doc in enumerate(documents):\n",
        "            content = doc.get('content', str(doc))\n",
        "\n",
        "            # 2. 文档编码\n",
        "            doc_embeddings = self._encode_document(content)\n",
        "\n",
        "            # 3. 计算ColBERT分数\n",
        "            score = self._compute_colbert_score(query_embeddings, doc_embeddings)\n",
        "\n",
        "            result = RerankResult(\n",
        "                doc_id=doc.get('doc_id', str(i)),\n",
        "                content=content,\n",
        "                original_score=doc.get('score', 0.0),\n",
        "                rerank_score=score,\n",
        "                final_rank=0\n",
        "            )\n",
        "            results.append(result)\n",
        "\n",
        "        # 4. 排序\n",
        "        results.sort(key=lambda x: x.rerank_score, reverse=True)\n",
        "\n",
        "        for rank, result in enumerate(results[:top_k], 1):\n",
        "            result.final_rank = rank\n",
        "\n",
        "        return results[:top_k]\n",
        "\n",
        "    def _encode_query(self, query: str) -> np.ndarray:\n",
        "        \"\"\"编码查询（模拟）\"\"\"\n",
        "        tokens = simple_chinese_tokenizer(query)\n",
        "        # 模拟每个token的向量表示\n",
        "        embeddings = []\n",
        "        for i, token in enumerate(tokens):\n",
        "            # 基于token生成确定性向量，重要词汇特殊处理\n",
        "            base_seed = abs(hash(token)) % (2**31)\n",
        "\n",
        "            # 为重要关键词（如RAG）生成更具区分性的向量\n",
        "            if token.lower() == 'rag':\n",
        "                # RAG使用特殊的向量表示\n",
        "                np.random.seed(base_seed)\n",
        "                embedding = np.random.randn(self.dim) * 1.5  # 增强幅度\n",
        "            elif token in ['检索', '技术', '原理']:\n",
        "                # 其他重要词汇也给予特殊处理\n",
        "                np.random.seed(base_seed + 1000)\n",
        "                embedding = np.random.randn(self.dim) * 1.2\n",
        "            else:\n",
        "                np.random.seed(base_seed)\n",
        "                embedding = np.random.randn(self.dim)\n",
        "\n",
        "            embedding = embedding / np.linalg.norm(embedding)  # 单位化\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        return np.array(embeddings)  # [query_len, dim]\n",
        "\n",
        "    def _encode_document(self, document: str) -> np.ndarray:\n",
        "        \"\"\"编码文档（模拟）\"\"\"\n",
        "        tokens = simple_chinese_tokenizer(document)[:50]  # 限制长度\n",
        "        embeddings = []\n",
        "\n",
        "        for token in tokens:\n",
        "            base_seed = abs(hash(token)) % (2**31)\n",
        "\n",
        "            # 为重要关键词生成特殊向量\n",
        "            if token.lower() == 'rag':\n",
        "                # RAG词汇使用与查询中相同的种子，增加相似度\n",
        "                np.random.seed(base_seed)\n",
        "                embedding = np.random.randn(self.dim) * 1.5\n",
        "            elif token in ['检索', '技术', '算法', '生成', '嵌入']:\n",
        "                np.random.seed(base_seed + 1000)\n",
        "                embedding = np.random.randn(self.dim) * 1.2\n",
        "            else:\n",
        "                np.random.seed(base_seed)\n",
        "                embedding = np.random.randn(self.dim)\n",
        "\n",
        "            embedding = embedding / np.linalg.norm(embedding)\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        return np.array(embeddings)  # [doc_len, dim]\n",
        "\n",
        "    def _compute_colbert_score(self, query_embs: np.ndarray, doc_embs: np.ndarray) -> float:\n",
        "        \"\"\"计算ColBERT分数：MaxSim操作\"\"\"\n",
        "        if len(query_embs) == 0 or len(doc_embs) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # 计算相似度矩阵\n",
        "        similarity_matrix = np.dot(query_embs, doc_embs.T)  # [query_len, doc_len]\n",
        "\n",
        "        # 每个查询token找到最相似的文档token\n",
        "        max_similarities = np.max(similarity_matrix, axis=1)  # [query_len]\n",
        "\n",
        "        # 加权求和：重要token获得更高权重\n",
        "        query_tokens = query_embs  # 获取查询token信息\n",
        "        weighted_score = 0.0\n",
        "        total_weight = 0.0\n",
        "\n",
        "        for i, sim in enumerate(max_similarities):\n",
        "            # 为重要位置的token分配更高权重\n",
        "            if i < len(query_embs):  # 确保索引有效\n",
        "                # 这里我们可以根据token的重要性来分配权重\n",
        "                # 假设第一个token（通常是关键词）更重要\n",
        "                weight = 2.0 if i == 0 else 1.0  # 简单的权重分配\n",
        "                weighted_score += sim * weight\n",
        "                total_weight += weight\n",
        "\n",
        "        if total_weight == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # 计算加权平均分数\n",
        "        avg_score = weighted_score / total_weight\n",
        "\n",
        "        # 转换到[0, 1]范围：(score + 1) / 2，并增加正向偏置\n",
        "        final_score = (avg_score + 1) / 2\n",
        "\n",
        "        # 如果有强匹配（分数>0.8），给予额外奖励\n",
        "        if final_score > 0.8:\n",
        "            final_score = min(1.0, final_score * 1.1)\n",
        "\n",
        "        return max(0.0, min(1.0, final_score))\n",
        "\n",
        "\n",
        "class LearningToRankReRanker:\n",
        "    \"\"\"Learning-to-Rank重排序器\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.feature_weights = {\n",
        "            'bm25_score': 0.3,\n",
        "            'semantic_score': 0.4,\n",
        "            'length_factor': 0.1,\n",
        "            'position_factor': 0.2\n",
        "        }\n",
        "\n",
        "    def rerank(self, query: str, documents: List[Dict], top_k: int = 5) -> List[RerankResult]:\n",
        "        \"\"\"基于特征的重排序\"\"\"\n",
        "        if not documents:\n",
        "            return []\n",
        "\n",
        "        results = []\n",
        "        for i, doc in enumerate(documents):\n",
        "            content = doc.get('content', str(doc))\n",
        "\n",
        "            # 提取特征\n",
        "            features = self._extract_features(query, content, i)\n",
        "\n",
        "            # 计算综合分数\n",
        "            score = sum(\n",
        "                features[feature] * weight\n",
        "                for feature, weight in self.feature_weights.items()\n",
        "                if feature in features\n",
        "            )\n",
        "\n",
        "            result = RerankResult(\n",
        "                doc_id=doc.get('doc_id', str(i)),\n",
        "                content=content,\n",
        "                original_score=doc.get('score', 0.0),\n",
        "                rerank_score=score,\n",
        "                final_rank=0\n",
        "            )\n",
        "            results.append(result)\n",
        "\n",
        "        # 排序\n",
        "        results.sort(key=lambda x: x.rerank_score, reverse=True)\n",
        "\n",
        "        for rank, result in enumerate(results[:top_k], 1):\n",
        "            result.final_rank = rank\n",
        "\n",
        "        return results[:top_k]\n",
        "\n",
        "    def _extract_features(self, query: str, document: str, position: int) -> Dict[str, float]:\n",
        "        \"\"\"提取特征\"\"\"\n",
        "        query_words = set(simple_chinese_tokenizer(query))\n",
        "        doc_words = simple_chinese_tokenizer(document)\n",
        "        doc_words_set = set(doc_words)\n",
        "\n",
        "        # BM25相似特征（改进的词汇重叠计算）\n",
        "        bm25_score = len(query_words & doc_words_set) / max(len(query_words), 1)\n",
        "\n",
        "        # 精确匹配奖励\n",
        "        exact_match_bonus = 0.0\n",
        "        for q_word in query_words:\n",
        "            if q_word in doc_words_set and len(q_word) >= 2:  # 适应中文\n",
        "                exact_match_bonus += 0.15\n",
        "\n",
        "        # 语义相似度特征（基于内容相关性）\n",
        "        semantic_score = 0.4  # 基础语义分数\n",
        "        tech_words = {'技术', '算法', '检索', '生成', '嵌入', '相似度', '匹配', 'rag', 'bm25'}\n",
        "        query_tech = len(set(simple_chinese_tokenizer(query)) & tech_words)\n",
        "        doc_tech = len(set(simple_chinese_tokenizer(document)) & tech_words)\n",
        "        if query_tech > 0 and doc_tech > 0:\n",
        "            semantic_score += min(0.4, (query_tech + doc_tech) * 0.1)\n",
        "\n",
        "        # 文档长度特征\n",
        "        length_factor = min(1.0, len(doc_words) / 100)\n",
        "\n",
        "        # 位置特征（早期结果有优势）\n",
        "        position_factor = 1.0 / (1 + position * 0.1)\n",
        "\n",
        "        return {\n",
        "            'bm25_score': bm25_score + exact_match_bonus,\n",
        "            'semantic_score': semantic_score,\n",
        "            'length_factor': length_factor,\n",
        "            'position_factor': position_factor\n",
        "        }\n",
        "\n",
        "\n",
        "class ReRankingSystem:\n",
        "    \"\"\"重排序系统集成类\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.cross_encoder = CrossEncoderReRanker()\n",
        "        self.colbert = ColBERTReRanker()\n",
        "        self.learning_to_rank = LearningToRankReRanker()\n",
        "\n",
        "    def rerank(\n",
        "        self,\n",
        "        query: str,\n",
        "        documents: List[Dict],\n",
        "        method: str = \"cross_encoder\",\n",
        "        top_k: int = 5\n",
        "    ) -> List[RerankResult]:\n",
        "        \"\"\"统一重排序接口\"\"\"\n",
        "\n",
        "        if method == \"cross_encoder\":\n",
        "            return self.cross_encoder.rerank(query, documents, top_k)\n",
        "        elif method == \"colbert\":\n",
        "            return self.colbert.rerank(query, documents, top_k)\n",
        "        elif method == \"learning_to_rank\":\n",
        "            return self.learning_to_rank.rerank(query, documents, top_k)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported reranking method: {method}\")\n",
        "\n",
        "    def compare_methods(\n",
        "        self,\n",
        "        query: str,\n",
        "        documents: List[Dict],\n",
        "        top_k: int = 5\n",
        "    ) -> Dict[str, List[RerankResult]]:\n",
        "        \"\"\"比较不同重排序方法\"\"\"\n",
        "        methods = [\"cross_encoder\", \"colbert\", \"learning_to_rank\"]\n",
        "        results = {}\n",
        "\n",
        "        for method in methods:\n",
        "            try:\n",
        "                results[method] = self.rerank(query, documents, method, top_k)\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {method}: {e}\")\n",
        "                results[method] = []\n",
        "\n",
        "        return results\n",
        "\n",
        "    def analyze_ranking_decisions(\n",
        "        self,\n",
        "        query: str,\n",
        "        documents: List[Dict],\n",
        "        top_k: int = 3\n",
        "    ) -> None:\n",
        "        \"\"\"分析不同重排序方法的决策过程\"\"\"\n",
        "        print(f\"=== 重排序决策分析 ===\")\n",
        "        print(f\"查询: {query}\\n\")\n",
        "\n",
        "        # 获取各方法的排序结果\n",
        "        results = self.compare_methods(query, documents, top_k)\n",
        "\n",
        "        # 分析每个文档的特征\n",
        "        print(\"文档特征分析:\")\n",
        "        for i, doc in enumerate(documents):\n",
        "            content = doc.get('content', '')\n",
        "            doc_words = set(simple_chinese_tokenizer(content))\n",
        "            query_words = set(simple_chinese_tokenizer(query))\n",
        "\n",
        "            print(f\"查询分词: {list(query_words)}\")  # 调试信息\n",
        "            print(f\"文档{i+1}分词: {list(doc_words)[:10]}...\")  # 显示前10个词\n",
        "\n",
        "            overlap = len(query_words & doc_words)\n",
        "            overlap_words = query_words & doc_words\n",
        "            print(f\"文档{i+1}: {content[:30]}...\")\n",
        "            print(f\"  - 词汇重叠: {overlap}/{len(query_words)} = {overlap/len(query_words):.2f}\")\n",
        "            print(f\"  - 重叠词汇: {list(overlap_words)}\")\n",
        "            print(f\"  - 包含'RAG': {'RAG' in content}\")\n",
        "            print(f\"  - 包含'检索': {'检索' in content}\")\n",
        "            print(f\"  - 包含'技术': {'技术' in content}\")\n",
        "            print()\n",
        "\n",
        "        # 显示排序结果对比\n",
        "        print(\"排序结果对比:\")\n",
        "        for method, method_results in results.items():\n",
        "            print(f\"{method.upper()}:\")\n",
        "            for result in method_results:\n",
        "                print(f\"  排名{result.final_rank}: [分数: {result.rerank_score:.3f}] 文档{result.doc_id}\")\n",
        "\n",
        "        print(\"\\n=== 分析总结 ===\")\n",
        "        rag_rankings = {}\n",
        "        for method, method_results in results.items():\n",
        "            for result in method_results:\n",
        "                if 'RAG' in result.content:\n",
        "                    rag_rankings[method] = result.final_rank\n",
        "                    break\n",
        "\n",
        "        print(\"RAG文档在各算法中的排名:\")\n",
        "        for method, rank in rag_rankings.items():\n",
        "            status = \"✅\" if rank == 1 else \"❌\"\n",
        "            print(f\"  {method}: 第{rank}名 {status}\")\n",
        "\n",
        "# 使用示例\n",
        "if __name__ == \"__main__\":\n",
        "    # 模拟文档数据\n",
        "    documents = [\n",
        "        {\n",
        "            \"doc_id\": \"1\",\n",
        "            \"content\": \"RAG是检索增强生成技术，它结合了信息检索和自然语言生成的优势\",\n",
        "            \"score\": 0.8\n",
        "        },\n",
        "        {\n",
        "            \"doc_id\": \"2\",\n",
        "            \"content\": \"BM25是一种经典的信息检索算法，基于TF-IDF的改进版本\",\n",
        "            \"score\": 0.6\n",
        "        },\n",
        "        {\n",
        "            \"doc_id\": \"3\",\n",
        "            \"content\": \"向量检索通过语义嵌入实现文档的相似度计算和匹配\",\n",
        "            \"score\": 0.7\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # 初始化重排序系统\n",
        "    rerank_system = ReRankingSystem()\n",
        "\n",
        "    query = \"RAG检索技术原理\"\n",
        "\n",
        "    # 比较不同方法\n",
        "    results = rerank_system.compare_methods(query, documents, top_k=3)\n",
        "\n",
        "    print(f\"查询: {query}\\n\")\n",
        "\n",
        "    for method, method_results in results.items():\n",
        "        print(f\"{method.upper()} 重排序结果:\")\n",
        "        for result in method_results:\n",
        "            print(f\"  排名{result.final_rank}: [分数: {result.rerank_score:.3f}] {result.content[:50]}...\")\n",
        "        print()\n",
        "\n",
        "    # 添加详细分析\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    rerank_system.analyze_ranking_decisions(query, documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVJOTlWvUzq-",
        "outputId": "2fa2654f-59cd-4ffd-fd13-d8b00d307e6f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "查询: RAG检索技术原理\n",
            "\n",
            "CROSS_ENCODER 重排序结果:\n",
            "  排名1: [分数: 1.000] RAG是检索增强生成技术，它结合了信息检索和自然语言生成的优势...\n",
            "  排名2: [分数: 0.772] BM25是一种经典的信息检索算法，基于TF-IDF的改进版本...\n",
            "  排名3: [分数: 0.722] 向量检索通过语义嵌入实现文档的相似度计算和匹配...\n",
            "\n",
            "COLBERT 重排序结果:\n",
            "  排名1: [分数: 0.985] RAG是检索增强生成技术，它结合了信息检索和自然语言生成的优势...\n",
            "  排名2: [分数: 0.700] 向量检索通过语义嵌入实现文档的相似度计算和匹配...\n",
            "  排名3: [分数: 0.687] BM25是一种经典的信息检索算法，基于TF-IDF的改进版本...\n",
            "\n",
            "LEARNING_TO_RANK 重排序结果:\n",
            "  排名1: [分数: 0.727] RAG是检索增强生成技术，它结合了信息检索和自然语言生成的优势...\n",
            "  排名2: [分数: 0.451] BM25是一种经典的信息检索算法，基于TF-IDF的改进版本...\n",
            "  排名3: [分数: 0.435] 向量检索通过语义嵌入实现文档的相似度计算和匹配...\n",
            "\n",
            "\n",
            "============================================================\n",
            "=== 重排序决策分析 ===\n",
            "查询: RAG检索技术原理\n",
            "\n",
            "文档特征分析:\n",
            "查询分词: ['理', 'rag', '术', '检', '索', '原', '技']\n",
            "文档1分词: ['强', '语', '自', '成', '息', '然', '索', '优', '生', '言']...\n",
            "文档1: RAG是检索增强生成技术，它结合了信息检索和自然语言生成的优...\n",
            "  - 词汇重叠: 5/7 = 0.71\n",
            "  - 重叠词汇: ['rag', '术', '检', '索', '技']\n",
            "  - 包含'RAG': True\n",
            "  - 包含'检索': True\n",
            "  - 包含'技术': True\n",
            "\n",
            "查询分词: ['理', 'rag', '术', '检', '索', '原', '技']\n",
            "文档2分词: ['一', '算', '于', '息', 'bm', '索', '种', '25', '基', '是']...\n",
            "文档2: BM25是一种经典的信息检索算法，基于TF-IDF的改进版本...\n",
            "  - 词汇重叠: 2/7 = 0.29\n",
            "  - 重叠词汇: ['检', '索']\n",
            "  - 包含'RAG': False\n",
            "  - 包含'检索': True\n",
            "  - 包含'技术': False\n",
            "\n",
            "查询分词: ['理', 'rag', '术', '检', '索', '原', '技']\n",
            "文档3分词: ['现', '匹', '语', '算', '相', '索', '度', '档', '文', '似']...\n",
            "文档3: 向量检索通过语义嵌入实现文档的相似度计算和匹配...\n",
            "  - 词汇重叠: 2/7 = 0.29\n",
            "  - 重叠词汇: ['检', '索']\n",
            "  - 包含'RAG': False\n",
            "  - 包含'检索': True\n",
            "  - 包含'技术': False\n",
            "\n",
            "排序结果对比:\n",
            "CROSS_ENCODER:\n",
            "  排名1: [分数: 1.000] 文档1\n",
            "  排名2: [分数: 0.772] 文档2\n",
            "  排名3: [分数: 0.722] 文档3\n",
            "COLBERT:\n",
            "  排名1: [分数: 0.985] 文档1\n",
            "  排名2: [分数: 0.700] 文档3\n",
            "  排名3: [分数: 0.687] 文档2\n",
            "LEARNING_TO_RANK:\n",
            "  排名1: [分数: 0.727] 文档1\n",
            "  排名2: [分数: 0.451] 文档2\n",
            "  排名3: [分数: 0.435] 文档3\n",
            "\n",
            "=== 分析总结 ===\n",
            "RAG文档在各算法中的排名:\n",
            "  cross_encoder: 第1名 ✅\n",
            "  colbert: 第1名 ✅\n",
            "  learning_to_rank: 第1名 ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YA98y_2GU18L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}