{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqjwhBERONhyR4qhXvbb91",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangyiyang/RAG-Cookbook-Code/blob/main/ch03/hybrid_retriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7UW1JzaQXy8",
        "outputId": "e0fe8a0b-1425-4403-cb51-30bc8ba54aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed faiss-cpu-1.11.0.post1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "%pip install sentence-transformers torch transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "混合检索策略实现\n",
        "结合关键词检索和语义检索的优势\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Any\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SearchResult:\n",
        "    \"\"\"检索结果\"\"\"\n",
        "    doc_id: str\n",
        "    content: str\n",
        "    score: float\n",
        "    metadata: Dict[str, Any]\n",
        "    source: str  # 'semantic', 'keyword', 'hybrid'\n",
        "\n",
        "\n",
        "class BM25Retriever:\n",
        "    \"\"\"BM25关键词检索器\"\"\"\n",
        "\n",
        "    def __init__(self, k1: float = 1.5, b: float = 0.75):\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.documents = []\n",
        "        self.doc_freqs = []\n",
        "        self.idf_cache = {}\n",
        "        self.avgdl = 0\n",
        "\n",
        "    def fit(self, documents: List[str]):\n",
        "        \"\"\"构建BM25索引\"\"\"\n",
        "        self.documents = documents\n",
        "        self.doc_freqs = []\n",
        "\n",
        "        # 计算词频和文档长度\n",
        "        total_length = 0\n",
        "        word_doc_count = defaultdict(int)\n",
        "\n",
        "        for doc in documents:\n",
        "            words = doc.lower().split()\n",
        "            total_length += len(words)\n",
        "\n",
        "            word_freq = defaultdict(int)\n",
        "            unique_words = set()\n",
        "\n",
        "            for word in words:\n",
        "                word_freq[word] += 1\n",
        "                unique_words.add(word)\n",
        "\n",
        "            # 计算每个唯一词的文档频率\n",
        "            for word in unique_words:\n",
        "                word_doc_count[word] += 1\n",
        "\n",
        "            self.doc_freqs.append(word_freq)\n",
        "\n",
        "        self.avgdl = total_length / len(documents)\n",
        "\n",
        "        # 计算IDF\n",
        "        N = len(documents)\n",
        "        for word, df in word_doc_count.items():\n",
        "            self.idf_cache[word] = math.log((N - df + 0.5) / (df + 0.5))\n",
        "\n",
        "    def search(self, query: str, top_k: int = 10) -> List[SearchResult]:\n",
        "        \"\"\"BM25检索\"\"\"\n",
        "        query_words = query.lower().split()\n",
        "        scores = []\n",
        "\n",
        "        for doc_idx, doc_freq in enumerate(self.doc_freqs):\n",
        "            score = 0.0\n",
        "            doc_len = sum(doc_freq.values())\n",
        "\n",
        "            for word in query_words:\n",
        "                if word in doc_freq:\n",
        "                    tf = doc_freq[word]\n",
        "                    idf = self.idf_cache.get(word, 0)\n",
        "\n",
        "                    # BM25公式\n",
        "                    numerator = tf * (self.k1 + 1)\n",
        "                    denominator = tf + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)\n",
        "                    score += idf * (numerator / denominator)\n",
        "\n",
        "            scores.append((doc_idx, score))\n",
        "\n",
        "        # 排序并返回top-k\n",
        "        scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        results = []\n",
        "        for doc_idx, score in scores[:top_k]:\n",
        "            results.append(SearchResult(\n",
        "                doc_id=str(doc_idx),\n",
        "                content=self.documents[doc_idx],\n",
        "                score=score,\n",
        "                metadata={'doc_index': doc_idx},\n",
        "                source='keyword'\n",
        "            ))\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "class DenseRetriever:\n",
        "    \"\"\"密集检索器（模拟）\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim: int = 768):\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.document_embeddings = []\n",
        "        self.documents = []\n",
        "\n",
        "    def fit(self, documents: List[str]):\n",
        "        \"\"\"构建向量索引\"\"\"\n",
        "        self.documents = documents\n",
        "        # 模拟文档向量化（实际应用中使用预训练模型）\n",
        "        self.document_embeddings = [\n",
        "            self._mock_embed(doc) for doc in documents\n",
        "        ]\n",
        "\n",
        "    def _mock_embed(self, text: str) -> np.ndarray:\n",
        "        \"\"\"模拟文本嵌入（实际使用sentence-transformers）\"\"\"\n",
        "        # 基于文本哈希生成模拟向量\n",
        "        hash_val = hash(text) % (2**31)\n",
        "        np.random.seed(hash_val)\n",
        "        return np.random.randn(self.embedding_dim)\n",
        "\n",
        "    def search(self, query: str, top_k: int = 10) -> List[SearchResult]:\n",
        "        \"\"\"向量相似度检索\"\"\"\n",
        "        query_embedding = self._mock_embed(query)\n",
        "\n",
        "        # 计算余弦相似度\n",
        "        similarities = []\n",
        "        for doc_idx, doc_embedding in enumerate(self.document_embeddings):\n",
        "            similarity = np.dot(query_embedding, doc_embedding) / (\n",
        "                np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)\n",
        "            )\n",
        "            similarities.append((doc_idx, similarity))\n",
        "\n",
        "        # 排序并返回top-k\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        results = []\n",
        "        for doc_idx, similarity in similarities[:top_k]:\n",
        "            results.append(SearchResult(\n",
        "                doc_id=str(doc_idx),\n",
        "                content=self.documents[doc_idx],\n",
        "                score=similarity,\n",
        "                metadata={'doc_index': doc_idx},\n",
        "                source='semantic'\n",
        "            ))\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "class AdaptiveWeightManager:\n",
        "    \"\"\"自适应权重管理器\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.query_patterns = {\n",
        "            'exact_match': 0.3,      # 精确匹配查询，偏向关键词检索\n",
        "            'conceptual': 0.8,       # 概念性查询，偏向语义检索\n",
        "            'mixed': 0.5,            # 混合查询，平衡权重\n",
        "            'technical': 0.6         # 技术查询，稍偏向语义检索\n",
        "        }\n",
        "\n",
        "    def get_optimal_weight(self, query: str) -> float:\n",
        "        \"\"\"根据查询特征动态调整权重\"\"\"\n",
        "        query_type = self._classify_query_pattern(query)\n",
        "        return self.query_patterns.get(query_type, 0.7)\n",
        "\n",
        "    def _classify_query_pattern(self, query: str) -> str:\n",
        "        \"\"\"分类查询模式\"\"\"\n",
        "        query = query.lower()\n",
        "\n",
        "        # 精确匹配模式\n",
        "        if '\"' in query or '精确' in query or '具体' in query:\n",
        "            return 'exact_match'\n",
        "\n",
        "        # 概念性查询\n",
        "        concept_keywords = ['原理', '概念', '理论', '什么是', '如何理解']\n",
        "        if any(keyword in query for keyword in concept_keywords):\n",
        "            return 'conceptual'\n",
        "\n",
        "        # 技术查询\n",
        "        tech_keywords = ['实现', '算法', '代码', '技术', '方法']\n",
        "        if any(keyword in query for keyword in tech_keywords):\n",
        "            return 'technical'\n",
        "\n",
        "        return 'mixed'\n",
        "\n",
        "\n",
        "class HybridRetriever:\n",
        "    \"\"\"混合检索器主类\"\"\"\n",
        "\n",
        "    def __init__(self, documents: List[str]):\n",
        "        self.documents = documents\n",
        "        self.bm25_retriever = BM25Retriever()\n",
        "        self.dense_retriever = DenseRetriever()\n",
        "        self.weight_manager = AdaptiveWeightManager()\n",
        "\n",
        "        # 构建索引\n",
        "        self.bm25_retriever.fit(documents)\n",
        "        self.dense_retriever.fit(documents)\n",
        "\n",
        "    def search(self, query: str, top_k: int = 10, alpha: float = None) -> List[SearchResult]:\n",
        "        \"\"\"混合检索主方法\"\"\"\n",
        "        # 自适应权重\n",
        "        if alpha is None:\n",
        "            alpha = self.weight_manager.get_optimal_weight(query)\n",
        "\n",
        "        # 并行执行双路检索\n",
        "        semantic_results = self.dense_retriever.search(query, top_k=top_k * 2)\n",
        "        keyword_results = self.bm25_retriever.search(query, top_k=top_k * 2)\n",
        "\n",
        "        # 结果融合\n",
        "        combined_results = self.reciprocal_rank_fusion(\n",
        "            semantic_results,\n",
        "            keyword_results,\n",
        "            alpha\n",
        "        )\n",
        "\n",
        "        return combined_results[:top_k]\n",
        "\n",
        "    def reciprocal_rank_fusion(\n",
        "        self,\n",
        "        semantic_results: List[SearchResult],\n",
        "        keyword_results: List[SearchResult],\n",
        "        alpha: float,\n",
        "        k: int = 60\n",
        "    ) -> List[SearchResult]:\n",
        "        \"\"\"倒数排名融合算法（RRF）\"\"\"\n",
        "        combined_scores = {}\n",
        "        doc_objects = {}\n",
        "\n",
        "        # 语义检索分数\n",
        "        for rank, result in enumerate(semantic_results):\n",
        "            doc_id = result.doc_id\n",
        "            rrf_score = alpha / (k + rank + 1)\n",
        "            combined_scores[doc_id] = rrf_score\n",
        "            doc_objects[doc_id] = result\n",
        "\n",
        "        # 关键词检索分数\n",
        "        for rank, result in enumerate(keyword_results):\n",
        "            doc_id = result.doc_id\n",
        "            rrf_score = (1 - alpha) / (k + rank + 1)\n",
        "\n",
        "            if doc_id in combined_scores:\n",
        "                combined_scores[doc_id] += rrf_score\n",
        "            else:\n",
        "                combined_scores[doc_id] = rrf_score\n",
        "                doc_objects[doc_id] = result\n",
        "\n",
        "        # 按分数排序\n",
        "        sorted_items = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # 构建最终结果\n",
        "        final_results = []\n",
        "        for doc_id, score in sorted_items:\n",
        "            result = doc_objects[doc_id]\n",
        "            final_results.append(SearchResult(\n",
        "                doc_id=result.doc_id,\n",
        "                content=result.content,\n",
        "                score=score,\n",
        "                metadata=result.metadata,\n",
        "                source='hybrid'\n",
        "            ))\n",
        "\n",
        "        return final_results\n",
        "\n",
        "    def evaluate_fusion_strategies(\n",
        "        self,\n",
        "        query: str,\n",
        "        strategies: List[str] = None\n",
        "    ) -> Dict[str, List[SearchResult]]:\n",
        "        \"\"\"评估不同融合策略的效果\"\"\"\n",
        "        if strategies is None:\n",
        "            strategies = ['rrf', 'weighted_sum', 'comb_mnz']\n",
        "\n",
        "        semantic_results = self.dense_retriever.search(query, top_k=20)\n",
        "        keyword_results = self.bm25_retriever.search(query, top_k=20)\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for strategy in strategies:\n",
        "            if strategy == 'rrf':\n",
        "                results[strategy] = self.reciprocal_rank_fusion(\n",
        "                    semantic_results, keyword_results, 0.7\n",
        "                )[:10]\n",
        "            elif strategy == 'weighted_sum':\n",
        "                results[strategy] = self._weighted_sum_fusion(\n",
        "                    semantic_results, keyword_results, 0.7\n",
        "                )[:10]\n",
        "            elif strategy == 'comb_mnz':\n",
        "                results[strategy] = self._comb_mnz_fusion(\n",
        "                    semantic_results, keyword_results\n",
        "                )[:10]\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _weighted_sum_fusion(\n",
        "        self,\n",
        "        semantic_results: List[SearchResult],\n",
        "        keyword_results: List[SearchResult],\n",
        "        alpha: float\n",
        "    ) -> List[SearchResult]:\n",
        "        \"\"\"加权和融合策略\"\"\"\n",
        "        combined_scores = {}\n",
        "        doc_objects = {}\n",
        "\n",
        "        # 归一化分数\n",
        "        max_semantic = max((r.score for r in semantic_results), default=1) if semantic_results else 1\n",
        "        max_keyword = max((r.score for r in keyword_results), default=1) if keyword_results else 1\n",
        "\n",
        "        # 防止除零错误\n",
        "        if max_semantic == 0:\n",
        "            max_semantic = 1\n",
        "        if max_keyword == 0:\n",
        "            max_keyword = 1\n",
        "\n",
        "        # 语义检索分数\n",
        "        for result in semantic_results:\n",
        "            doc_id = result.doc_id\n",
        "            normalized_score = result.score / max_semantic\n",
        "            combined_scores[doc_id] = alpha * normalized_score\n",
        "            doc_objects[doc_id] = result\n",
        "\n",
        "        # 关键词检索分数\n",
        "        for result in keyword_results:\n",
        "            doc_id = result.doc_id\n",
        "            normalized_score = result.score / max_keyword\n",
        "\n",
        "            if doc_id in combined_scores:\n",
        "                combined_scores[doc_id] += (1 - alpha) * normalized_score\n",
        "            else:\n",
        "                combined_scores[doc_id] = (1 - alpha) * normalized_score\n",
        "                doc_objects[doc_id] = result\n",
        "\n",
        "        # 排序\n",
        "        sorted_items = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        final_results = []\n",
        "        for doc_id, score in sorted_items:\n",
        "            result = doc_objects[doc_id]\n",
        "            final_results.append(SearchResult(\n",
        "                doc_id=result.doc_id,\n",
        "                content=result.content,\n",
        "                score=score,\n",
        "                metadata=result.metadata,\n",
        "                source='hybrid'\n",
        "            ))\n",
        "\n",
        "        return final_results\n",
        "\n",
        "    def _comb_mnz_fusion(\n",
        "        self,\n",
        "        semantic_results: List[SearchResult],\n",
        "        keyword_results: List[SearchResult]\n",
        "    ) -> List[SearchResult]:\n",
        "        \"\"\"CombMNZ融合策略\"\"\"\n",
        "        combined_scores = {}\n",
        "        doc_objects = {}\n",
        "        match_counts = defaultdict(int)\n",
        "\n",
        "        # 收集所有结果\n",
        "        for result in semantic_results:\n",
        "            doc_id = result.doc_id\n",
        "            combined_scores[doc_id] = result.score\n",
        "            doc_objects[doc_id] = result\n",
        "            match_counts[doc_id] += 1\n",
        "\n",
        "        for result in keyword_results:\n",
        "            doc_id = result.doc_id\n",
        "            if doc_id in combined_scores:\n",
        "                combined_scores[doc_id] += result.score\n",
        "                match_counts[doc_id] += 1\n",
        "            else:\n",
        "                combined_scores[doc_id] = result.score\n",
        "                doc_objects[doc_id] = result\n",
        "                match_counts[doc_id] += 1\n",
        "\n",
        "        # CombMNZ: score * match_count\n",
        "        for doc_id in combined_scores:\n",
        "            combined_scores[doc_id] *= match_counts[doc_id]\n",
        "\n",
        "        # 排序\n",
        "        sorted_items = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        final_results = []\n",
        "        for doc_id, score in sorted_items:\n",
        "            result = doc_objects[doc_id]\n",
        "            final_results.append(SearchResult(\n",
        "                doc_id=result.doc_id,\n",
        "                content=result.content,\n",
        "                score=score,\n",
        "                metadata=result.metadata,\n",
        "                source='hybrid'\n",
        "            ))\n",
        "\n",
        "        return final_results\n",
        "\n",
        "\n",
        "# 使用示例\n",
        "if __name__ == \"__main__\":\n",
        "    # 模拟文档数据\n",
        "    documents = [\n",
        "        \"RAG是检索增强生成技术，结合了信息检索和语言生成\",\n",
        "        \"BM25是经典的关键词检索算法，基于TF-IDF改进\",\n",
        "        \"向量检索通过语义嵌入实现文档相似度计算\",\n",
        "        \"混合检索策略可以结合关键词和语义检索的优势\",\n",
        "        \"重排序算法进一步提升检索结果的相关性\"\n",
        "    ]\n",
        "\n",
        "    # 初始化混合检索器\n",
        "    retriever = HybridRetriever(documents)\n",
        "\n",
        "    # 执行检索\n",
        "    query = \"RAG检索算法\"\n",
        "    results = retriever.search(query, top_k=3)\n",
        "\n",
        "    print(f\"查询: {query}\")\n",
        "    print(f\"检索结果:\")\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"{i}. [分数: {result.score:.3f}] {result.content[:50]}...\")\n",
        "\n",
        "    # 评估不同融合策略\n",
        "    print(f\"\\n融合策略对比:\")\n",
        "    strategy_results = retriever.evaluate_fusion_strategies(query)\n",
        "    for strategy, results in strategy_results.items():\n",
        "        print(f\"{strategy}: {len(results)} 个结果\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvJaX4-pQdE2",
        "outputId": "27122963-d082-4c26-a86b-ba879397db02"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "查询: RAG检索算法\n",
            "检索结果:\n",
            "1. [分数: 0.016] 重排序算法进一步提升检索结果的相关性...\n",
            "2. [分数: 0.016] RAG是检索增强生成技术，结合了信息检索和语言生成...\n",
            "3. [分数: 0.016] 混合检索策略可以结合关键词和语义检索的优势...\n",
            "\n",
            "融合策略对比:\n",
            "rrf: 5 个结果\n",
            "weighted_sum: 5 个结果\n",
            "comb_mnz: 5 个结果\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oiwkQseQQerB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}