"""
è¯­ä¹‰æ„ŸçŸ¥åˆ†å‰²æ¨¡å—
å®ç°åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ™ºèƒ½æ–‡æ¡£åˆ†å‰²åŠŸèƒ½
"""

import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Tuple
import re


class SemanticAwareChunker:
    """è¯­ä¹‰æ„ŸçŸ¥åˆ†å‰²å™¨"""
    
    def __init__(self, target_size: int = 512, overlap_ratio: float = 0.1):
        """
        åˆå§‹åŒ–è¯­ä¹‰åˆ†å‰²å™¨
        
        Args:
            target_size: ç›®æ ‡chunké•¿åº¦ï¼ˆè¯æ•°ï¼‰
            overlap_ratio: é‡å æ¯”ä¾‹ï¼Œé˜²æ­¢ä¿¡æ¯ä¸¢å¤±
        """
        self.target_size = target_size        # ç›®æ ‡chunké•¿åº¦
        self.overlap_ratio = overlap_ratio    # é‡å æ¯”ä¾‹ï¼Œé˜²æ­¢ä¿¡æ¯ä¸¢å¤±
        # ä½¿ç”¨è½»é‡çº§è¯­ä¹‰æ¨¡å‹ï¼Œå¹³è¡¡æ•ˆæœå’Œé€Ÿåº¦
        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
    
    def intelligent_chunking(self, text: str) -> List[str]:
        """
        æ™ºèƒ½è¯­ä¹‰åˆ†å‰²çš„å®Œæ•´æµç¨‹
        
        Args:
            text: å¾…åˆ†å‰²çš„æ–‡æœ¬
            
        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
        """
        # ç¬¬1æ­¥ï¼šå¥å­çº§åˆ«åˆ†å‰²
        sentences = self.split_into_sentences(text)
        
        if len(sentences) <= 1:
            return [text]  # æ–‡æœ¬å¤ªçŸ­ï¼Œç›´æ¥è¿”å›
        
        # ç¬¬2æ­¥ï¼šä¸ºæ¯ä¸ªå¥å­ç”Ÿæˆè¯­ä¹‰å‘é‡
        embeddings = self.sentence_model.encode(sentences)
        
        # ç¬¬3æ­¥ï¼šè®¡ç®—ç›¸é‚»å¥å­çš„è¯­ä¹‰ç›¸ä¼¼åº¦
        similarity_scores = self.calculate_semantic_similarity(embeddings)
        
        # ç¬¬4æ­¥ï¼šåŸºäºç›¸ä¼¼åº¦åŠ¨æ€ç¡®å®šåˆ†å‰²è¾¹ç•Œ
        chunks = self.find_semantic_boundaries(sentences, similarity_scores)
        
        return chunks
    
    def split_into_sentences(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²ä¸ºå¥å­
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å¥å­åˆ—è¡¨
        """
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œå¥å­åˆ†å‰²ï¼ˆç®€åŒ–ç‰ˆï¼‰
        # å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPåº“å¦‚spacyæˆ–nltk
        sentence_endings = r'[.!?ã€‚ï¼ï¼Ÿï¼›;]'
        sentences = re.split(sentence_endings, text)
        
        # æ¸…ç†å’Œè¿‡æ»¤å¥å­
        cleaned_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 5:  # è¿‡æ»¤è¿‡çŸ­çš„ç‰‡æ®µ
                cleaned_sentences.append(sentence)
        
        return cleaned_sentences
    
    def calculate_semantic_similarity(self, embeddings: np.ndarray) -> List[float]:
        """
        è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ï¼šæ ¸å¿ƒç®—æ³•
        
        Args:
            embeddings: å¥å­åµŒå…¥å‘é‡æ•°ç»„
            
        Returns:
            ç›¸é‚»å¥å­é—´çš„ç›¸ä¼¼åº¦åˆ—è¡¨
        """
        similarities = []
        for i in range(len(embeddings) - 1):
            # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è¡¡é‡è¯­ä¹‰æ¥è¿‘ç¨‹åº¦
            sim = cosine_similarity([embeddings[i]], [embeddings[i+1]])[0][0]
            similarities.append(sim)
        return similarities
    
    def find_semantic_boundaries(self, sentences: List[str], similarities: List[float]) -> List[str]:
        """
        æ™ºèƒ½è¾¹ç•Œè¯†åˆ«ï¼šå…³é”®å†³ç­–é€»è¾‘
        
        Args:
            sentences: å¥å­åˆ—è¡¨
            similarities: ç›¸ä¼¼åº¦åˆ—è¡¨
            
        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        chunks = []
        current_chunk = []
        current_length = 0
        
        for i, sentence in enumerate(sentences):
            current_chunk.append(sentence)
            # ä¿®å¤ä¸­æ–‡æ–‡æœ¬çš„è¯æ•°è®¡ç®—
            current_length += len(sentence.replace('ï¼Œ', ' ').replace('ã€‚', ' ').split())
            
            # åˆ†å‰²å†³ç­–ï¼šåŒæ—¶è€ƒè™‘é•¿åº¦å’Œè¯­ä¹‰
            should_split = (
                current_length >= self.target_size and  # é•¿åº¦çº¦æŸï¼šè¾¾åˆ°ç›®æ ‡å¤§å°
                i < len(similarities) and               # è¾¹ç•Œæ£€æŸ¥ï¼šä¸æ˜¯æœ€åä¸€å¥
                similarities[i] < 0.95                  # é™ä½é˜ˆå€¼ï¼Œæ›´å®¹æ˜“åˆ†å‰²
            )
            
            if should_split:
                chunks.append(self.join_sentences(current_chunk))
                
                # é‡å ç­–ç•¥ï¼šä¿ç•™éƒ¨åˆ†å†…å®¹ï¼Œç¡®ä¿ä¸Šä¸‹æ–‡è¿è´¯
                overlap_size = int(len(current_chunk) * self.overlap_ratio)
                current_chunk = current_chunk[-overlap_size:] if overlap_size > 0 else []
                current_length = sum(len(s.split()) for s in current_chunk)
        
        # å¤„ç†æ–‡æ¡£æœ«å°¾çš„å‰©ä½™å†…å®¹
        if current_chunk:
            chunks.append(self.join_sentences(current_chunk))
        
        return chunks
    
    def join_sentences(self, sentences: List[str]) -> str:
        """
        å°†å¥å­åˆ—è¡¨åˆå¹¶ä¸ºå®Œæ•´æ–‡æœ¬
        
        Args:
            sentences: å¥å­åˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„æ–‡æœ¬
        """
        return 'ã€‚'.join(sentences) + 'ã€‚'


class AdaptiveChunker:
    """è‡ªé€‚åº”åˆ†å‰²å™¨ï¼Œæ”¯æŒå¤šç§åˆ†å‰²ç­–ç•¥"""
    
    def __init__(self, chunk_size: int = 512, overlap: int = 50):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”åˆ†å‰²å™¨
        
        Args:
            chunk_size: ç›®æ ‡chunkå¤§å°
            overlap: é‡å é•¿åº¦
        """
        self.chunk_size = chunk_size
        self.overlap = overlap
    
    def chunk_by_strategy(self, text: str, strategy: str = 'semantic') -> List[str]:
        """
        æ ¹æ®ç­–ç•¥è¿›è¡Œæ–‡æœ¬åˆ†å‰²
        
        Args:
            text: å¾…åˆ†å‰²æ–‡æœ¬
            strategy: åˆ†å‰²ç­–ç•¥ ('fixed', 'semantic', 'sliding_window')
            
        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        if strategy == 'semantic':
            chunker = SemanticAwareChunker(self.chunk_size, self.overlap / self.chunk_size)
            return chunker.intelligent_chunking(text)
        elif strategy == 'sliding_window':
            return self.sliding_window_chunking(text)
        else:
            return self.fixed_size_chunking(text)
    
    def fixed_size_chunking(self, text: str) -> List[str]:
        """å›ºå®šé•¿åº¦åˆ†å‰²"""
        # ä¿®å¤ä¸­æ–‡æ–‡æœ¬çš„è¯æ•°è®¡ç®—
        words = text.replace('ï¼Œ', ' ').replace('ã€‚', ' ').replace('ï¼', ' ').replace('ï¼Ÿ', ' ').split()
        chunks = []
        
        if len(words) <= self.chunk_size:
            return [text]
        
        step = max(1, self.chunk_size - self.overlap)
        
        for i in range(0, len(words), step):
            chunk_words = words[i:i + self.chunk_size]
            if chunk_words:  # ç¡®ä¿ä¸æ˜¯ç©ºåˆ—è¡¨
                chunks.append(' '.join(chunk_words))
        
        return chunks
    
    def sliding_window_chunking(self, text: str) -> List[str]:
        """æ»‘åŠ¨çª—å£åˆ†å‰²"""
        # ä¿®å¤ä¸­æ–‡æ–‡æœ¬çš„è¯æ•°è®¡ç®—
        words = text.replace('ï¼Œ', ' ').replace('ã€‚', ' ').replace('ï¼', ' ').replace('ï¼Ÿ', ' ').split()
        chunks = []
        
        # å¦‚æœæ–‡æœ¬å¤ªçŸ­ï¼Œç›´æ¥è¿”å›
        if len(words) <= self.chunk_size:
            return [text]
        
        step_size = max(1, self.chunk_size - self.overlap)
        
        # ç”Ÿæˆæ»‘åŠ¨çª—å£
        i = 0
        while i + self.chunk_size <= len(words):
            chunk_words = words[i:i + self.chunk_size]
            chunks.append(' '.join(chunk_words))
            i += step_size
        
        # å¤„ç†å‰©ä½™çš„è¯
        if i < len(words):
            remaining_words = words[i:]
            if len(remaining_words) > self.overlap:  # åªæœ‰å½“å‰©ä½™è¯æ•°è¶³å¤Ÿæ—¶æ‰æ·»åŠ 
                chunks.append(' '.join(remaining_words))
        
        return chunks


def evaluate_chunking_quality(chunks: List[str]) -> dict:
    """
    è¯„ä¼°åˆ†å‰²è´¨é‡
    
    Args:
        chunks: æ–‡æœ¬å—åˆ—è¡¨
        
    Returns:
        è´¨é‡è¯„ä¼°æŒ‡æ ‡
    """
    if not chunks:
        return {'quality_score': 0.0, 'uniformity': 0.0, 'count': 0}
    
    # è®¡ç®—é•¿åº¦åˆ†å¸ƒ - ä¿®å¤ä¸­æ–‡æ–‡æœ¬è¯æ•°è®¡ç®—
    lengths = [len(chunk.replace('ã€‚', ' ').split()) for chunk in chunks]
    avg_length = sum(lengths) / len(lengths)
    
    # è®¡ç®—é•¿åº¦æ–¹å·®ï¼ˆå‡åŒ€æ€§æŒ‡æ ‡ï¼‰
    length_variance = sum((l - avg_length) ** 2 for l in lengths) / len(lengths)
    uniformity = 1 - min(length_variance / (avg_length ** 2), 1.0)
    
    # è®¡ç®—å†…å®¹è´¨é‡ï¼ˆç®€åŒ–æŒ‡æ ‡ï¼‰
    content_quality = sum(1 for chunk in chunks if len(chunk.strip()) > 50) / len(chunks)
    
    overall_quality = (uniformity + content_quality) / 2
    
    return {
        'quality_score': overall_quality,
        'uniformity': uniformity,
        'content_quality': content_quality,
        'avg_length': avg_length,
        'count': len(chunks),
        'length_std': length_variance ** 0.5
    }


if __name__ == "__main__":
    # åˆ›å»ºæ›´æœ‰å¯¹æ¯”æ€§çš„æµ‹è¯•æ–‡æœ¬ï¼ŒåŒ…å«ä¸åŒä¸»é¢˜çš„æ®µè½
    sample_text = """
    æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ æ¨¡å¼å’Œè§„å¾‹ã€‚ä¼ ç»Ÿçš„ç¼–ç¨‹éœ€è¦å¼€å‘è€…æ˜ç¡®åœ°ç¼–å†™æ¯ä¸ªæ­¥éª¤çš„æŒ‡ä»¤ï¼Œè€Œæœºå™¨å­¦ä¹ å…è®¸è®¡ç®—æœºé€šè¿‡åˆ†æå¤§é‡æ•°æ®æ¥è‡ªåŠ¨å‘ç°è§„å¾‹ã€‚æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚è¿™ç§æŠ€æœ¯åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³å¤„ç†å’Œè‡ªç„¶è¯­è¨€ç†è§£ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
    
    é‡‘èç§‘æŠ€æ­£åœ¨æ”¹å˜ä¼ ç»Ÿé“¶è¡Œä¸šåŠ¡æ¨¡å¼ã€‚åŒºå—é“¾æŠ€æœ¯ä¸ºæ•°å­—è´§å¸æä¾›äº†å»ä¸­å¿ƒåŒ–çš„è§£å†³æ–¹æ¡ˆã€‚ç§»åŠ¨æ”¯ä»˜å·²ç»æˆä¸ºæ—¥å¸¸ç”Ÿæ´»ä¸­ä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†ã€‚æ•°å­—é’±åŒ…è®©ç”¨æˆ·èƒ½å¤Ÿè½»æ¾ç®¡ç†å¤šç§æ”¯ä»˜æ–¹å¼ã€‚
    
    äº‘è®¡ç®—æŠ€æœ¯ä½¿å¾—ä¼ä¸šèƒ½å¤Ÿçµæ´»åœ°æ‰©å±•è®¡ç®—èµ„æºã€‚å¾®æœåŠ¡æ¶æ„å¸®åŠ©å¼€å‘å›¢é˜Ÿæ›´å¥½åœ°ç®¡ç†å¤æ‚çš„åº”ç”¨ç³»ç»Ÿã€‚å®¹å™¨åŒ–æŠ€æœ¯ç®€åŒ–äº†åº”ç”¨çš„éƒ¨ç½²å’Œè¿ç»´å·¥ä½œã€‚DevOpså®è·µä¿ƒè¿›äº†å¼€å‘å’Œè¿è¥å›¢é˜Ÿä¹‹é—´çš„åä½œã€‚
    
    äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨å‰æ™¯å¹¿é˜”ã€‚æœºå™¨å­¦ä¹ ç®—æ³•èƒ½å¤Ÿå¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¯Šæ–­ç–¾ç—…ã€‚åŒ»ç–—å½±åƒåˆ†ææŠ€æœ¯æ­£åœ¨é©å‘½æ€§åœ°æ”¹å˜æ”¾å°„ç§‘çš„å·¥ä½œæµç¨‹ã€‚ä¸ªæ€§åŒ–åŒ»ç–—å°†æ ¹æ®æ‚£è€…çš„åŸºå› ä¿¡æ¯åˆ¶å®šæ²»ç–—æ–¹æ¡ˆã€‚
    """
    
    print("=== è¯­ä¹‰æ„ŸçŸ¥åˆ†å‰²æ¼”ç¤º ===")
    
    # ä½¿ç”¨è¾ƒå°çš„ç›®æ ‡å¤§å°å’Œæ›´é«˜çš„é˜ˆå€¼æ¥å±•ç¤ºåˆ†å‰²æ•ˆæœ
    semantic_chunker = SemanticAwareChunker(target_size=15, overlap_ratio=0.1)
    
    # è°ƒè¯•ä¿¡æ¯
    sentences = semantic_chunker.split_into_sentences(sample_text)
    print(f"åˆ†å‰²å‡ºçš„å¥å­æ•°é‡: {len(sentences)}")
    
    if len(sentences) > 1:
        embeddings = semantic_chunker.sentence_model.encode(sentences)
        similarities = semantic_chunker.calculate_semantic_similarity(embeddings)
        print("ç›¸é‚»å¥å­è¯­ä¹‰ç›¸ä¼¼åº¦:")
        for i, sim in enumerate(similarities):
            print(f"  å¥å­{i+1}â†’{i+2}: {sim:.3f}")
    
    # æ‰§è¡Œè¯­ä¹‰åˆ†å‰²
    semantic_chunks = semantic_chunker.intelligent_chunking(sample_text)
    
    print(f"\nè¯­ä¹‰åˆ†å‰²ç»“æœ (å…±{len(semantic_chunks)}ä¸ªç‰‡æ®µ):")
    for i, chunk in enumerate(semantic_chunks):
        words = chunk.replace('ã€‚', ' ').split()
        print(f"Chunk {i+1} ({len(words)}è¯): {chunk[:100]}...")
    
    print("\n=== ä¸åŒç­–ç•¥æ•ˆæœå¯¹æ¯” ===")
    
    # é¦–å…ˆè®¡ç®—æ–‡æœ¬çš„å®é™…è¯æ•°
    total_words = len(sample_text.replace('ï¼Œ', ' ').replace('ã€‚', ' ').replace('ï¼', ' ').replace('ï¼Ÿ', ' ').split())
    print(f"æ–‡æœ¬æ€»è¯æ•°: {total_words}")
    
    # åˆ›å»ºæµ‹è¯•é…ç½® - ä½¿ç”¨æ›´å°çš„chunk_sizeæ¥å¼ºåˆ¶åˆ†å‰²
    test_configs = [
        {"strategy": "fixed", "chunker": AdaptiveChunker(chunk_size=10, overlap=2)},
        {"strategy": "semantic", "chunker": AdaptiveChunker(chunk_size=10, overlap=2)},
        {"strategy": "sliding_window", "chunker": AdaptiveChunker(chunk_size=10, overlap=2)}
    ]
    
    results = {}
    
    for config in test_configs:
        strategy = config["strategy"]
        chunker = config["chunker"]
        
        print(f"\nè°ƒè¯• {strategy.upper()} ç­–ç•¥ (chunk_size={chunker.chunk_size}, overlap={chunker.overlap}):")
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        if strategy == "fixed":
            test_words = sample_text.replace('ï¼Œ', ' ').replace('ã€‚', ' ').replace('ï¼', ' ').replace('ï¼Ÿ', ' ').split()
            print(f"  å¤„ç†è¯æ•°: {len(test_words)}")
            print(f"  é¢„æœŸç‰‡æ®µæ•°: {max(1, len(test_words) // max(1, chunker.chunk_size - chunker.overlap))}")
        
        chunks = chunker.chunk_by_strategy(sample_text, strategy)
        quality = evaluate_chunking_quality(chunks)
        results[strategy] = {'chunks': chunks, 'quality': quality}
        
        print(f"\n{strategy.upper()} ç­–ç•¥:")
        print(f"  ç‰‡æ®µæ•°é‡: {len(chunks)}")
        print(f"  è´¨é‡åˆ†æ•°: {quality['quality_score']:.3f}")
        print(f"  å¹³å‡é•¿åº¦: {quality['avg_length']:.1f} è¯")
        print(f"  é•¿åº¦æ ‡å‡†å·®: {quality['length_std']:.1f}")
        
        # æ˜¾ç¤ºå®é™…åˆ†å‰²ç»“æœ
        for i, chunk in enumerate(chunks):
            words = chunk.replace('ï¼Œ', ' ').replace('ã€‚', ' ').replace('ï¼', ' ').replace('ï¼Ÿ', ' ').split()
            print(f"  ç‰‡æ®µ{i+1}: {chunk[:60]}... ({len(words)}è¯)")
            
            # å¯¹äºæ»‘åŠ¨çª—å£ï¼Œæ˜¾ç¤ºé‡å ä¿¡æ¯
            if strategy == "sliding_window" and i > 0:
                prev_words = chunks[i-1].replace('ï¼Œ', ' ').replace('ã€‚', ' ').replace('ï¼', ' ').replace('ï¼Ÿ', ' ').split()
                curr_words = words
                
                # æ˜¾ç¤ºå‰ä¸€ç‰‡æ®µæœ«å°¾å’Œå½“å‰ç‰‡æ®µå¼€å¤´
                if len(prev_words) >= 2 and len(curr_words) >= 2:
                    print(f"    â””â”€ å‰ç‰‡æ®µæœ«å°¾: ...{' '.join(prev_words[-2:])}")
                    print(f"    â””â”€ å½“å‰ç‰‡æ®µå¼€å¤´: {' '.join(curr_words[:2])}...")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å è¯æ±‡
                    overlap_words = []
                    for word in prev_words[-chunker.overlap:]:
                        if word in curr_words[:chunker.overlap]:
                            overlap_words.append(word)
                    
                    if overlap_words:
                        print(f"    â””â”€ é‡å è¯æ±‡: {' '.join(overlap_words)}")
    
    print("\n=== åˆ†å‰²ç­–ç•¥åˆ†æ ===")
    
    # æ‰¾åˆ°æœ€ä½³ç­–ç•¥
    best_strategy = max(results.keys(), key=lambda x: results[x]['quality']['quality_score'])
    print(f"æ¨èç­–ç•¥: {best_strategy.upper()}")
    
    # åˆ†æå„ç­–ç•¥ç‰¹ç‚¹
    print("\nç­–ç•¥ç‰¹ç‚¹åˆ†æ:")
    for strategy, result in results.items():
        chunks = result['chunks']
        quality = result['quality']
        print(f"  {strategy.upper()}:")
        print(f"    - ç‰‡æ®µæ•°é‡: {len(chunks)}")
        print(f"    - å†…å®¹è´¨é‡: {quality['content_quality']:.3f}")
        print(f"    - é•¿åº¦å‡åŒ€æ€§: {quality['uniformity']:.3f}")
        
        if strategy == 'semantic':
            print(f"    - è¯­ä¹‰è¿è´¯æ€§: åœ¨è¯­ä¹‰è½¬æ¢ç‚¹åˆ†å‰² (é˜ˆå€¼<0.95)")
            print(f"    - æ™ºèƒ½è¾¹ç•Œ: è¯†åˆ«ä¸»é¢˜å˜åŒ–ï¼Œä¿æŒå†…å®¹å®Œæ•´æ€§")
        elif strategy == 'sliding_window':
            print(f"    - é‡å ç­–ç•¥: æ¯ä¸ªç‰‡æ®µä¸å‰ä¸€ç‰‡æ®µé‡å {chunker.overlap}ä¸ªè¯")
            print(f"    - ä¸Šä¸‹æ–‡ä¿æŒ: é˜²æ­¢é‡è¦ä¿¡æ¯åœ¨åˆ†å‰²è¾¹ç•Œä¸¢å¤±")
        else:
            print(f"    - å›ºå®šåˆ†å‰²: æŒ‰è¯æ•°({chunker.chunk_size})å‡åŒ€åˆ†å‰²")
            print(f"    - ç®€å•é«˜æ•ˆ: ä¸è€ƒè™‘è¯­ä¹‰ï¼Œå¤„ç†é€Ÿåº¦æœ€å¿«")
    
    print("\n=== ğŸ’¡ RAGåº”ç”¨å»ºè®® ===")
    print("ğŸ“Š æ€§èƒ½å¯¹æ¯”:")
    print(f"  â€¢ è¯­ä¹‰åˆ†å‰²: {results['semantic']['quality']['quality_score']:.3f}åˆ† - æœ€é€‚åˆé—®ç­”ç³»ç»Ÿ")
    print(f"  â€¢ å›ºå®šåˆ†å‰²: {results['fixed']['quality']['quality_score']:.3f}åˆ† - æœ€é€‚åˆå¤§æ‰¹é‡å¤„ç†")
    print(f"  â€¢ æ»‘åŠ¨çª—å£: {results['sliding_window']['quality']['quality_score']:.3f}åˆ† - æœ€é€‚åˆéœ€è¦ä¸Šä¸‹æ–‡çš„åœºæ™¯")
    
    print("\nğŸ¯ ä½¿ç”¨åœºæ™¯:")
    print("  ğŸ“ è¯­ä¹‰åˆ†å‰² â†’ æ™ºèƒ½é—®ç­”ã€çŸ¥è¯†å›¾è°±æ„å»º")
    print("  âš¡ å›ºå®šåˆ†å‰² â†’ å¤§è§„æ¨¡æ–‡æ¡£ç´¢å¼•ã€æ‰¹é‡å¤„ç†")
    print("  ğŸ”„ æ»‘åŠ¨çª—å£ â†’ é•¿æ–‡æ¡£ç†è§£ã€è·¨æ®µè½æ£€ç´¢")

# === è¯­ä¹‰æ„ŸçŸ¥åˆ†å‰²æ¼”ç¤º ===
# åˆ†å‰²å‡ºçš„å¥å­æ•°é‡: 16
# ç›¸é‚»å¥å­è¯­ä¹‰ç›¸ä¼¼åº¦:
#   å¥å­1â†’2: 0.593
#   å¥å­2â†’3: 0.524
#   å¥å­3â†’4: 0.402
#   å¥å­4â†’5: 0.498
#   å¥å­5â†’6: 0.501
#   å¥å­6â†’7: 0.578
#   å¥å­7â†’8: 0.350
#   å¥å­8â†’9: 0.505
#   å¥å­9â†’10: 0.943
#   å¥å­10â†’11: 0.473
#   å¥å­11â†’12: 0.322
#   å¥å­12â†’13: 0.240
#   å¥å­13â†’14: 0.489
#   å¥å­14â†’15: 0.758
#   å¥å­15â†’16: 0.425

# è¯­ä¹‰åˆ†å‰²ç»“æœ (å…±2ä¸ªç‰‡æ®µ):
# Chunk 1 (12è¯): æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ æ¨¡å¼å’Œè§„å¾‹ã€‚ä¼ ç»Ÿçš„ç¼–ç¨‹éœ€è¦å¼€å‘è€…æ˜ç¡®åœ°ç¼–å†™æ¯ä¸ªæ­¥éª¤çš„æŒ‡ä»¤ï¼Œè€Œæœºå™¨å­¦ä¹ å…è®¸è®¡ç®—æœºé€šè¿‡åˆ†æå¤§é‡æ•°æ®æ¥è‡ªåŠ¨å‘ç°è§„å¾‹ã€‚æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€...
# Chunk 2 (5è¯): DevOpså®è·µä¿ƒè¿›äº†å¼€å‘å’Œè¿è¥å›¢é˜Ÿä¹‹é—´çš„åä½œã€‚äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨å‰æ™¯å¹¿é˜”ã€‚æœºå™¨å­¦ä¹ ç®—æ³•èƒ½å¤Ÿå¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¯Šæ–­ç–¾ç—…ã€‚åŒ»ç–—å½±åƒåˆ†ææŠ€æœ¯æ­£åœ¨é©å‘½æ€§åœ°æ”¹å˜æ”¾å°„ç§‘çš„å·¥ä½œæµç¨‹ã€‚ä¸ªæ€§åŒ–åŒ»ç–—å°†æ ¹æ®æ‚£è€…çš„åŸºå› ...

# === ä¸åŒç­–ç•¥æ•ˆæœå¯¹æ¯” ===
# æ–‡æœ¬æ€»è¯æ•°: 19

# è°ƒè¯• FIXED ç­–ç•¥ (chunk_size=10, overlap=2):
#   å¤„ç†è¯æ•°: 19
#   é¢„æœŸç‰‡æ®µæ•°: 2

# FIXED ç­–ç•¥:
#   ç‰‡æ®µæ•°é‡: 3
#   è´¨é‡åˆ†æ•°: 0.907
#   å¹³å‡é•¿åº¦: 7.7 è¯
#   é•¿åº¦æ ‡å‡†å·®: 3.3
#   ç‰‡æ®µ1: æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ å®ƒé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ æ¨¡å¼å’Œè§„å¾‹ ä¼ ç»Ÿçš„ç¼–ç¨‹éœ€è¦å¼€å‘è€…æ˜ç¡®åœ°ç¼–å†™æ¯ä¸ªæ­¥éª¤çš„... (10è¯)
#   ç‰‡æ®µ2: åŒºå—é“¾æŠ€æœ¯ä¸ºæ•°å­—è´§å¸æä¾›äº†å»ä¸­å¿ƒåŒ–çš„è§£å†³æ–¹æ¡ˆ ç§»åŠ¨æ”¯ä»˜å·²ç»æˆä¸ºæ—¥å¸¸ç”Ÿæ´»ä¸­ä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ† æ•°å­—é’±åŒ…è®©ç”¨æˆ·èƒ½å¤Ÿè½»æ¾ç®¡ç†å¤šç§... (10è¯)
#   ç‰‡æ®µ3: æœºå™¨å­¦ä¹ ç®—æ³•èƒ½å¤Ÿå¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¯Šæ–­ç–¾ç—… åŒ»ç–—å½±åƒåˆ†ææŠ€æœ¯æ­£åœ¨é©å‘½æ€§åœ°æ”¹å˜æ”¾å°„ç§‘çš„å·¥ä½œæµç¨‹ ä¸ªæ€§åŒ–åŒ»ç–—å°†æ ¹æ®æ‚£è€…çš„åŸºå› ä¿¡... (3è¯)

# è°ƒè¯• SEMANTIC ç­–ç•¥ (chunk_size=10, overlap=2):

# SEMANTIC ç­–ç•¥:
#   ç‰‡æ®µæ•°é‡: 2
#   è´¨é‡åˆ†æ•°: 0.984
#   å¹³å‡é•¿åº¦: 8.5 è¯
#   é•¿åº¦æ ‡å‡†å·®: 1.5
#   ç‰‡æ®µ1: æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ æ¨¡å¼å’Œè§„å¾‹ã€‚ä¼ ç»Ÿçš„ç¼–ç¨‹éœ€è¦å¼€å‘è€…æ˜ç¡®åœ°ç¼–å†™æ¯ä¸ªæ­¥éª¤çš„... (10è¯)
#   ç‰‡æ®µ2: ç§»åŠ¨æ”¯ä»˜å·²ç»æˆä¸ºæ—¥å¸¸ç”Ÿæ´»ä¸­ä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†ã€‚æ•°å­—é’±åŒ…è®©ç”¨æˆ·èƒ½å¤Ÿè½»æ¾ç®¡ç†å¤šç§æ”¯ä»˜æ–¹å¼ã€‚äº‘è®¡ç®—æŠ€æœ¯ä½¿å¾—ä¼ä¸šèƒ½å¤Ÿçµæ´»åœ°æ‰©å±•è®¡ç®—... (10è¯)

# è°ƒè¯• SLIDING_WINDOW ç­–ç•¥ (chunk_size=10, overlap=2):

# SLIDING_WINDOW ç­–ç•¥:
#   ç‰‡æ®µæ•°é‡: 3
#   è´¨é‡åˆ†æ•°: 0.907
#   å¹³å‡é•¿åº¦: 7.7 è¯
#   é•¿åº¦æ ‡å‡†å·®: 3.3
#   ç‰‡æ®µ1: æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ å®ƒé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ æ¨¡å¼å’Œè§„å¾‹ ä¼ ç»Ÿçš„ç¼–ç¨‹éœ€è¦å¼€å‘è€…æ˜ç¡®åœ°ç¼–å†™æ¯ä¸ªæ­¥éª¤çš„... (10è¯)
#   ç‰‡æ®µ2: åŒºå—é“¾æŠ€æœ¯ä¸ºæ•°å­—è´§å¸æä¾›äº†å»ä¸­å¿ƒåŒ–çš„è§£å†³æ–¹æ¡ˆ ç§»åŠ¨æ”¯ä»˜å·²ç»æˆä¸ºæ—¥å¸¸ç”Ÿæ´»ä¸­ä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ† æ•°å­—é’±åŒ…è®©ç”¨æˆ·èƒ½å¤Ÿè½»æ¾ç®¡ç†å¤šç§... (10è¯)
#     â””â”€ å‰ç‰‡æ®µæœ«å°¾: ...åŒºå—é“¾æŠ€æœ¯ä¸ºæ•°å­—è´§å¸æä¾›äº†å»ä¸­å¿ƒåŒ–çš„è§£å†³æ–¹æ¡ˆ ç§»åŠ¨æ”¯ä»˜å·²ç»æˆä¸ºæ—¥å¸¸ç”Ÿæ´»ä¸­ä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†
#     â””â”€ å½“å‰ç‰‡æ®µå¼€å¤´: åŒºå—é“¾æŠ€æœ¯ä¸ºæ•°å­—è´§å¸æä¾›äº†å»ä¸­å¿ƒåŒ–çš„è§£å†³æ–¹æ¡ˆ ç§»åŠ¨æ”¯ä»˜å·²ç»æˆä¸ºæ—¥å¸¸ç”Ÿæ´»ä¸­ä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†...
#     â””â”€ é‡å è¯æ±‡: åŒºå—é“¾æŠ€æœ¯ä¸ºæ•°å­—è´§å¸æä¾›äº†å»ä¸­å¿ƒåŒ–çš„è§£å†³æ–¹æ¡ˆ ç§»åŠ¨æ”¯ä»˜å·²ç»æˆä¸ºæ—¥å¸¸ç”Ÿæ´»ä¸­ä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†
#   ç‰‡æ®µ3: æœºå™¨å­¦ä¹ ç®—æ³•èƒ½å¤Ÿå¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¯Šæ–­ç–¾ç—… åŒ»ç–—å½±åƒåˆ†ææŠ€æœ¯æ­£åœ¨é©å‘½æ€§åœ°æ”¹å˜æ”¾å°„ç§‘çš„å·¥ä½œæµç¨‹ ä¸ªæ€§åŒ–åŒ»ç–—å°†æ ¹æ®æ‚£è€…çš„åŸºå› ä¿¡... (3è¯)
#     â””â”€ å‰ç‰‡æ®µæœ«å°¾: ...æœºå™¨å­¦ä¹ ç®—æ³•èƒ½å¤Ÿå¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¯Šæ–­ç–¾ç—… åŒ»ç–—å½±åƒåˆ†ææŠ€æœ¯æ­£åœ¨é©å‘½æ€§åœ°æ”¹å˜æ”¾å°„ç§‘çš„å·¥ä½œæµç¨‹
#     â””â”€ å½“å‰ç‰‡æ®µå¼€å¤´: æœºå™¨å­¦ä¹ ç®—æ³•èƒ½å¤Ÿå¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¯Šæ–­ç–¾ç—… åŒ»ç–—å½±åƒåˆ†ææŠ€æœ¯æ­£åœ¨é©å‘½æ€§åœ°æ”¹å˜æ”¾å°„ç§‘çš„å·¥ä½œæµç¨‹...
#     â””â”€ é‡å è¯æ±‡: æœºå™¨å­¦ä¹ ç®—æ³•èƒ½å¤Ÿå¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¯Šæ–­ç–¾ç—… åŒ»ç–—å½±åƒåˆ†ææŠ€æœ¯æ­£åœ¨é©å‘½æ€§åœ°æ”¹å˜æ”¾å°„ç§‘çš„å·¥ä½œæµç¨‹

# === åˆ†å‰²ç­–ç•¥åˆ†æ ===
# æ¨èç­–ç•¥: SEMANTIC

# ç­–ç•¥ç‰¹ç‚¹åˆ†æ:
#   FIXED:
#     - ç‰‡æ®µæ•°é‡: 3
#     - å†…å®¹è´¨é‡: 1.000
#     - é•¿åº¦å‡åŒ€æ€§: 0.815
#     - å›ºå®šåˆ†å‰²: æŒ‰è¯æ•°(10)å‡åŒ€åˆ†å‰²
#     - ç®€å•é«˜æ•ˆ: ä¸è€ƒè™‘è¯­ä¹‰ï¼Œå¤„ç†é€Ÿåº¦æœ€å¿«
#   SEMANTIC:
#     - ç‰‡æ®µæ•°é‡: 2
#     - å†…å®¹è´¨é‡: 1.000
#     - é•¿åº¦å‡åŒ€æ€§: 0.969
#     - è¯­ä¹‰è¿è´¯æ€§: åœ¨è¯­ä¹‰è½¬æ¢ç‚¹åˆ†å‰² (é˜ˆå€¼<0.95)
#     - æ™ºèƒ½è¾¹ç•Œ: è¯†åˆ«ä¸»é¢˜å˜åŒ–ï¼Œä¿æŒå†…å®¹å®Œæ•´æ€§
#   SLIDING_WINDOW:
#     - ç‰‡æ®µæ•°é‡: 3
#     - å†…å®¹è´¨é‡: 1.000
#     - é•¿åº¦å‡åŒ€æ€§: 0.815
#     - é‡å ç­–ç•¥: æ¯ä¸ªç‰‡æ®µä¸å‰ä¸€ç‰‡æ®µé‡å 2ä¸ªè¯
#     - ä¸Šä¸‹æ–‡ä¿æŒ: é˜²æ­¢é‡è¦ä¿¡æ¯åœ¨åˆ†å‰²è¾¹ç•Œä¸¢å¤±

# === ğŸ’¡ RAGåº”ç”¨å»ºè®® ===
# ğŸ“Š æ€§èƒ½å¯¹æ¯”:
#   â€¢ è¯­ä¹‰åˆ†å‰²: 0.984åˆ† - æœ€é€‚åˆé—®ç­”ç³»ç»Ÿ
#   â€¢ å›ºå®šåˆ†å‰²: 0.907åˆ† - æœ€é€‚åˆå¤§æ‰¹é‡å¤„ç†
#   â€¢ æ»‘åŠ¨çª—å£: 0.907åˆ† - æœ€é€‚åˆéœ€è¦ä¸Šä¸‹æ–‡çš„åœºæ™¯

# ğŸ¯ ä½¿ç”¨åœºæ™¯:
#   ğŸ“ è¯­ä¹‰åˆ†å‰² â†’ æ™ºèƒ½é—®ç­”ã€çŸ¥è¯†å›¾è°±æ„å»º
#   âš¡ å›ºå®šåˆ†å‰² â†’ å¤§è§„æ¨¡æ–‡æ¡£ç´¢å¼•ã€æ‰¹é‡å¤„ç†
#   ğŸ”„ æ»‘åŠ¨çª—å£ â†’ é•¿æ–‡æ¡£ç†è§£ã€è·¨æ®µè½æ£€ç´¢