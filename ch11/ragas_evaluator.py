"""
RAGAS RAGç³»ç»Ÿè¯„ä¼°å™¨
åŸºäºRAGASæ¡†æ¶çš„å¿«é€ŸRAGç³»ç»Ÿè´¨é‡è¯„ä¼°å·¥å…·
"""

from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_relevancy, context_precision
from datasets import Dataset
from typing import List, Dict, Any
import time
import logging


class RAGASEvaluator:
    """RAGASè¯„ä¼°å™¨ - RAGç³»ç»Ÿå¿«é€Ÿä½“æ£€å·¥å…·"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        # è®¾ç½®æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡
        self.core_metrics = {
            'faithfulness': faithfulness,           # å¿ å®åº¦ - æœ€é‡è¦
            'answer_relevancy': answer_relevancy,   # ç­”æ¡ˆç›¸å…³æ€§ 
            'context_relevancy': context_relevancy, # ä¸Šä¸‹æ–‡ç›¸å…³æ€§
            'context_precision': context_precision, # ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦
        }
        
        # è®¾ç½®è´¨é‡é˜ˆå€¼ï¼ˆåŸºäºå®è·µç»éªŒï¼‰
        self.quality_thresholds = {
            'faithfulness': 0.8,        # å¿ å®åº¦æœ€å…³é”®
            'answer_relevancy': 0.7,    # ç­”æ¡ˆç›¸å…³æ€§
            'context_relevancy': 0.6,   # ä¸Šä¸‹æ–‡ç›¸å…³æ€§  
            'context_precision': 0.5,   # ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦
        }
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def evaluate(self, test_data: List[Dict]) -> Dict[str, Any]:
        """
        æ‰§è¡ŒRAGASè¯„ä¼°
        
        Args:
            test_data: æµ‹è¯•æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
                - question: ç”¨æˆ·é—®é¢˜
                - answer: RAGç³»ç»Ÿç”Ÿæˆçš„ç­”æ¡ˆ  
                - contexts: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åˆ—è¡¨
                - ground_truth: æ ‡å‡†ç­”æ¡ˆï¼ˆå¯é€‰ï¼‰
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        self.logger.info(f"å¼€å§‹RAGASè¯„ä¼°ï¼Œå…±{len(test_data)}ä¸ªæµ‹è¯•æ ·æœ¬...")
        
        start_time = time.time()
        
        try:
            # è½¬æ¢æ•°æ®æ ¼å¼
            dataset = self._prepare_dataset(test_data)
            
            # æ‰§è¡Œè¯„ä¼°
            result = evaluate(
                dataset=dataset,
                metrics=list(self.core_metrics.values())
            )
            
            # å¤„ç†å’Œè§£é‡Šç»“æœ
            evaluation_report = self._process_results(result)
            evaluation_report['evaluation_time'] = time.time() - start_time
            evaluation_report['sample_count'] = len(test_data)
            
            self.logger.info(f"è¯„ä¼°å®Œæˆï¼Œç”¨æ—¶{evaluation_report['evaluation_time']:.2f}ç§’")
            return evaluation_report
            
        except Exception as e:
            self.logger.error(f"è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {str(e)}")
            raise
    
    def _prepare_dataset(self, test_data: List[Dict]) -> Dataset:
        """å‡†å¤‡RAGASè¯„ä¼°æ•°æ®é›†"""
        
        # éªŒè¯æ•°æ®æ ¼å¼
        required_fields = ['question', 'answer', 'contexts']
        for i, item in enumerate(test_data):
            for field in required_fields:
                if field not in item:
                    raise ValueError(f"æµ‹è¯•æ ·æœ¬{i}ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
        
        # è½¬æ¢ä¸ºRAGASæ ¼å¼
        dataset_dict = {
            'question': [item['question'] for item in test_data],
            'answer': [item['answer'] for item in test_data],
            'contexts': [item['contexts'] for item in test_data],
        }
        
        # å¦‚æœæœ‰ground_truthï¼Œæ·»åŠ è¿›å»
        if all('ground_truth' in item for item in test_data):
            dataset_dict['ground_truth'] = [item['ground_truth'] for item in test_data]
        
        return Dataset.from_dict(dataset_dict)
    
    def _process_results(self, ragas_result: Dict) -> Dict[str, Any]:
        """å¤„ç†RAGASè¯„ä¼°ç»“æœ"""
        
        # æå–å„æŒ‡æ ‡åˆ†æ•°
        scores = {}
        for metric_name, metric_obj in self.core_metrics.items():
            metric_key = metric_obj.name if hasattr(metric_obj, 'name') else metric_name
            if metric_key in ragas_result:
                scores[metric_name] = ragas_result[metric_key]
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
        weights = {
            'faithfulness': 0.4,        # å¿ å®åº¦æƒé‡æœ€é«˜
            'answer_relevancy': 0.3,    # ç­”æ¡ˆç›¸å…³æ€§æ¬¡ä¹‹
            'context_relevancy': 0.2,   # ä¸Šä¸‹æ–‡ç›¸å…³æ€§
            'context_precision': 0.1,   # ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦
        }
        
        overall_score = sum(
            scores.get(metric, 0) * weight 
            for metric, weight in weights.items()
        )
        
        # åˆ¤æ–­è´¨é‡ç­‰çº§
        quality_grade = self._get_quality_grade(scores)
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        recommendations = self._generate_recommendations(scores)
        
        return {
            'scores': scores,
            'overall_score': overall_score,
            'quality_grade': quality_grade,
            'recommendations': recommendations,
            'detailed_analysis': self._analyze_performance(scores)
        }
    
    def _get_quality_grade(self, scores: Dict[str, float]) -> str:
        """æ ¹æ®åˆ†æ•°åˆ¤æ–­è´¨é‡ç­‰çº§"""
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¥é‡é—®é¢˜
        critical_issues = []
        if scores.get('faithfulness', 0) < 0.6:
            critical_issues.append('å¿ å®åº¦ä¸¥é‡ä¸è¶³')
        if scores.get('answer_relevancy', 0) < 0.5:
            critical_issues.append('ç­”æ¡ˆç›¸å…³æ€§æå·®')
        
        if critical_issues:
            return f"ä¸åˆæ ¼ - {', '.join(critical_issues)}"
        
        # æ ¹æ®æ•´ä½“è¡¨ç°è¯„çº§
        good_metrics = sum(1 for metric, score in scores.items() 
                          if score >= self.quality_thresholds.get(metric, 0.7))
        
        total_metrics = len(scores)
        
        if good_metrics == total_metrics:
            return "ä¼˜ç§€ - å¯ä»¥æ”¾å¿ƒä¸Šçº¿"
        elif good_metrics >= total_metrics * 0.75:
            return "è‰¯å¥½ - å¯ä»¥éƒ¨ç½²ä½¿ç”¨"
        elif good_metrics >= total_metrics * 0.5:
            return "éœ€æ”¹è¿› - å»ºè®®ä¼˜åŒ–åä½¿ç”¨"
        else:
            return "ä¸åˆæ ¼ - éœ€è¦é‡æ–°è®¾è®¡"
    
    def _generate_recommendations(self, scores: Dict[str, float]) -> List[str]:
        """ç”Ÿæˆé’ˆå¯¹æ€§ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥é—®é¢˜
        if scores.get('faithfulness', 1.0) < self.quality_thresholds['faithfulness']:
            recommendations.append({
                'priority': 'high',
                'metric': 'faithfulness',
                'issue': f"å¿ å®åº¦åä½ ({scores['faithfulness']:.3f})",
                'solution': "ä¼˜åŒ–promptè®¾è®¡ï¼ŒåŠ å¼º'ä»…åŸºäºä¸Šä¸‹æ–‡å›ç­”'çš„çº¦æŸ",
                'example': "åœ¨promptä¸­æ˜ç¡®è¦æ±‚ï¼š'è¯·ä»…åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ï¼Œä¸è¦æ·»åŠ é¢å¤–ä¿¡æ¯ã€‚'"
            })
        
        if scores.get('answer_relevancy', 1.0) < self.quality_thresholds['answer_relevancy']:
            recommendations.append({
                'priority': 'medium',
                'metric': 'answer_relevancy', 
                'issue': f"ç­”æ¡ˆç›¸å…³æ€§ä¸è¶³ ({scores['answer_relevancy']:.3f})",
                'solution': "æ”¹è¿›ç”Ÿæˆç­–ç•¥ï¼Œç¡®ä¿å›ç­”ç›´æ¥é’ˆå¯¹ç”¨æˆ·é—®é¢˜",
                'example': "è°ƒæ•´promptè®©æ¨¡å‹å…ˆç†è§£é—®é¢˜é‡ç‚¹ï¼Œå†åŸºäºä¸Šä¸‹æ–‡ç»„ç»‡ç­”æ¡ˆã€‚"
            })
        
        if scores.get('context_relevancy', 1.0) < self.quality_thresholds['context_relevancy']:
            recommendations.append({
                'priority': 'medium',
                'metric': 'context_relevancy',
                'issue': f"æ£€ç´¢è´¨é‡æœ‰å¾…æå‡ ({scores['context_relevancy']:.3f})", 
                'solution': "ä¼˜åŒ–æ£€ç´¢ç®—æ³•ï¼Œæ”¹è¿›embeddingæ¨¡å‹æˆ–æŸ¥è¯¢é¢„å¤„ç†",
                'example': "è€ƒè™‘ä½¿ç”¨æ›´å¥½çš„embeddingæ¨¡å‹ï¼Œæˆ–å¯¹æŸ¥è¯¢è¿›è¡Œæ‰©å±•å’Œé‡å†™ã€‚"
            })
        
        if scores.get('context_precision', 1.0) < self.quality_thresholds['context_precision']:
            recommendations.append({
                'priority': 'low',
                'metric': 'context_precision',
                'issue': f"æ£€ç´¢æ’åºéœ€è¦ä¼˜åŒ– ({scores['context_precision']:.3f})",
                'solution': "å®æ–½é‡æ’åºæœºåˆ¶ï¼Œç¡®ä¿æœ€ç›¸å…³å†…å®¹æ’åœ¨å‰é¢", 
                'example': "å¯ä»¥ä½¿ç”¨Cross-Encoderè¿›è¡Œé‡æ’åºï¼Œæˆ–è°ƒæ•´æ£€ç´¢ç®—æ³•å‚æ•°ã€‚"
            })
        
        if not recommendations:
            recommendations.append({
                'priority': 'info',
                'metric': 'overall',
                'issue': 'ç³»ç»Ÿè¡¨ç°è‰¯å¥½',
                'solution': 'å„é¡¹æŒ‡æ ‡å‡è¾¾æ ‡ï¼Œå¯ä»¥æ”¾å¿ƒä½¿ç”¨',
                'example': 'å»ºè®®å®šæœŸç”¨çœŸå®ç”¨æˆ·é—®é¢˜è¿›è¡Œè¯„ä¼°ï¼ŒæŒç»­ç›‘æ§è´¨é‡ã€‚'
            })
        
        return recommendations
    
    def _analyze_performance(self, scores: Dict[str, float]) -> Dict[str, str]:
        """è¯¦ç»†æ€§èƒ½åˆ†æ"""
        analysis = {}
        
        for metric, score in scores.items():
            if score >= 0.9:
                analysis[metric] = "è¡¨ç°ä¼˜ç§€"
            elif score >= self.quality_thresholds.get(metric, 0.7):
                analysis[metric] = "è¡¨ç°è‰¯å¥½"
            elif score >= 0.5:
                analysis[metric] = "éœ€è¦æ”¹è¿›"
            else:
                analysis[metric] = "è¡¨ç°è¾ƒå·®"
        
        return analysis
    
    def compare_versions(self, version_a_data: List[Dict], version_b_data: List[Dict], 
                        names: tuple = ("ç‰ˆæœ¬A", "ç‰ˆæœ¬B")) -> Dict[str, Any]:
        """å¯¹æ¯”ä¸¤ä¸ªRAGç³»ç»Ÿç‰ˆæœ¬"""
        
        self.logger.info(f"å¼€å§‹ç‰ˆæœ¬å¯¹æ¯”è¯„ä¼°...")
        
        # åˆ†åˆ«è¯„ä¼°ä¸¤ä¸ªç‰ˆæœ¬
        result_a = self.evaluate(version_a_data)
        result_b = self.evaluate(version_b_data)
        
        # å¯¹æ¯”åˆ†æ
        comparison = {
            'version_names': names,
            'scores_comparison': {},
            'overall_comparison': {
                names[0]: result_a['overall_score'],
                names[1]: result_b['overall_score']
            },
            'improvements': [],
            'regressions': []
        }
        
        # å¯¹æ¯”å„é¡¹æŒ‡æ ‡
        for metric in self.core_metrics.keys():
            score_a = result_a['scores'].get(metric, 0)
            score_b = result_b['scores'].get(metric, 0)
            diff = score_b - score_a
            
            comparison['scores_comparison'][metric] = {
                names[0]: score_a,
                names[1]: score_b,
                'difference': diff,
                'change': 'improved' if diff > 0.02 else 'regressed' if diff < -0.02 else 'stable'
            }
            
            if diff > 0.02:
                comparison['improvements'].append(f"{metric}: +{diff:.3f}")
            elif diff < -0.02:
                comparison['regressions'].append(f"{metric}: {diff:.3f}")
        
        return comparison
    
    def generate_report(self, evaluation_result: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Š"""
        
        report = f"""
=== RAGç³»ç»ŸRAGASè¯„ä¼°æŠ¥å‘Š ===
è¯„ä¼°æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
æ ·æœ¬æ•°é‡: {evaluation_result.get('sample_count', 'N/A')}
è¯„ä¼°è€—æ—¶: {evaluation_result.get('evaluation_time', 0):.2f}ç§’

== ç»¼åˆè¯„ä¼° ==
æ•´ä½“è¯„åˆ†: {evaluation_result['overall_score']:.3f}
è´¨é‡ç­‰çº§: {evaluation_result['quality_grade']}

== è¯¦ç»†æŒ‡æ ‡ ==
"""
        
        for metric, score in evaluation_result['scores'].items():
            threshold = self.quality_thresholds.get(metric, 0.7)
            status = "âœ… è¾¾æ ‡" if score >= threshold else "âŒ ä¸è¾¾æ ‡"
            analysis = evaluation_result['detailed_analysis'].get(metric, "")
            
            report += f"- {metric}: {score:.3f} (é˜ˆå€¼: {threshold}) {status} - {analysis}\n"
        
        report += "\n== ä¼˜åŒ–å»ºè®® ==\n"
        for i, rec in enumerate(evaluation_result['recommendations'], 1):
            priority_icon = {"high": "ğŸ”¥", "medium": "âš ï¸", "low": "ğŸ’¡", "info": "â„¹ï¸"}.get(rec['priority'], "")
            report += f"{i}. {priority_icon} {rec['issue']}\n"
            report += f"   è§£å†³æ–¹æ¡ˆ: {rec['solution']}\n"
            report += f"   ç¤ºä¾‹: {rec['example']}\n\n"
        
        return report
    
    def quick_check(self, question: str, answer: str, contexts: List[str], 
                   ground_truth: str = None) -> Dict[str, Any]:
        """å¿«é€Ÿå•æ¬¡è¯„ä¼°"""
        
        test_data = [{
            'question': question,
            'answer': answer, 
            'contexts': contexts
        }]
        
        if ground_truth:
            test_data[0]['ground_truth'] = ground_truth
        
        return self.evaluate(test_data)


# ä¾¿æ·å‡½æ•°
def quick_evaluate(question: str, answer: str, contexts: List[str], 
                  ground_truth: str = None) -> Dict[str, Any]:
    """å¿«é€Ÿè¯„ä¼°å‡½æ•° - é€‚åˆå•æ¬¡è¯„ä¼°"""
    evaluator = RAGASEvaluator()
    return evaluator.quick_check(question, answer, contexts, ground_truth)


def batch_evaluate(test_data: List[Dict]) -> Dict[str, Any]:
    """æ‰¹é‡è¯„ä¼°å‡½æ•° - é€‚åˆæ‰¹é‡è¯„ä¼°"""
    evaluator = RAGASEvaluator() 
    return evaluator.evaluate(test_data)


if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_data = [
        {
            'question': 'Pythonä¸­å¦‚ä½•åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Ÿ',
            'answer': 'åœ¨Pythonä¸­åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¯ä»¥ä½¿ç”¨venvæ¨¡å—ã€‚å‘½ä»¤æ˜¯ï¼špython -m venv myenvï¼Œç„¶åç”¨source myenv/bin/activateæ¿€æ´»ã€‚',
            'contexts': [
                'Pythonè™šæ‹Ÿç¯å¢ƒå¯ä»¥ä½¿ç”¨venvæ¨¡å—åˆ›å»ºï¼Œå‘½ä»¤ä¸ºpython -m venv ç¯å¢ƒåç§°ã€‚',
                'æ¿€æ´»è™šæ‹Ÿç¯å¢ƒçš„å‘½ä»¤åœ¨Linux/Macä¸‹æ˜¯source venv/bin/activateï¼ŒWindowsä¸‹æ˜¯venv\\Scripts\\activateã€‚'
            ],
            'ground_truth': 'Pythonåˆ›å»ºè™šæ‹Ÿç¯å¢ƒä½¿ç”¨python -m venvå‘½ä»¤ï¼Œæ¿€æ´»ç”¨source venv/bin/activateã€‚'
        },
        {
            'question': 'ä»€ä¹ˆæ˜¯Dockerå®¹å™¨ï¼Ÿ',
            'answer': 'Dockerå®¹å™¨æ˜¯ä¸€ç§è½»é‡çº§çš„è™šæ‹ŸåŒ–æŠ€æœ¯ï¼Œå®ƒå°†åº”ç”¨ç¨‹åºåŠå…¶ä¾èµ–æ‰“åŒ…åœ¨ä¸€ä¸ªå¯ç§»æ¤çš„å®¹å™¨ä¸­ã€‚',
            'contexts': [
                'Dockeræ˜¯ä¸€ä¸ªå¼€æºçš„å®¹å™¨åŒ–å¹³å°ï¼Œç”¨äºå¼€å‘ã€å‘å¸ƒå’Œè¿è¡Œåº”ç”¨ç¨‹åºã€‚',
                'å®¹å™¨æ˜¯ä¸€ç§è½»é‡çº§çš„è™šæ‹ŸåŒ–æŠ€æœ¯ï¼Œç›¸æ¯”ä¼ ç»Ÿè™šæ‹Ÿæœºæ¶ˆè€—æ›´å°‘èµ„æºã€‚'
            ],
            'ground_truth': 'Dockerå®¹å™¨æ˜¯è½»é‡çº§è™šæ‹ŸåŒ–æŠ€æœ¯ï¼Œç”¨äºæ‰“åŒ…å’Œè¿è¡Œåº”ç”¨ç¨‹åºã€‚'
        }
    ]
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = RAGASEvaluator()
    
    # æ‰§è¡Œè¯„ä¼°
    print("å¼€å§‹RAGASè¯„ä¼°...")
    result = evaluator.evaluate(test_data)
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    report = evaluator.generate_report(result)
    print(report)
    
    # å¿«é€Ÿå•æ¬¡è¯„ä¼°ç¤ºä¾‹
    print("\n=== å¿«é€Ÿè¯„ä¼°ç¤ºä¾‹ ===")
    quick_result = quick_evaluate(
        question="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        answer="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚",
        contexts=["æœºå™¨å­¦ä¹ æ˜¯AIçš„é‡è¦åˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºå­¦ä¹ æ•°æ®æ¨¡å¼ã€‚"]
    )
    
    print(f"å¿«é€Ÿè¯„ä¼°ç»“æœ: {quick_result['overall_score']:.3f}")
    print(f"è´¨é‡ç­‰çº§: {quick_result['quality_grade']}")