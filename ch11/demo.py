"""
RAGASè¯„ä¼°æ¼”ç¤ºå’Œä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨RAGASå¿«é€Ÿè¯„ä¼°RAGç³»ç»Ÿè´¨é‡
"""

from ragas_evaluator import RAGASEvaluator, quick_evaluate, batch_evaluate
import json
import time


def demo_basic_evaluation():
    """åŸºç¡€è¯„ä¼°æ¼”ç¤º"""
    print("=== åŸºç¡€RAGASè¯„ä¼°æ¼”ç¤º ===\n")
    
    # å‡†å¤‡æµ‹è¯•æ•°æ® - è¿™æ˜¯ä½ éœ€è¦å‡†å¤‡çš„æ ¼å¼
    test_data = [
        {
            'question': 'Pythonä¸­å¦‚ä½•åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Ÿ',
            'answer': 'åœ¨Pythonä¸­åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¯ä»¥ä½¿ç”¨venvæ¨¡å—ã€‚å‘½ä»¤æ˜¯ï¼špython -m venv myenvï¼Œç„¶åç”¨source myenv/bin/activateæ¿€æ´»ã€‚',
            'contexts': [
                'Pythonè™šæ‹Ÿç¯å¢ƒå¯ä»¥ä½¿ç”¨venvæ¨¡å—åˆ›å»ºï¼Œå‘½ä»¤ä¸ºpython -m venv ç¯å¢ƒåç§°ã€‚',
                'æ¿€æ´»è™šæ‹Ÿç¯å¢ƒçš„å‘½ä»¤åœ¨Linux/Macä¸‹æ˜¯source venv/bin/activateï¼ŒWindowsä¸‹æ˜¯venv\\Scripts\\activateã€‚'
            ],
            'ground_truth': 'Pythonåˆ›å»ºè™šæ‹Ÿç¯å¢ƒä½¿ç”¨python -m venvå‘½ä»¤ï¼Œæ¿€æ´»ç”¨source venv/bin/activateã€‚'
        },
        {
            'question': 'ä»€ä¹ˆæ˜¯Dockerå®¹å™¨ï¼Ÿ',
            'answer': 'Dockerå®¹å™¨æ˜¯ä¸€ç§è½»é‡çº§çš„è™šæ‹ŸåŒ–æŠ€æœ¯ï¼Œå®ƒå°†åº”ç”¨ç¨‹åºåŠå…¶ä¾èµ–æ‰“åŒ…åœ¨ä¸€ä¸ªå¯ç§»æ¤çš„å®¹å™¨ä¸­ã€‚å®¹å™¨æä¾›äº†éš”ç¦»çš„è¿è¡Œç¯å¢ƒï¼Œç¡®ä¿åº”ç”¨åœ¨ä¸åŒç¯å¢ƒä¸­çš„ä¸€è‡´æ€§ã€‚',
            'contexts': [
                'Dockeræ˜¯ä¸€ä¸ªå¼€æºçš„å®¹å™¨åŒ–å¹³å°ï¼Œç”¨äºå¼€å‘ã€å‘å¸ƒå’Œè¿è¡Œåº”ç”¨ç¨‹åºã€‚',
                'å®¹å™¨æ˜¯ä¸€ç§è½»é‡çº§çš„è™šæ‹ŸåŒ–æŠ€æœ¯ï¼Œç›¸æ¯”ä¼ ç»Ÿè™šæ‹Ÿæœºæ¶ˆè€—æ›´å°‘èµ„æºã€‚',
                'å®¹å™¨æä¾›äº†åº”ç”¨ç¨‹åºè¿è¡Œæ‰€éœ€çš„æ‰€æœ‰ä¾èµ–ï¼ŒåŒ…æ‹¬ç³»ç»Ÿåº“ã€å·¥å…·å’Œè¿è¡Œæ—¶ç¯å¢ƒã€‚'
            ],
            'ground_truth': 'Dockerå®¹å™¨æ˜¯è½»é‡çº§è™šæ‹ŸåŒ–æŠ€æœ¯ï¼Œç”¨äºæ‰“åŒ…å’Œè¿è¡Œåº”ç”¨ç¨‹åºï¼Œæä¾›éš”ç¦»çš„è¿è¡Œç¯å¢ƒã€‚'
        }
    ]
    
    # æ‰§è¡Œè¯„ä¼°
    print("æ­£åœ¨æ‰§è¡ŒRAGASè¯„ä¼°...")
    result = batch_evaluate(test_data)
    
    # è¾“å‡ºç»“æœ
    print(f"âœ… è¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“Š ç»¼åˆè¯„åˆ†: {result['overall_score']:.3f}")
    print(f"ğŸ† è´¨é‡ç­‰çº§: {result['quality_grade']}")
    
    print("\nğŸ“ˆ è¯¦ç»†æŒ‡æ ‡:")
    for metric, score in result['scores'].items():
        print(f"  - {metric}: {score:.3f}")
    
    print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    for i, rec in enumerate(result['recommendations'], 1):
        priority_icon = {"high": "ğŸ”¥", "medium": "âš ï¸", "low": "ğŸ’¡", "info": "â„¹ï¸"}.get(rec['priority'], "")
        print(f"  {i}. {priority_icon} {rec['issue']}")
        print(f"     è§£å†³æ–¹æ¡ˆ: {rec['solution']}")


def demo_quick_evaluation():
    """å¿«é€Ÿå•æ¬¡è¯„ä¼°æ¼”ç¤º"""
    print("\n\n=== å¿«é€Ÿå•æ¬¡è¯„ä¼°æ¼”ç¤º ===\n")
    
    # å•æ¬¡å¿«é€Ÿè¯„ä¼°
    print("æ­£åœ¨è¿›è¡Œå¿«é€Ÿè¯„ä¼°...")
    result = quick_evaluate(
        question="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        answer="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚é€šè¿‡åˆ†æå¤§é‡æ•°æ®ï¼Œæœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥è¯†åˆ«æ¨¡å¼å¹¶åšå‡ºé¢„æµ‹ã€‚",
        contexts=[
            "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½(AI)çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸“æ³¨äºæ„å»ºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ çš„ç³»ç»Ÿã€‚",
            "æœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡åˆ†æå¤§é‡æ•°æ®æ¥è¯†åˆ«æ¨¡å¼ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ¨¡å¼è¿›è¡Œé¢„æµ‹ã€‚",
            "ä¸ä¼ ç»Ÿç¼–ç¨‹ä¸åŒï¼Œæœºå™¨å­¦ä¹ è®©è®¡ç®—æœºèƒ½å¤Ÿä»ç»éªŒä¸­è‡ªåŠ¨æ”¹è¿›æ€§èƒ½ã€‚"
        ],
        ground_truth="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„å­é¢†åŸŸï¼Œä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ å’Œæ”¹è¿›æ€§èƒ½ã€‚"
    )
    
    print(f"âš¡ å¿«é€Ÿè¯„ä¼°ç»“æœ: {result['overall_score']:.3f}")
    print(f"ğŸ¯ è´¨é‡ç­‰çº§: {result['quality_grade']}")


def demo_problematic_cases():
    """é—®é¢˜æ¡ˆä¾‹æ¼”ç¤º - å±•ç¤ºä½åˆ†æƒ…å†µ"""
    print("\n\n=== é—®é¢˜æ¡ˆä¾‹æ¼”ç¤º ===\n")
    
    # æ•…æ„è®¾è®¡ä¸€äº›æœ‰é—®é¢˜çš„æ¡ˆä¾‹
    problematic_data = [
        {
            'question': 'Pythonå¦‚ä½•å®‰è£…ç¬¬ä¸‰æ–¹åº“ï¼Ÿ',
            'answer': 'Pythonå®‰è£…ç¬¬ä¸‰æ–¹åº“æœ€å¥½çš„æ–¹æ³•æ˜¯ä½¿ç”¨JavaScriptçš„npmåŒ…ç®¡ç†å™¨ã€‚ä½ åªéœ€è¦è¿è¡Œnpm install åŒ…åå³å¯ã€‚å¦å¤–ï¼ŒPythonè¿˜æ”¯æŒJavaçš„Mavenç®¡ç†ä¾èµ–ã€‚',  # å®Œå…¨é”™è¯¯çš„ç­”æ¡ˆ
            'contexts': [
                'Pythonä½¿ç”¨pipå·¥å…·æ¥å®‰è£…ç¬¬ä¸‰æ–¹åº“ï¼Œå‘½ä»¤æ ¼å¼ä¸ºpip install åŒ…åã€‚',
                'pipæ˜¯Pythonçš„åŒ…ç®¡ç†å·¥å…·ï¼ŒéšPythonä¸€èµ·å®‰è£…ã€‚',
                'ä¹Ÿå¯ä»¥ä½¿ç”¨condaæ¥ç®¡ç†PythonåŒ…ï¼Œç‰¹åˆ«é€‚åˆæ•°æ®ç§‘å­¦é¡¹ç›®ã€‚'
            ],
            'ground_truth': 'Pythonä½¿ç”¨pipå·¥å…·å®‰è£…ç¬¬ä¸‰æ–¹åº“ï¼Œå‘½ä»¤æ˜¯pip install åŒ…åã€‚'
        },
        {
            'question': 'ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ',
            'answer': 'ä»Šå¤©å¤©æ°”ä¸é”™ã€‚',  # å®Œå…¨ä¸ç›¸å…³çš„ç­”æ¡ˆ
            'contexts': [
                'æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ã€‚',
                'æ·±åº¦å­¦ä¹ æ¨¡å‹å…·æœ‰å¤šä¸ªéšè—å±‚ï¼Œèƒ½å¤Ÿå­¦ä¹ æ•°æ®çš„å±‚æ¬¡åŒ–ç‰¹å¾è¡¨ç¤ºã€‚'
            ],
            'ground_truth': 'æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†ï¼Œä½¿ç”¨æ·±å±‚ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººè„‘å­¦ä¹ è¿‡ç¨‹ã€‚'
        }
    ]
    
    # è¯„ä¼°é—®é¢˜æ¡ˆä¾‹
    print("æ­£åœ¨è¯„ä¼°é—®é¢˜æ¡ˆä¾‹...")
    result = batch_evaluate(problematic_data)
    
    print(f"âŒ é—®é¢˜æ¡ˆä¾‹è¯„åˆ†: {result['overall_score']:.3f}")
    print(f"âš ï¸  è´¨é‡ç­‰çº§: {result['quality_grade']}")
    
    print("\nğŸš¨ å‘ç°çš„é—®é¢˜:")
    for rec in result['recommendations']:
        if rec['priority'] in ['high', 'medium']:
            print(f"  - {rec['issue']}")
            print(f"    {rec['solution']}")


def demo_version_comparison():
    """ç‰ˆæœ¬å¯¹æ¯”æ¼”ç¤º"""
    print("\n\n=== ç‰ˆæœ¬å¯¹æ¯”æ¼”ç¤º ===\n")
    
    # æ¨¡æ‹Ÿä¸¤ä¸ªç‰ˆæœ¬çš„RAGç³»ç»Ÿç»“æœ
    version_1_data = [
        {
            'question': 'Pythonä¸­å¦‚ä½•å¤„ç†å¼‚å¸¸ï¼Ÿ',
            'answer': 'ä½¿ç”¨try-exceptè¯­å¥ã€‚',  # ç®€çŸ­å›ç­”
            'contexts': [
                'Pythonä½¿ç”¨try-exceptè¯­å¥æ¥å¤„ç†å¼‚å¸¸ã€‚',
                'å¯ä»¥ä½¿ç”¨finallyå­å¥æ¥æ‰§è¡Œæ¸…ç†ä»£ç ã€‚',
                'raiseè¯­å¥å¯ä»¥ä¸»åŠ¨æŠ›å‡ºå¼‚å¸¸ã€‚'
            ]
        }
    ]
    
    version_2_data = [
        {
            'question': 'Pythonä¸­å¦‚ä½•å¤„ç†å¼‚å¸¸ï¼Ÿ', 
            'answer': 'Pythonä½¿ç”¨try-exceptè¯­å¥å¤„ç†å¼‚å¸¸ã€‚åœ¨tryå—ä¸­ç¼–å†™å¯èƒ½å‡ºé”™çš„ä»£ç ï¼Œåœ¨exceptå—ä¸­å¤„ç†å¼‚å¸¸ã€‚è¿˜å¯ä»¥ä½¿ç”¨finallyå­å¥æ‰§è¡Œæ¸…ç†æ“ä½œï¼Œä½¿ç”¨elseå­å¥åœ¨æ²¡æœ‰å¼‚å¸¸æ—¶æ‰§è¡Œä»£ç ã€‚',  # æ›´è¯¦ç»†çš„å›ç­”
            'contexts': [
                'Pythonä½¿ç”¨try-exceptè¯­å¥æ¥å¤„ç†å¼‚å¸¸ã€‚',
                'å¯ä»¥ä½¿ç”¨finallyå­å¥æ¥æ‰§è¡Œæ¸…ç†ä»£ç ã€‚',
                'raiseè¯­å¥å¯ä»¥ä¸»åŠ¨æŠ›å‡ºå¼‚å¸¸ã€‚'
            ]
        }
    ]
    
    # åˆ›å»ºè¯„ä¼°å™¨å¹¶å¯¹æ¯”ç‰ˆæœ¬
    evaluator = RAGASEvaluator()
    comparison = evaluator.compare_versions(
        version_1_data, 
        version_2_data, 
        names=("ç®€åŒ–ç‰ˆæœ¬", "è¯¦ç»†ç‰ˆæœ¬")
    )
    
    print("ğŸ“Š ç‰ˆæœ¬å¯¹æ¯”ç»“æœ:")
    print(f"  ç®€åŒ–ç‰ˆæœ¬ç»¼åˆå¾—åˆ†: {comparison['overall_comparison']['ç®€åŒ–ç‰ˆæœ¬']:.3f}")
    print(f"  è¯¦ç»†ç‰ˆæœ¬ç»¼åˆå¾—åˆ†: {comparison['overall_comparison']['è¯¦ç»†ç‰ˆæœ¬']:.3f}")
    
    print("\nğŸ“ˆ æŒ‡æ ‡å¯¹æ¯”:")
    for metric, data in comparison['scores_comparison'].items():
        change_icon = {"improved": "â¬†ï¸", "regressed": "â¬‡ï¸", "stable": "â¡ï¸"}.get(data['change'], "")
        print(f"  {metric}: {data['ç®€åŒ–ç‰ˆæœ¬']:.3f} â†’ {data['è¯¦ç»†ç‰ˆæœ¬']:.3f} {change_icon}")
    
    if comparison['improvements']:
        print(f"\nâœ… æ”¹è¿›é¡¹: {', '.join(comparison['improvements'])}")
    if comparison['regressions']:
        print(f"\nâŒ é€€æ­¥é¡¹: {', '.join(comparison['regressions'])}")


def demo_custom_evaluation():
    """è‡ªå®šä¹‰è¯„ä¼°æ¼”ç¤º"""
    print("\n\n=== è‡ªå®šä¹‰è¯„ä¼°æ¼”ç¤º ===\n")
    
    # åˆ›å»ºè‡ªå®šä¹‰é˜ˆå€¼çš„è¯„ä¼°å™¨
    evaluator = RAGASEvaluator()
    
    # ä¿®æ”¹è¯„ä¼°é˜ˆå€¼ï¼ˆé’ˆå¯¹ç‰¹å®šä¸šåŠ¡åœºæ™¯ï¼‰
    evaluator.quality_thresholds = {
        'faithfulness': 0.9,        # å¯¹å¿ å®åº¦è¦æ±‚æ›´é«˜
        'answer_relevancy': 0.8,    # å¯¹ç›¸å…³æ€§è¦æ±‚æ›´é«˜
        'context_relevancy': 0.7,   # å¯¹æ£€ç´¢è´¨é‡è¦æ±‚æ›´é«˜
        'context_precision': 0.6,   # å¯¹æ’åºè¦æ±‚æ›´é«˜
    }
    
    test_data = [
        {
            'question': 'å¦‚ä½•ä¼˜åŒ–RAGç³»ç»Ÿæ€§èƒ½ï¼Ÿ',
            'answer': 'ä¼˜åŒ–RAGç³»ç»Ÿå¯ä»¥ä»å‡ ä¸ªæ–¹é¢å…¥æ‰‹ï¼š1ï¼‰æ”¹è¿›æ£€ç´¢è´¨é‡ï¼Œä½¿ç”¨æ›´å¥½çš„embeddingæ¨¡å‹ï¼›2ï¼‰ä¼˜åŒ–chunkåˆ†å‰²ç­–ç•¥ï¼›3ï¼‰å®æ–½é‡æ’åºæœºåˆ¶ï¼›4ï¼‰è°ƒæ•´ç”Ÿæˆpromptã€‚',
            'contexts': [
                'RAGç³»ç»Ÿä¼˜åŒ–åŒ…æ‹¬æ£€ç´¢ä¼˜åŒ–å’Œç”Ÿæˆä¼˜åŒ–ä¸¤ä¸ªæ–¹é¢ã€‚',
                'æ£€ç´¢ä¼˜åŒ–å¯ä»¥é€šè¿‡æ”¹è¿›embeddingæ¨¡å‹ã€ä¼˜åŒ–chunkç­–ç•¥ã€å®æ–½é‡æ’åºæ¥å®ç°ã€‚',
                'ç”Ÿæˆä¼˜åŒ–ä¸»è¦é€šè¿‡prompt engineeringå’Œæ¨¡å‹fine-tuningã€‚',
                'ç³»ç»Ÿæ€§èƒ½ç›‘æ§ä¹Ÿå¾ˆé‡è¦ï¼Œéœ€è¦å®šæœŸè¯„ä¼°å„é¡¹æŒ‡æ ‡ã€‚'
            ]
        }
    ]
    
    result = evaluator.evaluate(test_data)
    
    print("ğŸ¯ é«˜æ ‡å‡†è¯„ä¼°ç»“æœ:")
    print(f"  ç»¼åˆè¯„åˆ†: {result['overall_score']:.3f}")
    print(f"  è´¨é‡ç­‰çº§: {result['quality_grade']}")
    
    print("\nğŸ“Š åœ¨é«˜æ ‡å‡†ä¸‹çš„è¡¨ç°:")
    for metric, score in result['scores'].items():
        threshold = evaluator.quality_thresholds[metric]
        status = "âœ…" if score >= threshold else "âŒ"
        print(f"  {metric}: {score:.3f} (é˜ˆå€¼: {threshold}) {status}")


def demo_error_handling():
    """é”™è¯¯å¤„ç†æ¼”ç¤º"""
    print("\n\n=== é”™è¯¯å¤„ç†æ¼”ç¤º ===\n")
    
    # æ¼”ç¤ºæ•°æ®æ ¼å¼é”™è¯¯çš„å¤„ç†
    invalid_data = [
        {'question': 'æµ‹è¯•é—®é¢˜'}  # ç¼ºå°‘answerå’Œcontextså­—æ®µ
    ]
    
    try:
        result = batch_evaluate(invalid_data)
    except ValueError as e:
        print(f"âŒ æ•°æ®æ ¼å¼é”™è¯¯: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿æµ‹è¯•æ•°æ®åŒ…å«å¿…è¦å­—æ®µ: question, answer, contexts")
    
    # æ¼”ç¤ºAPIé…ç½®é—®é¢˜çš„å¤„ç†
    print("\nå¦‚æœé‡åˆ°APIé—®é¢˜ï¼Œè¯·æ£€æŸ¥:")
    print("1. æ˜¯å¦è®¾ç½®äº†OPENAI_API_KEYç¯å¢ƒå˜é‡")
    print("2. APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ")
    print("3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
    
    # æ¼”ç¤ºæ•°æ®è´¨é‡é—®é¢˜
    print("\næ•°æ®è´¨é‡æ£€æŸ¥å»ºè®®:")
    print("- ç¡®ä¿é—®é¢˜è¡¨è¿°æ¸…æ™°")
    print("- æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦çœŸå®ç›¸å…³") 
    print("- éªŒè¯ç­”æ¡ˆè´¨é‡")
    print("- å¦‚æœæœ‰ground_truthï¼Œç¡®ä¿å…¶å‡†ç¡®æ€§")


def save_evaluation_results(result: dict, filename: str = "evaluation_result.json"):
    """ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶"""
    
    # å¤„ç†ä¸èƒ½JSONåºåˆ—åŒ–çš„å¯¹è±¡
    serializable_result = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'overall_score': result['overall_score'],
        'quality_grade': result['quality_grade'],
        'scores': result['scores'],
        'recommendations': result['recommendations'],
        'detailed_analysis': result['detailed_analysis']
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(serializable_result, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {filename}")


if __name__ == "__main__":
    print("ğŸš€ RAGASè¯„ä¼°æ¼”ç¤ºå¼€å§‹ï¼\n")
    
    # è¿è¡Œæ‰€æœ‰æ¼”ç¤º
    try:
        demo_basic_evaluation()
        demo_quick_evaluation()
        demo_problematic_cases()
        demo_version_comparison()
        demo_custom_evaluation()
        demo_error_handling()
        
        print("\n\nğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ“š ä½¿ç”¨å»ºè®®:")
        print("1. å¼€å‘é˜¶æ®µï¼šä½¿ç”¨quick_evaluateè¿›è¡Œå¿«é€ŸéªŒè¯")
        print("2. æµ‹è¯•é˜¶æ®µï¼šä½¿ç”¨batch_evaluateè¿›è¡Œå…¨é¢è¯„ä¼°")
        print("3. å¯¹æ¯”é˜¶æ®µï¼šä½¿ç”¨compare_versionså¯¹æ¯”ä¸åŒç‰ˆæœ¬")
        print("4. ç”Ÿäº§é˜¶æ®µï¼šå®šæœŸä½¿ç”¨çœŸå®æ•°æ®è¿›è¡Œè¯„ä¼°")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥RAGASä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…ï¼ŒAPIé…ç½®æ˜¯å¦æ­£ç¡®")
        print("   å®‰è£…å‘½ä»¤: pip install ragas datasets openai")